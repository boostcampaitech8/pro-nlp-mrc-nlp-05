import os
import json
import torch
import tqdm
import re
import pickle
import networkx as nx
import numpy as np
from functools import partial
from collections import defaultdict
from typing import List, Dict, Callable, Optional, Tuple, Any, Union

from dotenv import load_dotenv
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer
from kiwipiepy import Kiwi
from gliner import GLiNER
from sentence_transformers import CrossEncoder
import bm25s

# LlamaIndex ê´€ë ¨
from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import QueryFusionRetriever, BaseRetriever
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.schema import TextNode, NodeWithScore, QueryBundle, BaseNode
from llama_index.vector_stores.faiss import FaissVectorStore
import faiss
from datasets import load_from_disk

# --- ì„¤ì • ìƒìˆ˜ ---
GEMMA_MODEL_NAME = "google/gemma-3-4b-it"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
RERANKER_MODEL_NAME = "BAAI/bge-reranker-v2-m3"
WIKI_DATA_PATH = './data/wikipedia_documents.json'
TRAIN_SET_DIR = "./data/test_dataset/"
KG_FILE_PATH = "./urimalsaem_graph_FINAL2.pkl" # ì§€ì‹ ê·¸ë˜í”„ ê²½ë¡œ
OUTPUT_FILE_PATH = './test_context_kiwi_NER_synonym_dense.json'

# --- 1. í™˜ê²½ ë° ë°ì´í„° ì„¤ì • ---
def setup_environment():
    load_dotenv()
    HF_TOKEN = os.getenv("HF_TOKEN")
    if HF_TOKEN:
        login(token=HF_TOKEN)
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

def load_wiki_data():
    with open(WIKI_DATA_PATH) as f:
        return json.load(f)

def load_train_dataset():
    return load_from_disk(TRAIN_SET_DIR)

# --- 2. ë§¤í•‘ ë°ì´í„° ë° ì‚¬ì „ ì •ì˜ ---
KIWI_TO_URIMALSAEM_MAP = {
    "NNG": "ëª…ì‚¬", "NNP": "ëª…ì‚¬", "NNB": "ì˜ì¡´ ëª…ì‚¬", "NR": "ìˆ˜ì‚¬", "XR": "ëª…ì‚¬", "SN": "ìˆ˜ì‚¬",
    "VV": "ë™ì‚¬", "VA": "í˜•ìš©ì‚¬", "MM": "ê´€í˜•ì‚¬"
}

KIWI_TAGS = ['NNG', 'NNP', 'NNB', 'NR', 'VV', 'VA', 'MM', 'XR', 'SW', 'SL', 'SH', 'SN', 'SB']

GLINER_TAG = {
    "ì–¸ì–´": {"CV_LANGUAGE": "ë¬¸ëª…_ì–¸ì–´", "TR_HUMANITIES": "ì´ë¡ _ì² í•™/ì–¸ì–´/ì—­ì‚¬"}, "ë¬¸í•™": {"FD_HUMANITIES": "í•™ë¬¸ ë¶„ì•¼_ì¸ë¬¸í•™", "AFA_DOCUMENT": "ì¸ê³µë¬¼_ë„ì„œ/ì„œì  ì‘í’ˆëª…"}, "ì—­ì‚¬": {"CV_CULTURE": "ë¬¸ëª…_ë¬¸ëª…/ë¬¸í™”", "AF_CULTURAL_ASSET": "ì¸ê³µë¬¼_ë¬¸í™”ì¬", "DT_DYNASTY": "ë‚ ì§œ_ì™•ì¡°ì‹œëŒ€", "DT_GEOAGE": "ë‚ ì§œ_ì§€ì§ˆì‹œëŒ€", "EV_WAR_REVOLUTION": "ì‚¬ê±´_ì „ìŸ/í˜ëª…", "EV_OTHERS": "ì‚¬ê±´_ê¸°íƒ€"}, "ì² í•™": {"TR_HUMANITIES": "ì´ë¡ _ì² í•™/ì–¸ì–´/ì—­ì‚¬", "CV_TRIBE": "ë¬¸ëª…_ë¯¼ì¡±/ì¢…ì¡±"}, "êµìœ¡": {"OGG_EDUCATION": "ê¸°ê´€_êµìœ¡", "OGG_LIBRARY": "ê¸°ê´€_ë„ì„œê´€"}, "ë¯¼ì†": {"CV_CULTURE": "ë¬¸ëª…_ë¬¸ëª…/ë¬¸í™”", "EV_FESTIVAL": "ì‚¬ê±´_ì¶•ì œ/ì˜í™”ì œ"}, "ì¸ë¬¸ ì¼ë°˜": {"FD_HUMANITIES": "í•™ë¬¸ ë¶„ì•¼_ì¸ë¬¸í•™"},
    "ë²•ë¥ ": {"CV_LAW": "ë¬¸ëª…_ë²•/ë²•ë¥ ", "OGG_LAW": "ê¸°ê´€_ë²•ë¥ ", "CV_POLICY": "ë¬¸ëª…_ì œë„/ì •ì±…"}, "êµ°ì‚¬": {"OGG_MILITARY": "ê¸°ê´€_êµ°ì‚¬", "AF_WEAPON": "ì¸ê³µë¬¼_ë¬´ê¸°"}, "ê²½ì˜": {"OGG_ECONOMY": "ê¸°ê´€_ê²½ì œ", "AFW_SERVICE_PRODUCTS": "ì¸ê³µë¬¼_ì„œë¹„ìŠ¤ ìƒí’ˆ"}, "ê²½ì œ": {"OGG_ECONOMY": "ê¸°ê´€_ê²½ì œ", "CV_CURRENCY": "ë¬¸ëª…_í†µí™”", "CV_TAX": "ë¬¸ëª…_ì¡°ì„¸"}, "ë³µì§€": {"CV_FUNDS": "ë¬¸ëª…_ì—°ê¸ˆ/ê¸°ê¸ˆ"}, "ì •ì¹˜": {"OGG_POLITICS": "ê¸°ê´€_ì •ë¶€/ê³µê³µ", "CV_POLICY": "ë¬¸ëª…_ì œë„/ì •ì±…", "EV_ACTIVITY": "ì‚¬ê±´_ì‚¬íšŒìš´ë™/ì„ ì–¸"}, "ë§¤ì²´": {"OGG_MEDIA": "ê¸°ê´€_ë¯¸ë””ì–´", "AFA_VIDEO": "ì¸ê³µë¬¼_ì˜í™”/TV í”„ë¡œê·¸ë¨"}, "í–‰ì •": {"OGG_POLITICS": "ê¸°ê´€_ì •ë¶€/ê³µê³µ"}, "ì‹¬ë¦¬": {"FD_SOCIAL_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ì‚¬íšŒê³¼í•™"}, "ì‚¬íšŒ ì¼ë°˜": {"FD_SOCIAL_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ì‚¬íšŒê³¼í•™"},
    "ì§€êµ¬": {"FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™", "MT_ROCK": "ë¬¼ì§ˆ_ì•”ì„"}, "ì§€ë¦¬": {"LC_OTHERS": "ì¥ì†Œ_ê¸°íƒ€", "LCG_MOUNTAIN": "ì¥ì†Œ_ì‚°/ì‚°ë§¥", "LCG_RIVER": "ì¥ì†Œ_ê°•/í˜¸ìˆ˜", "LCG_OCEAN": "ì¥ì†Œ_ë°”ë‹¤", "LCG_ISLAND": "ì¥ì†Œ_ì„¬", "LCG_CONTINENT": "ì¥ì†Œ_ëŒ€ë¥™", "TM_DIRECTION": "ìš©ì–´_ë°©í–¥"}, "í•´ì–‘": {"LCG_OCEAN": "ì¥ì†Œ_ë°”ë‹¤", "LCG_BAY": "ì¥ì†Œ_ë°˜ë„/ë§Œ"}, "ì²œë¬¸": {"LC_SPACE": "ì¥ì†Œ_ì²œì²´", "FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™"}, "í™˜ê²½": {"TM_CLIMATE": "ìš©ì–´_ê¸°í›„ ì§€ì—­", "FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™"}, "ìƒëª…": {"TM_CELL_TISSUE_ORGAN": "ìš©ì–´_ì„¸í¬/ì¡°ì§/ê¸°ê´€", "FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™"}, "ë™ë¬¼": {"AM_INSECT": "ë™ë¬¼_ê³¤ì¶©", "AM_BIRD": "ë™ë¬¼_ì¡°ë¥˜", "AM_FISH": "ë™ë¬¼_ì–´ë¥˜", "AM_MAMMALIA": "ë™ë¬¼_í¬ìœ ë¥˜", "AM_AMPHIBIA": "ë™ë¬¼_ì–‘ì„œë¥˜", "AM_REPTILIA": "ë™ë¬¼_íŒŒì¶©ë¥˜", "AM_TYPE": "ë™ë¬¼_ë¶„ë¥˜ëª…", "AM_PART": "ë™ë¬¼_ë¶€ìœ„ëª…", "AM_OTHERS": "ë™ë¬¼_ê¸°íƒ€"}, "ì‹ë¬¼": {"PT_FLOWER": "ì‹ë¬¼_ê½ƒ", "PT_GRASS": "ì‹ë¬¼_í’€", "PT_TYPE": "ì‹ë¬¼_ë¶„ë¥˜ëª…", "PT_PART": "ì‹ë¬¼_ë¶€ìœ„ëª…", "PT_OTHERS": "ì‹ë¬¼_ê¸°íƒ€", "PT_TREE": "ì‹ë¬¼_ë‚˜ë¬´", "PT_FRUIT": "ì‹ë¬¼_ê³¼ì¼/ì—´ë§¤"}, "ì²œì—°ìì›": {"MT_ELEMENT": "ë¬¼ì§ˆ_ì›ì†Œ"}, "ìˆ˜í•™": {"FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™", "TM_SHAPE": "ìš©ì–´_ëª¨ì–‘/í˜•íƒœ", "QT_SIZE": "ìˆ˜ëŸ‰_ë„“ì´/ë©´ì ", "QT_LENGTH": "ìˆ˜ëŸ‰_ê¸¸ì´/ê±°ë¦¬", "QT_VOLUME": "ìˆ˜ëŸ‰_ë¶€í”¼", "QT_PERCENTAGE": "ìˆ˜ëŸ‰_ë°±ë¶„ìœ¨"}, "ë¬¼ë¦¬": {"FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™", "TR_SCIENCE": "ì´ë¡ _ê³¼í•™", "QT_SPEED": "ìˆ˜ëŸ‰_ì†ë„", "QT_TEMPERATURE": "ìˆ˜ëŸ‰_ì˜¨ë„", "QT_WEIGHT": "ìˆ˜ëŸ‰_ë¬´ê²Œ"}, "í™”í•™": {"MT_CHEMICAL": "ë¬¼ì§ˆ_í™”í•™", "MT_ELEMENT": "ë¬¼ì§ˆ_ì›ì†Œ", "MT_METAL": "ë¬¼ì§ˆ_ê¸ˆì†"}, "ìì—° ì¼ë°˜": {"FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™"},
    "ë†ì—…": {"PT_FRUIT": "ì‹ë¬¼_ê³¼ì¼/ì—´ë§¤", "FD_OTHERS": "í•™ë¬¸ ë¶„ì•¼_ê¸°íƒ€"}, "ìˆ˜ì‚°ì—…": {"AM_FISH": "ë™ë¬¼_ì–´ë¥˜"}, "ì„ì—…": {"PT_TREE": "ì‹ë¬¼_ë‚˜ë¬´"}, "ê´‘ì—…": {"MT_ROCK": "ë¬¼ì§ˆ_ì•”ì„", "MT_METAL": "ë¬¼ì§ˆ_ê¸ˆì†"}, "ê³µì—…": {"AFW_OTHER_PRODUCTS": "ì¸ê³µë¬¼_ê¸°íƒ€ ìƒí’ˆ"}, "ì„œë¹„ìŠ¤ì—…": {"AFW_SERVICE_PRODUCTS": "ì¸ê³µë¬¼_ì„œë¹„ìŠ¤ ìƒí’ˆ", "OGG_HOTEL": "ê¸°ê´€_ìˆ™ë°• ì—…ì²´"}, "ì‚°ì—… ì¼ë°˜": {"OGG_ECONOMY": "ê¸°ê´€_ê²½ì œ"},
    "ì˜í•™": {"FD_MEDICINE": "í•™ë¬¸ ë¶„ì•¼_ì˜í•™", "TR_MEDICINE": "ì´ë¡ _ì˜í•™", "TMM_DISEASE": "ìš©ì–´_ì¦ìƒ/ì§ˆë³‘"}, "ì•½í•™": {"TMM_DRUG": "ìš©ì–´_ì•½í’ˆ"}, "í•œì˜": {"FD_MEDICINE": "í•™ë¬¸ ë¶„ì•¼_ì˜í•™"}, "ìˆ˜ì˜": {"FD_MEDICINE": "í•™ë¬¸ ë¶„ì•¼_ì˜í•™"}, "ì‹í’ˆ": {"CV_FOOD": "ë¬¸ëª…_ìŒì‹", "CV_DRINK": "ë¬¸ëª…_ìŒë£Œ/ìˆ ", "CV_FOOD_STYLE": "ë¬¸ëª…_ìŒì‹ ìœ í˜•"}, "ë³´ê±´ ì¼ë°˜": {"OGG_MEDICINE": "ê¸°ê´€_ì˜ë£Œ"},
    "ê±´ì„¤": {"AF_BUILDING": "ì¸ê³µë¬¼_ê±´ì¶•ë¬¼/í† ëª©ê±´ì„¤ë¬¼", "CV_BUILDING_TYPE": "ë¬¸ëª…_ê±´ì¶• ì–‘ì‹"}, "êµí†µ": {"AF_TRANSPORT": "ì¸ê³µë¬¼_êµí†µìˆ˜ë‹¨/ìš´ì†¡ìˆ˜ë‹¨", "AF_ROAD": "ì¸ê³µë¬¼_ë„ë¡œ/ì² ë¡œ"}, "ê¸°ê³„": {"TMI_HW": "ìš©ì–´_IT í•˜ë“œì›¨ì–´"}, "ì „ê¸°Â·ì „ì": {"TMI_HW": "ìš©ì–´_IT í•˜ë“œì›¨ì–´"}, "ì¬ë£Œ": {"MT_ELEMENT": "ë¬¼ì§ˆ_ì›ì†Œ"}, "ì •ë³´Â·í†µì‹ ": {"TMI_SW": "ìš©ì–´_IT ì†Œí”„íŠ¸ì›¨ì–´", "TMI_HW": "ìš©ì–´_IT í•˜ë“œì›¨ì–´", "TMI_SITE": "ìš©ì–´_URL ì£¼ì†Œ", "TMI_EMAIL": "ìš©ì–´_ì´ë©”ì¼ ì£¼ì†Œ", "TMI_MODEL": "ìš©ì–´_ì œí’ˆ ëª¨ë¸ëª…", "TMI_SERVICE": "ìš©ì–´_IT ì„œë¹„ìŠ¤", "TMI_PROJECT": "ìš©ì–´_í”„ë¡œì íŠ¸"}, "ê³µí•™ ì¼ë°˜": {"FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™"},
    "ì²´ìœ¡": {"CV_SPORTS": "ë¬¸ëª…_ìŠ¤í¬ì¸ ", "OGG_SPORTS": "ê¸°ê´€_ìŠ¤í¬ì¸ ", "CV_SPORTS_POSITION": "ë¬¸ëª…_ìŠ¤í¬ì¸  í¬ì§€ì…˜", "CV_SPORTS_INST": "ë¬¸ëª…_ìŠ¤í¬ì¸  ìš©í’ˆ/ë„êµ¬", "EV_SPORTS": "ì‚¬ê±´_ìŠ¤í¬ì¸  í–‰ì‚¬", "TM_SPORTS": "ìš©ì–´_ìŠ¤í¬ì¸ "}, "ì—°ê¸°": {"AFA_PERFORMANCE": "ì¸ê³µë¬¼_ì¶¤/ê³µì—°/ì—°ê·¹ ì‘í’ˆëª…"}, "ì˜ìƒ": {"AFA_VIDEO": "ì¸ê³µë¬¼_ì˜í™”/TV í”„ë¡œê·¸ë¨"}, "ë¬´ìš©": {"AFA_PERFORMANCE": "ì¸ê³µë¬¼_ì¶¤/ê³µì—°/ì—°ê·¹ ì‘í’ˆëª…"}, "ìŒì•…": {"AFA_MUSIC": "ì¸ê³µë¬¼_ìŒì•… ì‘í’ˆëª…", "AF_MUSICAL_INSTRUMENT": "ì¸ê³µë¬¼_ì•…ê¸°", "OGG_ART": "ê¸°ê´€_ì˜ˆìˆ "}, "ë¯¸ìˆ ": {"AFA_ART_CRAFT": "ì¸ê³µë¬¼_ë¯¸ìˆ /ì¡°í˜• ì‘í’ˆëª…", "FD_ART": "í•™ë¬¸ ë¶„ì•¼_ì˜ˆìˆ ", "TM_COLOR": "ìš©ì–´_ìƒ‰ê¹”"}, "ë³µì‹": {"CV_CLOTHING": "ë¬¸ëª…_ì˜ë³µ/ì„¬ìœ "}, "ê³µì˜ˆ": {"AFA_ART_CRAFT": "ì¸ê³µë¬¼_ë¯¸ìˆ /ì¡°í˜• ì‘í’ˆëª…"}, "ì˜ˆì²´ëŠ¥ ì¼ë°˜": {"FD_ART": "í•™ë¬¸ ë¶„ì•¼_ì˜ˆìˆ "},
    "ê°€í†¨ë¦­": {"OGG_RELIGION": "ê¸°ê´€_ì¢…êµ"}, "ê¸°ë…êµ": {"OGG_RELIGION": "ê¸°ê´€_ì¢…êµ"}, "ë¶ˆêµ": {"OGG_RELIGION": "ê¸°ê´€_ì¢…êµ"}, "ì¢…êµ ì¼ë°˜": {"OGG_RELIGION": "ê¸°ê´€_ì¢…êµ"},
    "ì¸ëª…": {"PS_NAME": "ì¸ë¬¼_ì‚¬ëŒ", "PS_CHARACTER": "ì¸ë¬¼_ê°€ìƒ ìºë¦­í„°", "CV_OCCUPATION": "ë¬¸ëª…_ì§ì—…", "CV_POSITION": "ë¬¸ëª…_ì§ìœ„/ì§ì±…", "CV_RELATION": "ë¬¸ëª…_ê°€ì¡±/ì¹œì¡± ê´€ê³„"}, "ì§€ëª…": {"LCP_COUNTRY": "ì¥ì†Œ_êµ­ê°€", "LCP_PROVINCE": "ì¥ì†Œ_ë„/ì£¼ ì§€ì—­", "LCP_COUNTY": "ì¥ì†Œ_ì„¸ë¶€ í–‰ì •êµ¬ì—­", "LCP_CITY": "ì¥ì†Œ_ë„ì‹œ", "LCP_CAPITALCITY": "ì¥ì†Œ_ìˆ˜ë„", "LC_OTHERS": "ì¥ì†Œ_ê¸°íƒ€"}, "ì±…ëª…": {"AFA_DOCUMENT": "ì¸ê³µë¬¼_ë„ì„œ/ì„œì  ì‘í’ˆëª…"}, "ê³ ìœ ëª… ì¼ë°˜": {"OGG_OTHERS": "ê¸°ê´€_ê¸°íƒ€"}
}

gliner_labels = []

for middle_cat, inner_map in GLINER_TAG.items():
    for gliner_code, leaf_label in inner_map.items():
        gliner_labels.append(leaf_label)
gliner_labels = sorted(list(set(gliner_labels)))

KIWI_TO_URIMALSAEM_MAP = {
    "NNG": "ëª…ì‚¬", "NNP": "ëª…ì‚¬", "NNB": "ì˜ì¡´ ëª…ì‚¬", "NR": "ìˆ˜ì‚¬", "XR": "ëª…ì‚¬", "SN": "ìˆ˜ì‚¬",
    "VV": "ë™ì‚¬", "VA": "í˜•ìš©ì‚¬", "MM": "ê´€í˜•ì‚¬",
    "SW": None, "SB": None, "SL": None, "SH": None
}

def setup_environment():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° Hugging Face ë¡œê·¸ì¸"""
    load_dotenv()
    HF_TOKEN = os.getenv("HF_TOKEN")
    
    if HF_TOKEN:
        login(token=HF_TOKEN)
        print("Hugging Face ë¡œê·¸ì¸ ì„±ê³µ!")
    else:
        print("ì—ëŸ¬: .env íŒŒì¼ì—ì„œ HF_TOKENì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_wiki_data(wiki_path: str = WIKI_DATA_PATH) -> Dict:
    """Wikipedia ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(wiki_path) as f:
        wiki_data = json.load(f)
    return wiki_data


def get_id_to_title_mapping(wiki_data: Dict) -> Dict:
    """document_idì™€ title ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return {v["document_id"]: v["title"] for v in wiki_data.values()}


def load_train_dataset(train_set_dir: str = TRAIN_SET_DIR):
    """í•™ìŠµ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    return load_from_disk(train_set_dir)


# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_gemma(model_name: str = GEMMA_MODEL_NAME):
    """Gemma ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="cuda" if torch.cuda.is_available() else "cpu",
        torch_dtype=torch.bfloat16,
    )
    return tokenizer, model


def load_embedding_model(model_name: str = EMBEDDING_MODEL_NAME) -> HuggingFaceEmbedding:
    """ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    return HuggingFaceEmbedding(
        model_name=model_name,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )


def setup_llm_settings(model, tokenizer):
    """LlamaIndex LLM ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    gemma_llm = HuggingFaceLLM(
        model=model,
        tokenizer=tokenizer,
        context_window=8192,
    )
    Settings.llm = gemma_llm
    return gemma_llm


# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜
def create_documents_from_wiki(wiki_data: Dict) -> List[Document]:
    """Wiki ë°ì´í„°ë¡œë¶€í„° Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    documents: List[Document] = []
    for doc_id, data in wiki_data.items():
        documents.append(
            Document(
                text=data['text'],
                metadata={
                    "document_id": data['document_id'],
                    "title": data['title'],
                    "corpus_source": data['corpus_source']
                }
            )
        )
    return documents

def create_nodes_from_documents(
    documents: List[Document],
    chunk_size: int = 256,
    chunk_overlap: int = 128
) -> List[TextNode]:
    """ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ì—¬ Node ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    nodes: List[TextNode] = splitter.get_nodes_from_documents(documents)
    
    print(f"ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}ê°œ")
    print(f"ìƒì„±ëœ ì²­í¬(Node) ê°œìˆ˜: {len(nodes)}ê°œ")
    print(f"ì²« ë²ˆì§¸ ì²­í¬ í…ìŠ¤íŠ¸ ì˜ˆì‹œ: {nodes[0].get_content()[:100]}...")
    
    return nodes


# ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_faiss_vector_index(
    nodes: List[TextNode],
    embed_model: HuggingFaceEmbedding
) -> VectorStoreIndex:
    """FAISS ê¸°ë°˜ VectorStoreIndexë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    dummy_emb = embed_model.get_text_embedding("dim ì²´í¬ìš©")
    dim = len(dummy_emb)
    faiss_index = faiss.IndexFlatIP(dim)
    vector_store = FaissVectorStore(faiss_index=faiss_index)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    print("VectorStoreIndex ìƒì„± ì‹œì‘")
    vector_index = VectorStoreIndex(
        nodes,
        storage_context=storage_context,
        embed_model=embed_model,
    )
    return vector_index

# Reranker í´ë˜ìŠ¤
class Reranker:
    def __init__(self, model_name: str = RERANKER_MODEL_NAME):
        self.model = CrossEncoder(model_name, device="cuda" if torch.cuda.is_available() else "cpu")

    def rerank(self, query: str, docs: List[Dict], doc_id, top_k: int = 5) -> List[Dict]:
        """
        queryì™€ docs[{'text': ..., ...}]ë¥¼ ë°›ì•„, score ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•´ì„œ top_kë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not docs:
            return []

        pairs = [[query, d] for d in docs]
        scores = self.model.predict(pairs)  # shape (len(docs),)
        scored_docs = list(zip(docs, scores))
        scored_id = list(zip(doc_id, scores)) 

        scored_docs.sort(key=lambda x: x[1], reverse=True)
        scored_id.sort(key=lambda x: x[1], reverse=True)

        return scored_docs[:top_k], scored_id[:top_k]


def _fallback_tokenize(text: str) -> list[str]:
    """Kiwi ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ whitespace + ë¬¸ì ê¸°ë°˜ í† í°í™”"""
    # ê³µë°± ë¶„ë¦¬ + ì•ŒíŒŒë²³/ìˆ«ì/ê¸°íƒ€ ìœ ë‹ˆì½”ë“œ ë‹¨ì–´ ì¶”ì¶œ
    tokens = re.findall(r'\b\w+\b', text, re.UNICODE)
    return [t for t in tokens]

def tokenize_kiwi(
    text: str,
    kiwi: Kiwi,
    tag_include: List[str],
    text_type: str,
    top_n: int,
    score_threshold: float = 1.2,
) -> list[str]:
    try:
        # 1. í† í°í™”í•  í…ìŠ¤íŠ¸ê°€ ë¬¸ì„œ(Corpus)ì¼ ë•Œ
        if text_type == "corpus":
            # ë¬¸ì„œëŠ” ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ top_nì„ ìœ ë™ì ìœ¼ë¡œ ì„¤ì •
            analyzed = kiwi.analyze(text, top_n=top_n + len(text) // 200)

            if not analyzed:
                return _fallback_tokenize(text)

            num_candi = 1
            # 1ìœ„ ì ìˆ˜ ê¸°ì¤€ threshold ì´ë‚´ì˜ í›„ë³´êµ° ì¶”ê°€
            while (
                num_candi < len(analyzed)
                and analyzed[num_candi][1] > score_threshold * analyzed[0][1]
            ):
                num_candi += 1

        # 2. í† í°í™”í•  í…ìŠ¤íŠ¸ê°€ ì¿¼ë¦¬(Query)ì¼ ë•Œ
        elif text_type == "query":
            analyzed = kiwi.analyze(text, top_n=top_n)

            if not analyzed:
                return _fallback_tokenize(text)

            num_candi = 3 # ì¿¼ë¦¬ëŠ” ì¢€ ë” ë‹¤ì–‘í•˜ê²Œ í›„ë³´ë¥¼ ë´„

        # 3. í›„ë³´êµ°ì—ì„œ í† í° ì¶”ì¶œ
        all_tokenized = [
            (t.form, t.tag)
            for nc in range(num_candi)
            for t in analyzed[nc][0]
        ]

        # 4. ì¤‘ë³µ ì œê±°
        unique_tokenized = set(all_tokenized)

        # 5. [í•µì‹¬ ìˆ˜ì •] í•„í„°ë§ì€ í•˜ë˜, íƒœê·¸(/NNG ë“±)ëŠ” ë–¼ê³  'ë‹¨ì–´'ë§Œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŒ
        filtered = [
            form  # ğŸš¨ ìˆ˜ì •ë¨: f"{form}/{tag}" -> form
            for form, tag in unique_tokenized
            if tag in tag_include
        ]

        return filtered if filtered else _fallback_tokenize(text)

    except Exception:
        return _fallback_tokenize(text)

class RichHybridTokenizer:
    def __init__(self, gliner_model: GLiNER, labels: List[str], kiwi_tags: List[str]):
        print("ğŸ”§ Rich Hybrid Tokenizer (Raw Mode: í•„í„°ë§ ì—†ìŒ) ì´ˆê¸°í™”...")
        self.gliner = gliner_model
        self.labels = labels
        self.kiwi = Kiwi()

        # ì‚¬ìš©ìê°€ ìš”ì²­í•œ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        self.target_tags = set(kiwi_tags)

        # ğŸš¨ C++ì˜ '++' ë“±ì„ ì¡ê¸° ìœ„í•´ SW(ê¸°í˜¸)ê°€ target_tagsì— ì—†ë‹¤ë©´ ê°•ì œë¡œ ì¶”ê°€ ê¶Œì¥
        # (ì‚¬ìš©ìë‹˜ì´ ì „ë‹¬ì£¼ì‹¤ kiwi_tagsì— SWê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ì¶”ê°€)
        # ë¶™ì„í‘œë„ ì¼ë‹¨ ê°€ì ¸ì˜´ (í•„ìš” ì—†ìœ¼ë©´ ë‚˜ì¤‘ì— ë§¤í•‘ì—ì„œ None ì²˜ë¦¬)

    def tokenize(self, text: str) -> List[Dict[str, Any]]:
        if not text: return []

        token_info = {}

        # -------------------------------------------------------
        # 1. GLiNer: í•µì‹¬ ê°œì²´ëª… & ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        # -------------------------------------------------------
        try:
            preds = self.gliner.predict_entities(
                text, self.labels, flat_ner=True, threshold=0.1
            )
            for e in preds:
                raw_token = e['text'] # ê³µë°± ìœ ì§€ (ì˜ˆ: Visual Basic)
                category = e['label']

                if raw_token not in token_info:
                    token_info[raw_token] = {
                        'text': raw_token,
                        'category': category,
                        'pos': '(-)'
                    }
                else:
                    token_info[raw_token]['category'] = category

        except Exception as e:
            print(f"âš ï¸ GLiNer Error: {e}")

        # -------------------------------------------------------
        # 2. Kiwi: í˜•íƒœì†Œ ë¶„ì„ & í’ˆì‚¬ íƒœê¹…
        # -------------------------------------------------------
        try:
            res = self.kiwi.analyze(text, top_n=1)
            if res:
                for token in res[0][0]:
                    # 1. íƒ€ê²Ÿ íƒœê·¸ì¸ì§€ í™•ì¸
                    if token.tag in self.target_tags:
                        word = token.form

                        # ğŸš¨ [ì‚­ì œë¨] SW, SO ë°˜ë³µ ë¬¸ì í•„í„°ë§ ë¡œì§ ì œê±°!
                        # ì´ì œ "++", "C#", "~~~~" ëª¨ë‘ ìˆëŠ” ê·¸ëŒ€ë¡œ ë“¤ì–´ì˜µë‹ˆë‹¤.

                        # ì €ì¥ ë¡œì§
                        if word in token_info:
                            # GLiNerì™€ ê²¹ì¹˜ë©´ POS ì •ë³´ ì—…ë°ì´íŠ¸
                            token_info[word]['pos'] = token.tag
                        else:
                            # Kiwië§Œ ì°¾ì€ ë‹¨ì–´ ì¶”ê°€
                            token_info[word] = {
                                'text': word,
                                'category': None,
                                'pos': token.tag
                            }
        except Exception as e:
            print(f"âš ï¸ Kiwi Error: {e}")

        return list(token_info.values())
    

from typing import List, Dict, Any
from collections import defaultdict
import json


def create_leaf_to_middle_map(simplified_map: Dict[str, Dict[str, str]]) -> Dict[str, List[str]]:
    """ë¶„ë¥˜ ì²´ê³„ë¥¼ í‰íƒ„í™”í•˜ì—¬ (ìƒì„¸ ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ ë¦¬ìŠ¤íŠ¸) ë§µì„ ìƒì„±"""
    leaf_to_middle = defaultdict(set)
    # GLINER_TAG êµ¬ì¡°: {ì¤‘ë¶„ë¥˜: {ì½”ë“œ: ìƒì„¸ë¶„ë¥˜}}
    for middle_cat, tag_dict in simplified_map.items():
        for gliner_code, leaf_value in tag_dict.items():
            leaf_to_middle[leaf_value].add(middle_cat)
    return {k: sorted(list(v)) for k, v in leaf_to_middle.items()}

def transform_tokens_enrich_data(token_list: List[Dict[str, Any]], leaf_to_mid_map: Dict[str, List[str]]) -> List[Dict[str, Any]]:
    """
    ì…ë ¥ í† í° ë¦¬ìŠ¤íŠ¸ì— 'dict_cat'(ì¤‘ë¶„ë¥˜)ê³¼ 'dict_pos'(ì‚¬ì „ í’ˆì‚¬)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    enriched_list = []

    # ì „ì—­ ë³€ìˆ˜ KIWI_TO_URIMALSAEM_MAP ì‚¬ìš© (ìœ— ì…€ì—ì„œ ì •ì˜ë¨)
    # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ì•ˆì „ì¥ì¹˜ë¡œ get ì‚¬ìš©
    global KIWI_TO_URIMALSAEM_MAP

    for token in token_list:
        raw_text = token.get('text')
        leaf_cat = token.get('category')
        raw_pos = token.get('pos')

        # 1. ì¤‘ë¶„ë¥˜ ì¡°íšŒ (GLINER_TAG ê¸°ë°˜ ì—­ì¶”ì )
        middle_categories = leaf_to_mid_map.get(leaf_cat, [])

        # 2. í’ˆì‚¬ ë§¤í•‘ (KIWI_TO_URIMALSAEM_MAP ì‚¬ìš©)
        dict_pos = KIWI_TO_URIMALSAEM_MAP.get(raw_pos)

        new_token = {
            'text': raw_text,
            'category': leaf_cat,
            'pos': raw_pos,
            'dict_cat': middle_categories, # ì˜ˆ: ['ì¸ëª…']
            'dict_pos': dict_pos           # ì˜ˆ: 'ëª…ì‚¬'
        }
        enriched_list.append(new_token)

    return enriched_list

def load_knowledge_graph(file_path: str) -> nx.Graph:
    """pkl íŒŒì¼ì—ì„œ NetworkX ê·¸ë˜í”„ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(file_path, 'rb') as f:
            graph = pickle.load(f)
        print(f"âœ… ê·¸ë˜í”„ ë¡œë“œ ì„±ê³µ! (ë…¸ë“œ: {graph.number_of_nodes()}ê°œ, ì—£ì§€: {graph.number_of_edges()}ê°œ)")
        return graph
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

class GraphRetriever:
    def __init__(self, graph: nx.Graph, min_weight: float = 0.7):
        print("ğŸ” GraphRetriever ì´ˆê¸°í™” (Word Indexing)...")
        self.graph = graph
        self.min_weight = min_weight
        self.word_index = defaultdict(list)
        self._build_word_index()

    def _build_word_index(self):
        for node_id, data in self.graph.nodes(data=True):
            word = data.get('word')
            if word:
                self.word_index[word].append(node_id)

    def retrieve(self, enriched_tokens: List[Dict[str, Any]]) -> Dict[str, float]:
        expanded_weights = defaultdict(float)
        original_texts = set()

        for item in enriched_tokens:
            raw_text = item['text']
            dict_cats = item['dict_cat']
            dict_pos = item['dict_pos']
            raw_pos = item['pos']

            # 1. ê²€ìƒ‰ì–´ ì •ê·œí™”
            search_text = raw_text
            if raw_pos in ['VV', 'VA']:
                search_text += 'ë‹¤'

            original_texts.add(raw_text)
            original_texts.add(search_text)

            # 2. ìƒ‰ì¸ ì¡°íšŒ
            candidate_ids = self.word_index.get(search_text, [])
            valid_ids = []

            # 3. ì •ë°€ í•„í„°ë§ (ìˆ˜ì •ëœ ë¡œì§) ğŸš¨
            for nid in candidate_ids:
                node_cat = self.graph.nodes[nid].get('category')

                # Case A: GLiNERê°€ ì°¾ì•„ì¤€ ì¤‘ë¶„ë¥˜ê°€ ìˆìœ¼ë©´ -> ê°•ë ¥ í•„í„°ë§ (ìœ ì§€)
                if dict_cats:
                    if node_cat in dict_cats:
                        valid_ids.append(nid)

                # Case B: í’ˆì‚¬ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš°
                elif dict_pos:
                    # [ìˆ˜ì • í¬ì¸íŠ¸] ë™ì‚¬/í˜•ìš©ì‚¬ëŠ” ì—„ê²©í•˜ê²Œ ê²€ì‚¬
                    if dict_pos in ['ë™ì‚¬', 'í˜•ìš©ì‚¬']:
                        if node_cat == dict_pos:
                            valid_ids.append(nid)

                    # [ìˆ˜ì • í¬ì¸íŠ¸] ëª…ì‚¬(NNG/NNP) ë“±ì€ ì¹´í…Œê³ ë¦¬ ë¶ˆì¼ì¹˜ í—ˆìš©!
                    # "êµ­ê°€/ëª…ì‚¬" -> "êµ­ê°€/ì •ì¹˜" (OK!)
                    else:
                        valid_ids.append(nid)

            # 4. ìœ ì˜ì–´ í™•ì¥ (ê¸°ì¡´ ë™ì¼)
            for nid in valid_ids:
                expanded_weights[search_text] = 1.0
                for neighbor, edge in self.graph[nid].items():
                    w = edge.get('weight', 0.0)
                    if w >= self.min_weight:
                        neighbor_word = self.graph.nodes[neighbor].get('word', neighbor)
                        if neighbor_word not in original_texts:
                            expanded_weights[neighbor_word] = max(expanded_weights[neighbor_word], w)

        return dict(expanded_weights)
    
class KiwiWeightedBM25Retriever(BaseRetriever):
    """
    [í†µí•© ë²„ì „]
    1. ë¬¸ì„œëŠ” Kiwië¡œ ë¹ ë¥´ê²Œ ì¸ë±ì‹±
    2. ì§ˆë¬¸ì€ RichTokenizerê°€ ì¤€ ê°€ì¤‘ì¹˜(Dict)ë¥¼ ë°›ì•„
    3. ë‚´ë¶€ì—ì„œ ì ìˆ˜ë¥¼ ê³±í•˜ê³  ë”í•´ì„œ(Weight Sum) ê²°ê³¼ë¥¼ ë°˜í™˜
    """
    def __init__(
        self,
        nodes: List[BaseNode],
        similarity_top_k: int,
        corpus_tokenizer: Callable[[str], List[str]],         # ë¬¸ì„œëŠ” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        query_tokenizer: Callable[[str], Dict[str, float]]    # ğŸš¨ ì§ˆë¬¸ì€ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜!
    ) -> None:
        self._nodes = nodes
        self._similarity_top_k = similarity_top_k
        self._corpus_tokenizer = corpus_tokenizer
        self._query_tokenizer = query_tokenizer

        print("ğŸš€ [Index] ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘ (Kiwi)...")
        # ë¬¸ì„œëŠ” ê¸°ì¡´ëŒ€ë¡œ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¸ë±ì‹±
        corpus_tokens = [self._corpus_tokenizer(node.text) for node in nodes]

        self._bm25 = bm25s.BM25()
        self._bm25.index(corpus_tokens)

        super().__init__()

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        query_str = query_bundle.query_str

        # 1. ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§• (ì´ì œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ìŠµë‹ˆë‹¤!)
        # ì˜ˆ: {'ì‹œí–‰ì°©ì˜¤': 1.0, 'ì‹œì˜¤ë²•': 0.7}
        weighted_query = self._query_tokenizer(query_str)

        # 2. [í•µì‹¬ ë¡œì§ ì´ì‹] ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ (WeightedBM25S_Final ë¡œì§)
        doc_scores = defaultdict(float)

        # ë‹¨ì–´ í•˜ë‚˜ì”© ê²€ìƒ‰í•´ì„œ ê°€ì¤‘ì¹˜ ê³±í•´ì„œ ë”í•˜ê¸°
        for token, weight in weighted_query.items():
            try:
                # bm25sëŠ” ì…ë ¥ì„ ì´ì¤‘ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ìŒ [[token]]
                results = self._bm25.retrieve([[token]], k=len(self._nodes))
            except Exception:
                continue

            if results.documents.size == 0:
                continue

            indices = results.documents[0]
            scores = results.scores[0]

            # (BM25ì ìˆ˜ * ìš°ë¦¬ê°€ ì •í•œ ê°€ì¤‘ì¹˜) ëˆ„ì 
            for idx, score in zip(indices, scores):
                doc_scores[idx] += (score * weight)

        # 3. ì ìˆ˜ìˆœ ì •ë ¬
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)

        # 4. ìƒìœ„ kê°œë§Œ ì˜ë¼ì„œ LlamaIndex í¬ë§·(NodeWithScore)ìœ¼ë¡œ ë³€í™˜
        top_k_docs = sorted_docs[:self._similarity_top_k]

        nodes_with_scores = []
        for idx, score in top_k_docs:
            nodes_with_scores.append(
                NodeWithScore(node=self._nodes[idx], score=float(score))
            )

        return nodes_with_scores

def tokenize_query_rich(
    text: str,
    tokenizer_obj: RichHybridTokenizer,
    l2m_map_obj: Dict[str, List[str]],
    graph_retriever_obj: GraphRetriever
) -> Dict[str, float]:
    """
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°œì²´ëª… ì •ë³´ë¥¼ ì¶”ê°€í•˜ê³ , ì§€ì‹ ê·¸ë˜í”„ë¥¼ í†µí•´ ìœ ì˜ì–´ë¡œ í™•ì¥ëœ ê°€ì¤‘ì¹˜ BoWë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not text:
        return {}

    # 1. Rich Tokenizing (GLiNER + Kiwi)
    # ê²°ê³¼ ì˜ˆì‹œ: [{'text': 'ì—ì¼€ë¥´íŠ¸', 'category': 'ì¸ë¬¼_ì‚¬ëŒ', 'pos': 'NNP'}, ...]
    tokens = tokenizer_obj.tokenize(text)

    # 2. Enrich (ë©”íƒ€ë°ì´í„° ë° ì‚¬ì „ í’ˆì‚¬ ì¶”ê°€)
    # íŒŒì¼ ë‚´ ì •ì˜ëœ transform_tokens_enrich_data í•¨ìˆ˜ í˜¸ì¶œ
    enriched = transform_tokens_enrich_data(tokens, l2m_map_obj)

    # 3. Graph Expansion (ìœ ì˜ì–´ ì°¾ê¸° ë° ê°€ì¤‘ì¹˜ ê³„ì‚°)
    # ê²°ê³¼ ì˜ˆì‹œ: {'ì‹œí–‰ì°©ì˜¤': 1.0, 'ì‹œì˜¤ë²•': 0.7}
    final_bow = graph_retriever_obj.retrieve(enriched)

    # 4. Safety Net (ì›ë³¸ ë‹¨ì–´ ê°•ì œ í¬í•¨)
    # ìœ ì˜ì–´ ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ë¼ë„ ì›ë³¸ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë¼ë©´ ê°€ì¤‘ì¹˜ 1.0ìœ¼ë¡œ ì¶”ê°€
    for token in enriched:
        raw_text = token['text']
        if raw_text not in final_bow:
            final_bow[raw_text] = 1.0

    return final_bow

def setup_rich_query_tokenizer(
    gliner_model: GLiNER,
    synonym_graph: nx.Graph,
    labels: List[str],
    kiwi_tags: List[str],
    gliner_tag_map: Dict,
    min_weight: float = 0.7
) -> partial:
    """
    ëª¨ë“  ê²€ìƒ‰ ë¶€í’ˆ(ëª¨ë¸, ê·¸ë˜í”„, ë§µ)ì„ ì¡°ë¦½í•˜ì—¬ 
    í•˜ë‚˜ì˜ partial í† í¬ë‚˜ì´ì € í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # 1. ê¸°ë³¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
    tokenizer = RichHybridTokenizer(
        gliner_model=gliner_model,
        labels=labels,
        kiwi_tags=kiwi_tags
    )
    
    # 2. ë©”íƒ€ë°ì´í„° ë§¤í•‘ ìƒì„±
    l2m_map = create_leaf_to_middle_map(gliner_tag_map)
    
    # 3. ê·¸ë˜í”„ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    graph_retriever = GraphRetriever(synonym_graph, min_weight=min_weight)
    
    # 4. Partial í•¨ìˆ˜ ìƒì„± (ì‹¤ì œ ë¦¬íŠ¸ë¦¬ë²„ì— ì£¼ì…ë  í•¨ìˆ˜)
    query_tokenizer_rich = partial(
        tokenize_query_rich,
        tokenizer_obj=tokenizer,
        l2m_map_obj=l2m_map,
        graph_retriever_obj=graph_retriever
    )
    
    return query_tokenizer_rich

def create_kiwi_synonym_retriever(
    nodes: List[BaseNode],
    rich_tokenizer: RichHybridTokenizer,
    l2m_map: Dict[str, List[str]],
    graph_retriever: GraphRetriever,
    kiwi_tags: List[str] = KIWI_TAGS,
    similarity_top_k: int = 30
) -> KiwiWeightedBM25Retriever:
    """
    Kiwi, GLiNER, ì§€ì‹ ê·¸ë˜í”„ë¥¼ ê²°í•©í•œ ê°€ì¤‘ì¹˜ ê¸°ë°˜ BM25 ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # 1. ì¿¼ë¦¬ìš© í† í¬ë‚˜ì´ì € ì¡°ë¦½ (ì§ˆë¬¸ ë¶„ì„ -> í’ë¶€í™” -> ìœ ì˜ì–´ í™•ì¥)
    # ğŸš¨ Dict(ë‹¨ì–´:ê°€ì¤‘ì¹˜)ë¥¼ ë°˜í™˜í•˜ëŠ” tokenize_query_rich í•¨ìˆ˜ë¥¼ partialë¡œ ë˜í•‘í•©ë‹ˆë‹¤.
    query_tokenizer_rich = partial(
        tokenize_query_rich,
        tokenizer_obj=rich_tokenizer,
        l2m_map_obj=l2m_map,
        graph_retriever_obj=graph_retriever
    )

    # 2. ë¬¸ì„œìš© í† í¬ë‚˜ì´ì € ì¡°ë¦½ (ê¸°ì¡´ Kiwi - ë‹¨ìˆœ List[str] ë°˜í™˜)
    kiwi_instance = Kiwi()
    corpus_tokenizer = partial(
        tokenize_kiwi,
        kiwi=kiwi_instance,
        tag_include=kiwi_tags,
        text_type="corpus",
        top_n=2,
        score_threshold=1.2,
    )

    # 3. í†µí•© ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ë° ì¸ë±ì‹± ì‹œì‘
    # ë‚´ë¶€ì ìœ¼ë¡œ corpus_tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë…¸ë“œë¥¼ ì¸ë±ì‹±í•©ë‹ˆë‹¤.
    final_retriever = KiwiWeightedBM25Retriever(
        nodes=nodes,
        similarity_top_k=similarity_top_k,
        corpus_tokenizer=corpus_tokenizer,  # ë¬¸ì„œëŠ” ë¦¬ìŠ¤íŠ¸ ë°©ì‹
        query_tokenizer=query_tokenizer_rich # ì§ˆë¬¸ì€ ë”•ì…”ë„ˆë¦¬(ê°€ì¤‘ì¹˜) ë°©ì‹
    )
    
    return final_retriever

# ê²°ê³¼ ë³€í™˜ í•¨ìˆ˜
def convert_to_json(data: List) -> Dict:
    """ê²°ê³¼ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    question_id = []
    document_list = []

    for q_id, doc_list in data:
        question_id.append(q_id)
        document_list.append(list(map(int, (doc_list))))
    
    result_dict = {
        "question_id": question_id,
        "document_id": document_list
    }
    return result_dict


def save_results_to_json(data: Dict, file_path: str):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(file_path, 'w') as f:
        json.dump(data, f)
    print(f"ê²°ê³¼ê°€ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def retrieve_formatted_results(
    final_retriever: QueryFusionRetriever,
    reranker: Reranker,
    train_dataset,
    output_path: str = OUTPUT_FILE_PATH,
    rerank_top_k: int = 5
) -> Dict:
    """
    Final Retrieverì™€ Rerankerë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ ,
    ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    result_for_test = []

    for i in tqdm.tqdm(range(len(train_dataset['train']['question']))):
        # ì§ˆë¬¸ê³¼ id
        test_q_query = train_dataset['train'][i]['question']
        test_q_id = train_dataset['train'][i]['id']

        # final_retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ìˆ˜í–‰
        retrieved_nodes_test = final_retriever.retrieve(test_q_query)

        # data for reranker
        docs_for_rerank_test = [n.node.text for n in retrieved_nodes_test]
        ids_for_rerank_test = [n.node.metadata['document_id'] for n in retrieved_nodes_test]

        # rerank result
        reranked_results_test = reranker.rerank(test_q_query, docs_for_rerank_test, ids_for_rerank_test, top_k=rerank_top_k)
        result_for_test.append([test_q_id, (list(np.array(reranked_results_test[1])[:,0].astype(int)))])
    
    json_result = convert_to_json(result_for_test)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(output_path, 'w') as f:
        json.dump(json_result, f)
    print(f"ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return json_result

def initialize_rich_pipeline():
    """
    Kiwi + GLiNER + ì§€ì‹ ê·¸ë˜í”„ ìœ ì˜ì–´ í™•ì¥ì´ í¬í•¨ëœ 
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í•˜ì´ë¸Œë¦¬ë“œ í“¨ì „ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
    setup_environment()
    wiki_data = load_wiki_data()
    id_to_title = get_id_to_title_mapping(wiki_data)
    train_dataset = load_train_dataset()
    
    # 2. ë¬¸ì„œ ë° ë…¸ë“œ(ì²­í¬) ìƒì„±
    documents = create_documents_from_wiki(wiki_data)
    nodes = create_nodes_from_documents(documents)
    
    # 3. ëª¨ë¸ ë¡œë“œ (Embedding & LLM)
    embed_model = load_embedding_model()
    tokenizer_gemma, model_gemma = load_gemma()
    setup_llm_settings(model_gemma, tokenizer_gemma)
    
    # 4. GLiNER ëª¨ë¸ ë° ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ
    gliner_model_obj = GLiNER.from_pretrained("lots-o/gliner-bi-ko-xlarge-v1").to("cuda")
    synonym_graph = load_knowledge_graph(KG_FILE_PATH)
    
    # 5. Rich ë¶„ì„ ë¶€í’ˆ ì´ˆê¸°í™”
    rich_tokenizer = RichHybridTokenizer(
        gliner_model=gliner_model_obj, 
        labels=gliner_labels, 
        kiwi_tags=KIWI_TAGS
    )
    l2m_map = create_leaf_to_middle_map(GLINER_TAG)
    graph_retriever = GraphRetriever(synonym_graph, min_weight=0.7)
    
    # 6. ë²¡í„° ì¸ë±ìŠ¤ ë° Dense Retriever ìƒì„± (ì¶”ê°€ë¨)
    vector_index = create_faiss_vector_index(nodes, embed_model)
    dense_retriever = vector_index.as_retriever(similarity_top_k=50)
    
    # 7. ìœ ì˜ì–´ ê°€ì¤‘ì¹˜ BM25 Retriever ìƒì„± (Sparse)
    sparse_retriever = create_kiwi_synonym_retriever(
        nodes=nodes,
        rich_tokenizer=rich_tokenizer,
        l2m_map=l2m_map,
        graph_retriever=graph_retriever,
        kiwi_tags=KIWI_TAGS
    )
    
    # 8. ğŸš¨ í•µì‹¬ ìˆ˜ì •: ë‘ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ í•˜ë‚˜ë¡œ ê²°í•© (Fusion ë‹¨ê³„)
    final_fusion_retriever = QueryFusionRetriever(
        retrievers=[dense_retriever, sparse_retriever],
        similarity_top_k=30,          # ìµœì¢… í›„ë³´êµ° ê°œìˆ˜
        num_queries=1,                # ì¿¼ë¦¬ í™•ì¥ ë¯¸ì‚¬ìš©
        mode="reciprocal_rerank",      # RRF ë°©ì‹ ì ìš©
        use_async=False
    )
    
    # 9. Reranker ìƒì„±
    reranker = Reranker()
    
    print("âœ… ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸(Synonym Hybrid Fusion) ì´ˆê¸°í™” ì™„ë£Œ!")

    return {
        'wiki_data': wiki_data,
        'id_to_title': id_to_title,
        'train_dataset': train_dataset,
        'documents': documents,
        'nodes': nodes,
        'embed_model': embed_model,
        'vector_index': vector_index,
        'final_retriever': final_fusion_retriever, # ê²°í•©ëœ ë¦¬íŠ¸ë¦¬ë²„ ë°˜í™˜
        'reranker': reranker
    }

def main():
    """
    ì „ì²´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ì„ ê°€ë™í•©ë‹ˆë‹¤.
    1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    2. ê²€ìƒ‰ ë° ë¦¬ë­í‚¹ ìˆ˜í–‰
    3. ê²°ê³¼ ì €ì¥
    """
    # 1. ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    # initialize_rich_pipeline()ì—ì„œ ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ìŠµë‹ˆë‹¤.
    components = initialize_rich_pipeline()
    
    # 2. ê²€ìƒ‰ ìˆ˜í–‰ ë° ê²°ê³¼ JSON ì €ì¥
    # final_retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìœ ì˜ì–´ í™•ì¥ì´ ì ìš©ëœ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    json_result = retrieve_formatted_results(
        final_retriever=components['final_retriever'],
        reranker=components['reranker'],
        train_dataset=components['train_dataset'],
        output_path=OUTPUT_FILE_PATH,
        rerank_top_k=5
    )
    
    return json_result


if __name__ == "__main__":
    # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ main í•¨ìˆ˜ í˜¸ì¶œ
    print("Kiwi + NER + Synonym Hybrid ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    main()
    print("âœ¨ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
