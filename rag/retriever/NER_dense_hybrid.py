from datasets import load_from_disk
import os
from dotenv import load_dotenv  # <--- ëˆ„ë½ë¨
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from typing import List, Dict, Callable, Optional, Tuple, Any, Union 
import numpy as np
import json
import tqdm
from functools import partial

# LlamaIndex ê´€ë ¨
from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import QueryFusionRetriever, BaseRetriever
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.schema import TextNode, NodeWithScore, QueryBundle, BaseNode
from llama_index.vector_stores.faiss import FaissVectorStore
import faiss

# ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import bm25s
from gliner import GLiNER
from sentence_transformers import CrossEncoder 

# --- ì„¤ì • ìƒìˆ˜ ---
GEMMA_MODEL_NAME = "google/gemma-3-4b-it"
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
RERANKER_MODEL_NAME = "BAAI/bge-reranker-v2-m3"
WIKI_DATA_PATH = './data/wikipedia_documents.json'
TRAIN_SET_DIR = "./data/test_dataset/"
OUTPUT_FILE_PATH = './test_context_NER_dense.json'

def setup_environment():
    """í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° Hugging Face ë¡œê·¸ì¸"""
    load_dotenv()
    HF_TOKEN = os.getenv("HF_TOKEN")
    
    if HF_TOKEN:
        login(token=HF_TOKEN)
        print("Hugging Face ë¡œê·¸ì¸ ì„±ê³µ!")
    else:
        print("ì—ëŸ¬: .env íŒŒì¼ì—ì„œ HF_TOKENì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_wiki_data(wiki_path: str = WIKI_DATA_PATH) -> Dict:
    """Wikipedia ë¬¸ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(wiki_path) as f:
        wiki_data = json.load(f)
    return wiki_data


def get_id_to_title_mapping(wiki_data: Dict) -> Dict:
    """document_idì™€ title ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return {v["document_id"]: v["title"] for v in wiki_data.values()}


def load_train_dataset(train_set_dir: str = TRAIN_SET_DIR):
    """í•™ìŠµ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    return load_from_disk(train_set_dir)


# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_gemma(model_name: str = GEMMA_MODEL_NAME):
    """Gemma ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="cuda" if torch.cuda.is_available() else "cpu",
        torch_dtype=torch.bfloat16,
    )
    return tokenizer, model


def load_embedding_model(model_name: str = EMBEDDING_MODEL_NAME) -> HuggingFaceEmbedding:
    """ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    return HuggingFaceEmbedding(
        model_name=model_name,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )


def setup_llm_settings(model, tokenizer):
    """LlamaIndex LLM ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    gemma_llm = HuggingFaceLLM(
        model=model,
        tokenizer=tokenizer,
        context_window=8192,
    )
    Settings.llm = gemma_llm
    return gemma_llm


# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜
def create_documents_from_wiki(wiki_data: Dict) -> List[Document]:
    """Wiki ë°ì´í„°ë¡œë¶€í„° Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    documents: List[Document] = []
    for doc_id, data in wiki_data.items():
        documents.append(
            Document(
                text=data['text'],
                metadata={
                    "document_id": data['document_id'],
                    "title": data['title'],
                    "corpus_source": data['corpus_source']
                }
            )
        )
    return documents

def create_nodes_from_documents(
    documents: List[Document],
    chunk_size: int = 512,
    chunk_overlap: int = 50
) -> List[TextNode]:
    """ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ì—¬ Node ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    nodes: List[TextNode] = splitter.get_nodes_from_documents(documents)
    
    print(f"ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}ê°œ")
    print(f"ìƒì„±ëœ ì²­í¬(Node) ê°œìˆ˜: {len(nodes)}ê°œ")
    print(f"ì²« ë²ˆì§¸ ì²­í¬ í…ìŠ¤íŠ¸ ì˜ˆì‹œ: {nodes[0].get_content()[:100]}...")
    
    return nodes


# ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_faiss_vector_index(
    nodes: List[TextNode],
    embed_model: HuggingFaceEmbedding
) -> VectorStoreIndex:
    """FAISS ê¸°ë°˜ VectorStoreIndexë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    dummy_emb = embed_model.get_text_embedding("dim ì²´í¬ìš©")
    dim = len(dummy_emb)
    faiss_index = faiss.IndexFlatIP(dim)
    vector_store = FaissVectorStore(faiss_index=faiss_index)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    print("VectorStoreIndex ìƒì„± ì‹œì‘")
    vector_index = VectorStoreIndex(
        nodes,
        storage_context=storage_context,
        embed_model=embed_model,
    )
    return vector_index

# Reranker í´ë˜ìŠ¤
class Reranker:
    def __init__(self, model_name: str = RERANKER_MODEL_NAME):
        self.model = CrossEncoder(model_name, device="cuda" if torch.cuda.is_available() else "cpu")

    def rerank(self, query: str, docs: List[Dict], doc_id, top_k: int = 5) -> List[Dict]:
        """
        queryì™€ docs[{'text': ..., ...}]ë¥¼ ë°›ì•„, score ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•´ì„œ top_kë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not docs:
            return []

        pairs = [[query, d] for d in docs]
        scores = self.model.predict(pairs)  # shape (len(docs),)
        scored_docs = list(zip(docs, scores))
        scored_id = list(zip(doc_id, scores)) 

        scored_docs.sort(key=lambda x: x[1], reverse=True)
        scored_id.sort(key=lambda x: x[1], reverse=True)

        return scored_docs[:top_k], scored_id[:top_k]
    
# gliner ë¼ë²¨ ì •ì˜
entity_type_mapping = {
    "PS": { "PS_NAME": "ì¸ë¬¼_ì‚¬ëŒ", "PS_CHARACTER": "ì¸ë¬¼_ê°€ìƒ ìºë¦­í„°", "PS_PET": "ì¸ë¬¼_ë°˜ë ¤ë™ë¬¼"},
    "FD": { "FD_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ê³¼í•™", "FD_SOCIAL_SCIENCE": "í•™ë¬¸ ë¶„ì•¼_ì‚¬íšŒê³¼í•™", "FD_MEDICINE": "í•™ë¬¸ ë¶„ì•¼_ì˜í•™", "FD_ART": "í•™ë¬¸ ë¶„ì•¼_ì˜ˆìˆ ", "FD_HUMANITIES": "í•™ë¬¸ ë¶„ì•¼_ì¸ë¬¸í•™", "FD_OTHERS": "í•™ë¬¸ ë¶„ì•¼_ê¸°íƒ€"},
    "TR": { "TR_SCIENCE": "ì´ë¡ _ê³¼í•™", "TR_SOCIAL_SCIENCE": "ì´ë¡ _ì‚¬íšŒê³¼í•™", "TR_MEDICINE": "ì´ë¡ _ì˜í•™", "TR_ART": "ì´ë¡ _ì˜ˆìˆ ", "TR_HUMANITIES": "ì´ë¡ _ì² í•™/ì–¸ì–´/ì—­ì‚¬", "TR_OTHERS": "ì´ë¡ _ê¸°íƒ€"},
    "AF": { "AF_BUILDING": "ì¸ê³µë¬¼_ê±´ì¶•ë¬¼/í† ëª©ê±´ì„¤ë¬¼", "AF_CULTURAL_ASSET": "ì¸ê³µë¬¼_ë¬¸í™”ì¬", "AF_ROAD": "ì¸ê³µë¬¼_ë„ë¡œ/ì² ë¡œ", "AF_TRANSPORT": "ì¸ê³µë¬¼_êµí†µìˆ˜ë‹¨/ìš´ì†¡ìˆ˜ë‹¨", "AF_MUSICAL_INSTRUMENT": "ì¸ê³µë¬¼_ì•…ê¸°", "AF_WEAPON": "ì¸ê³µë¬¼_ë¬´ê¸°", "AFA_DOCUMENT": "ì¸ê³µë¬¼_ë„ì„œ/ì„œì  ì‘í’ˆëª…", "AFA_PERFORMANCE": "ì¸ê³µë¬¼_ì¶¤/ê³µì—°/ì—°ê·¹ ì‘í’ˆëª…", "AFA_VIDEO": "ì¸ê³µë¬¼_ì˜í™”/TV í”„ë¡œê·¸ë¨", "AFA_ART_CRAFT": "ì¸ê³µë¬¼_ë¯¸ìˆ /ì¡°í˜• ì‘í’ˆëª…", "AFA_MUSIC": "ì¸ê³µë¬¼_ìŒì•… ì‘í’ˆëª…", "AFW_SERVICE_PRODUCTS": "ì¸ê³µë¬¼_ì„œë¹„ìŠ¤ ìƒí’ˆ", "AFW_OTHER_PRODUCTS": "ì¸ê³µë¬¼_ê¸°íƒ€ ìƒí’ˆ"},
    "OG": { "OGG_ECONOMY": "ê¸°ê´€_ê²½ì œ", "OGG_EDUCATION": "ê¸°ê´€_êµìœ¡", "OGG_MILITARY": "ê¸°ê´€_êµ°ì‚¬", "OGG_MEDIA": "ê¸°ê´€_ë¯¸ë””ì–´", "OGG_SPORTS": "ê¸°ê´€_ìŠ¤í¬ì¸ ", "OGG_ART": "ê¸°ê´€_ì˜ˆìˆ ", "OGG_MEDICINE": "ê¸°ê´€_ì˜ë£Œ", "OGG_RELIGION": "ê¸°ê´€_ì¢…êµ", "OGG_SCIENCE": "ê¸°ê´€_ê³¼í•™", "OGG_LIBRARY": "ê¸°ê´€_ë„ì„œê´€", "OGG_LAW": "ê¸°ê´€_ë²•ë¥ ", "OGG_POLITICS": "ê¸°ê´€_ì •ë¶€/ê³µê³µ", "OGG_FOOD": "ê¸°ê´€_ìŒì‹ ì—…ì²´", "OGG_HOTEL": "ê¸°ê´€_ìˆ™ë°• ì—…ì²´", "OGG_OTHERS": "ê¸°ê´€_ê¸°íƒ€"},
    "LC": { "LCP_COUNTRY": "ì¥ì†Œ_êµ­ê°€", "LCP_PROVINCE": "ì¥ì†Œ_ë„/ì£¼ ì§€ì—­", "LCP_COUNTY": "ì¥ì†Œ_ì„¸ë¶€ í–‰ì •êµ¬ì—­", "LCP_CITY": "ì¥ì†Œ_ë„ì‹œ", "LCP_CAPITALCITY": "ì¥ì†Œ_ìˆ˜ë„", "LCG_RIVER": "ì¥ì†Œ_ê°•/í˜¸ìˆ˜", "LCG_OCEAN": "ì¥ì†Œ_ë°”ë‹¤", "LCG_BAY": "ì¥ì†Œ_ë°˜ë„/ë§Œ", "LCG_MOUNTAIN": "ì¥ì†Œ_ì‚°/ì‚°ë§¥", "LCG_ISLAND": "ì¥ì†Œ_ì„¬", "LCG_CONTINENT": "ì¥ì†Œ_ëŒ€ë¥™", "LC_SPACE": "ì¥ì†Œ_ì²œì²´", "LC_OTHERS": "ì¥ì†Œ_ê¸°íƒ€"},
    "CV": { "CV_CULTURE": "ë¬¸ëª…_ë¬¸ëª…/ë¬¸í™”", "CV_TRIBE": "ë¬¸ëª…_ë¯¼ì¡±/ì¢…ì¡±", "CV_LANGUAGE": "ë¬¸ëª…_ì–¸ì–´", "CV_POLICY": "ë¬¸ëª…_ì œë„/ì •ì±…", "CV_LAW": "ë¬¸ëª…_ë²•/ë²•ë¥ ", "CV_CURRENCY": "ë¬¸ëª…_í†µí™”", "CV_TAX": "ë¬¸ëª…_ì¡°ì„¸", "CV_FUNDS": "ë¬¸ëª…_ì—°ê¸ˆ/ê¸°ê¸ˆ", "CV_ART": "ë¬¸ëª…_ì˜ˆìˆ ", "CV_SPORTS": "ë¬¸ëª…_ìŠ¤í¬ì¸ ", "CV_SPORTS_POSITION": "ë¬¸ëª…_ìŠ¤í¬ì¸  í¬ì§€ì…˜", "CV_SPORTS_INST": "ë¬¸ëª…_ìŠ¤í¬ì¸  ìš©í’ˆ/ë„êµ¬", "CV_PRIZE": "ë¬¸ëª…_ìƒ/í›ˆì¥", "CV_RELATION": "ë¬¸ëª…_ê°€ì¡±/ì¹œì¡± ê´€ê³„", "CV_OCCUPATION": "ë¬¸ëª…_ì§ì—…", "CV_POSITION": "ë¬¸ëª…_ì§ìœ„/ì§ì±…", "CV_FOOD": "ë¬¸ëª…_ìŒì‹", "CV_DRINK": "ë¬¸ëª…_ìŒë£Œ/ìˆ ", "CV_FOOD_STYLE": "ë¬¸ëª…_ìŒì‹ ìœ í˜•", "CV_CLOTHING": "ë¬¸ëª…_ì˜ë³µ/ì„¬ìœ ", "CV_BUILDING_TYPE": "ë¬¸ëª…_ê±´ì¶• ì–‘ì‹"},
    "DT": { "DT_DURATION": "ë‚ ì§œ_ê¸°ê°„", "DT_DAY": "ë‚ ì§œ_ì¼", "DT_WEEK": "ë‚ ì§œ_ì£¼(ì£¼ì°¨)", "DT_MONTH": "ë‚ ì§œ_ë‹¬(ì›”)", "DT_YEAR": "ë‚ ì§œ_ì—°(ë…„)", "DT_SEASON": "ë‚ ì§œ_ê³„ì ˆ", "DT_GEOAGE": "ë‚ ì§œ_ì§€ì§ˆì‹œëŒ€", "DT_DYNASTY": "ë‚ ì§œ_ì™•ì¡°ì‹œëŒ€", "DT_OTHERS": "ë‚ ì§œ_ê¸°íƒ€"},
    "TI": { "TI_DURATION": "ì‹œê°„_ê¸°ê°„", "TI_HOUR": "ì‹œê°„_ì‹œê°(ì‹œ)", "TI_MINUTE": "ì‹œê°„_ë¶„", "TI_SECOND": "ì‹œê°„_ì´ˆ", "TI_OTHERS": "ì‹œê°„_ê¸°íƒ€"},
    "QT": { "QT_AGE": "ìˆ˜ëŸ‰_ë‚˜ì´", "QT_SIZE": "ìˆ˜ëŸ‰_ë„“ì´/ë©´ì ", "QT_LENGTH": "ìˆ˜ëŸ‰_ê¸¸ì´/ê±°ë¦¬", "QT_COUNT": "ìˆ˜ëŸ‰_ìˆ˜ëŸ‰/ë¹ˆë„", "QT_MAN_COUNT": "ìˆ˜ëŸ‰_ì¸ì›ìˆ˜", "QT_WEIGHT": "ìˆ˜ëŸ‰_ë¬´ê²Œ", "QT_PERCENTAGE": "ìˆ˜ëŸ‰_ë°±ë¶„ìœ¨", "QT_SPEED": "ìˆ˜ëŸ‰_ì†ë„", "QT_TEMPERATURE": "ìˆ˜ëŸ‰_ì˜¨ë„", "QT_VOLUME": "ìˆ˜ëŸ‰_ë¶€í”¼", "QT_ORDER": "ìˆ˜ëŸ‰_ìˆœì„œ", "QT_PRICE": "ìˆ˜ëŸ‰_ê¸ˆì•¡", "QT_PHONE": "ìˆ˜ëŸ‰_ì „í™”ë²ˆí˜¸", "QT_SPORTS": "ìˆ˜ëŸ‰_ìŠ¤í¬ì¸  ìˆ˜ëŸ‰", "QT_CHANNEL": "ìˆ˜ëŸ‰_ì±„ë„ ë²ˆí˜¸", "QT_ALBUM": "ìˆ˜ëŸ‰_ì•¨ë²” ìˆ˜ëŸ‰", "QT_ADDRESS": "ìˆ˜ëŸ‰_ì£¼ì†Œ ê´€ë ¨ ìˆ«ì", "QT_OTHERS": "ìˆ˜ëŸ‰_ê¸°íƒ€ ìˆ˜ëŸ‰"},
    "EV": { "EV_ACTIVITY": "ì‚¬ê±´_ì‚¬íšŒìš´ë™/ì„ ì–¸", "EV_WAR_REVOLUTION": "ì‚¬ê±´_ì „ìŸ/í˜ëª…", "EV_SPORTS": "ì‚¬ê±´_ìŠ¤í¬ì¸  í–‰ì‚¬", "EV_FESTIVAL": "ì‚¬ê±´_ì¶•ì œ/ì˜í™”ì œ", "EV_OTHERS": "ì‚¬ê±´_ê¸°íƒ€"},
    "AM": { "AM_INSECT": "ë™ë¬¼_ê³¤ì¶©", "AM_BIRD": "ë™ë¬¼_ì¡°ë¥˜", "AM_FISH": "ë™ë¬¼_ì–´ë¥˜", "AM_MAMMALIA": "ë™ë¬¼_í¬ìœ ë¥˜", "AM_AMPHIBIA": "ë™ë¬¼_ì–‘ì„œë¥˜", "AM_REPTILIA": "ë™ë¬¼_íŒŒì¶©ë¥˜", "AM_TYPE": "ë™ë¬¼_ë¶„ë¥˜ëª…", "AM_PART": "ë™ë¬¼_ë¶€ìœ„ëª…", "AM_OTHERS": "ë™ë¬¼_ê¸°íƒ€"},
    "PT": { "PT_FRUIT": "ì‹ë¬¼_ê³¼ì¼/ì—´ë§¤", "PT_FLOWER": "ì‹ë¬¼_ê½ƒ", "PT_TREE": "ì‹ë¬¼_ë‚˜ë¬´", "PT_GRASS": "ì‹ë¬¼_í’€", "PT_TYPE": "ì‹ë¬¼_ë¶„ë¥˜ëª…", "PT_PART": "ì‹ë¬¼_ë¶€ìœ„ëª…", "PT_OTHERS": "ì‹ë¬¼_ê¸°íƒ€"},
    "MT": { "MT_ELEMENT": "ë¬¼ì§ˆ_ì›ì†Œ", "MT_METAL": "ë¬¼ì§ˆ_ê¸ˆì†", "MT_ROCK": "ë¬¼ì§ˆ_ì•”ì„", "MT_CHEMICAL": "ë¬¼ì§ˆ_í™”í•™"},
    "TM": { "TM_COLOR": "ìš©ì–´_ìƒ‰ê¹”", "TM_DIRECTION": "ìš©ì–´_ë°©í–¥", "TM_CLIMATE": "ìš©ì–´_ê¸°í›„ ì§€ì—­", "TM_SHAPE": "ìš©ì–´_ëª¨ì–‘/í˜•íƒœ", "TM_CELL_TISSUE_ORGAN": "ìš©ì–´_ì„¸í¬/ì¡°ì§/ê¸°ê´€", "TMM_DISEASE": "ìš©ì–´_ì¦ìƒ/ì§ˆë³‘", "TMM_DRUG": "ìš©ì–´_ì•½í’ˆ", "TMI_HW": "ìš©ì–´_IT í•˜ë“œì›¨ì–´", "TMI_SW": "ìš©ì–´_IT ì†Œí”„íŠ¸ì›¨ì–´", "TMI_SITE": "ìš©ì–´_URL ì£¼ì†Œ", "TMI_EMAIL": "ìš©ì–´_ì´ë©”ì¼ ì£¼ì†Œ", "TMI_MODEL": "ìš©ì–´_ì œí’ˆ ëª¨ë¸ëª…", "TMI_SERVICE": "ìš©ì–´_IT ì„œë¹„ìŠ¤", "TMI_PROJECT": "ìš©ì–´_í”„ë¡œì íŠ¸", "TMIG_GENRE": "ìš©ì–´_ê²Œì„ ì¥ë¥´", "TM_SPORTS": "ìš©ì–´_ìŠ¤í¬ì¸ "},
}
labels = []
for main_category in entity_type_mapping:
    sub_dict = entity_type_mapping[main_category]
    for key in sub_dict:
        labels.append(sub_dict[key])

# GLiNer XL ëª¨ë¸ ë¡œë“œ
gliner_model = GLiNER.from_pretrained("lots-o/gliner-bi-ko-xlarge-v1")
gliner_model = gliner_model.to("cuda")


# gliner ê¸°ë°˜ í† í¬ë‚˜ì´ì €
def tokenize_gliner_batch(
    texts: List[str],  # ğŸš¨ ì…ë ¥ì´ ë‹¨ì¼ strì´ ì•„ë‹ˆë¼ List[str]ì…ë‹ˆë‹¤!
    model: GLiNER, 
    labels: List[str], 
    label_chunk_size: int = 20,
    score_threshold_ratio: float = 1.05
) -> List[List[str]]:
    if not texts: return []

    # ê²°ê³¼ ì €ì¥ìš© (ë¬¸ì„œ ê°œìˆ˜ë§Œí¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
    batch_results = [[] for _ in texts]
    
    # ë¼ë²¨ ë°°ì¹˜ ì²˜ë¦¬ (Label Chunking)
    for i in range(0, len(labels), label_chunk_size):
        sub_labels = labels[i : i + label_chunk_size]
        try:
            # ğŸš€ model.batch_predict_entities ì‚¬ìš© (ì†ë„ í–¥ìƒì˜ í•µì‹¬)
            batch_preds = model.batch_predict_entities(
                texts, sub_labels, flat_ner=True, threshold=0.1
            )
            # ê²°ê³¼ ë³‘í•©
            for doc_idx, entities in enumerate(batch_preds):
                batch_results[doc_idx].extend(entities)
        except Exception: continue
    
    # í›„ì²˜ë¦¬ (ê° ë¬¸ì„œë³„ë¡œ ì ìˆ˜ í•„í„° & ë¬¸ì¥ ìª¼ê°œê¸° ì ìš©)
    final_batch_tokens = []
    
    for entities in batch_results:
        if not entities:
            final_batch_tokens.append([])
            continue
            
        # 1. ì ìˆ˜ í•„í„°ë§ (Relative Threshold)
        max_score = max(e['score'] for e in entities)
        cutoff_score = max_score / score_threshold_ratio
        filtered = [e for e in entities if e['score'] >= cutoff_score]
        
        # 2. ìŠ¤ë§ˆíŠ¸ í•„í„° (ë¬¸ì¥ ìª¼ê°œê¸°)
        doc_tokens = set()
        for e in filtered:
            token_text = e['text']
            # ë„ì–´ì“°ê¸° 2ê°œ ì´ìƒ(3ì–´ì ˆ)ì´ë©´ ìª¼ê°œê¸°
            if token_text.count(' ') >= 2:
                for t in token_text.split():
                    doc_tokens.add(t)
            else:
                doc_tokens.add(token_text)
        
        final_batch_tokens.append(list(doc_tokens))
        
    return final_batch_tokens

# gliner bm25s ë¦¬íŠ¸ë¦¬ë²„
class GLiNerBM25Retriever(BaseRetriever):
    """partial í† í¬ë‚˜ì´ì €ë¥¼ ë°›ì•„ì„œ ë°°ì¹˜ ì²˜ë¦¬í•˜ëŠ” Retriever"""
    def __init__(
        self,
        nodes: List[BaseNode],
        tokenizer: Callable[[List[str]], List[List[str]]], # ğŸš¨ ë°°ì¹˜ í† í¬ë‚˜ì´ì € ì‹œê·¸ë‹ˆì²˜
        similarity_top_k: int = 30,
        doc_batch_size: int = 64 # ğŸš€ ë¬¸ì„œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ (í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜)
    ) -> None:
        
        self._nodes = nodes
        self._similarity_top_k = similarity_top_k
        self._tokenizer = tokenizer
        self.doc_batch_size = doc_batch_size

        print(f"ğŸš€ GLiNer Batch Indexing... Docs: {len(nodes)}, Batch: {doc_batch_size}")
        
        corpus_tokens = []
        
        # ë¬¸ì„œë¥¼ ë­‰í……ì´(Batch)ë¡œ ì˜ë¼ì„œ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ í˜¸ì¶œ
        for i in tqdm.tqdm(range(0, len(nodes), doc_batch_size), desc="Indexing"):
            batch_nodes = nodes[i : i + doc_batch_size]
            batch_texts = [n.text if n.text else "" for n in batch_nodes]
            
            # ì—¬ê¸°ì„œ partialë¡œ ë§Œë“  í•¨ìˆ˜ì— 'ë¦¬ìŠ¤íŠ¸'ë¥¼ ë˜ì§‘ë‹ˆë‹¤!
            batch_tokens_list = self._tokenizer(batch_texts)
            corpus_tokens.extend(batch_tokens_list)

        self._bm25 = bm25s.BM25()
        self._bm25.index(corpus_tokens)
        
        print("âœ… Indexing Complete!")
        super().__init__()

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        query = query_bundle.query_str
        
        # ì¿¼ë¦¬ëŠ” 1ê°œì§€ë§Œ, ë°°ì¹˜ í•¨ìˆ˜ë‹ˆê¹Œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë³´ëƒ„
        # ê²°ê³¼ë„ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë‹ˆê¹Œ [0]ìœ¼ë¡œ êº¼ëƒ„
        query_tokens = self._tokenizer([query])[0]
        
        if not query_tokens: return []
        
        tokenized_query = [query_tokens]
        actual_k = min(self._similarity_top_k, len(self._nodes))
        if actual_k == 0: return []

        results, scores = self._bm25.retrieve(tokenized_query, k=actual_k)
        
        nodes_with_scores: List[NodeWithScore] = []
        for idx, score in zip(results[0], scores[0]):
            if score > 0:
                nodes_with_scores.append(
                    NodeWithScore(node=self._nodes[idx], score=float(score))
                )
        return nodes_with_scores


def create_gliner_tokenizers(
    model: GLiNER,  # GLiNER ëª¨ë¸ ê°ì²´
    labels: List[str]
) -> partial:
    """
    GLiNER ëª¨ë¸ê³¼ ë¼ë²¨ì„ ê³ ì •í•œ partial í† í¬ë‚˜ì´ì € íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    # 1. Corpusìš© í† í¬ë‚˜ì´ì € (ì¸ë±ì‹±ìš©: ë³´í†µ ë” ì—„ê²©í•˜ê±°ë‚˜ ë„ë„í•˜ê²Œ ì¡°ì ˆ)
    gliner_batch_tokenizer = partial(
        tokenize_gliner_batch,
        model=model,
        labels=labels,
        label_chunk_size=50,        # ë¼ë²¨ 20ê°œì”© ëŠê¸°
        score_threshold_ratio=2.5  # ì ìˆ˜ í•„í„°
)

    return gliner_batch_tokenizer


def create_gliner_fusion_retriever(
    vector_index: VectorStoreIndex,
    nodes: List[TextNode],
    gliner_tokenizer: Callable,
    vector_top_k: int = 50,
    bm25_top_k: int = 30,
    fusion_top_k: int = 30,
    doc_batch_size: int = 8
) -> QueryFusionRetriever:
    """
    Vector Retrieverì™€ GLiNerBM25Retrieverë¥¼ ê²°í•©í•œ Fusion Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # 1. ê¸°ë³¸ Vector Retriever ìƒì„±
    vector_retriever = vector_index.as_retriever(similarity_top_k=vector_top_k)
    
    # 2. GLiNER ê¸°ë°˜ BM25 Retriever ìƒì„±
    gliner_bm25_retriever = GLiNerBM25Retriever(
        nodes=nodes,
        similarity_top_k=bm25_top_k,
        tokenizer=gliner_tokenizer,
        doc_batch_size=doc_batch_size 
    )
    
    # 3. Reciprocal Rerankë¥¼ ì´ìš©í•œ Fusion Retriever êµ¬ì„±
    fusion_retriever = QueryFusionRetriever(
        retrievers=[vector_retriever, gliner_bm25_retriever],
        similarity_top_k=fusion_top_k,
        num_queries=1,           # í˜„ì¬ëŠ” ì¿¼ë¦¬ í™•ì¥ ì—†ì´ 1ê°œë§Œ ì‚¬ìš©
        use_async=False,         # ë¡œì»¬ í™˜ê²½ì´ë‚˜ ë””ë²„ê¹… ì‹œ False ê¶Œì¥
        mode="reciprocal_rerank" # RRF ë°©ì‹ ì ìš©
    )
    
    return fusion_retriever

def convert_to_json(data: List) -> Dict:
    """ê²°ê³¼ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    question_id = []
    document_list = []

    for q_id, doc_list in data:
        question_id.append(q_id)
        document_list.append(list(map(int, (doc_list))))
    
    result_dict = {
        "question_id": question_id,
        "document_id": document_list
    }
    return result_dict


def save_results_to_json(data: Dict, file_path: str):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open(file_path, 'w') as f:
        json.dump(data, f)
    print(f"ê²°ê³¼ê°€ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def retrieve_formatted_results(
    fusion_retriever: QueryFusionRetriever,
    reranker: Reranker,
    train_dataset,
    output_path: str = OUTPUT_FILE_PATH,
    rerank_top_k: int = 5
) -> Dict:
    """
    Fusion Retrieverì™€ Rerankerë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ ,
    ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        fusion_retriever: QueryFusionRetriever ê°ì²´
        reranker: Reranker ê°ì²´
        train_dataset: í•™ìŠµ ë°ì´í„°ì…‹
        output_path: ê²°ê³¼ JSON íŒŒì¼ ì €ì¥ ê²½ë¡œ
        rerank_top_k: Reranking í›„ ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ê°œìˆ˜
    
    Returns:
        Dict: {"question_id": [...], "document_id": [...]} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
    """
    result_for_test = []

    for i in tqdm.tqdm(range(len(train_dataset['train']['question']))):
        # ì§ˆë¬¸ê³¼ id
        test_q_query = train_dataset['train'][i]['question']
        test_q_id = train_dataset['train'][i]['id']

        retrieved_nodes_test = fusion_retriever.retrieve(test_q_query)

        # data for reranker
        docs_for_rerank_test = [n.node.text for n in retrieved_nodes_test]
        ids_for_rerank_test = [n.node.metadata['document_id'] for n in retrieved_nodes_test]

        # rerank result
        reranked_results_test = reranker.rerank(test_q_query, docs_for_rerank_test, ids_for_rerank_test, top_k=rerank_top_k)
        result_for_test.append([test_q_id, (list(np.array(reranked_results_test[1])[:,0].astype(int)))])
    
    json_result = convert_to_json(result_for_test)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(output_path, 'w') as f:
        json.dump(json_result, f)
    print(f"ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return json_result


def initialize_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ê³  í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # 1~4. í™˜ê²½ ë° ë°ì´í„° ì„¤ì • (íŒŒì¼ ë‚´ ì •ì˜ëœ í•¨ìˆ˜ë“¤ ì‚¬ìš©)
    setup_environment()
    wiki_data = load_wiki_data()
    id_to_title = get_id_to_title_mapping(wiki_data)
    train_dataset = load_train_dataset()
    documents = create_documents_from_wiki(wiki_data)
    nodes = create_nodes_from_documents(documents)
    
    embed_model = load_embedding_model()
    tokenizer, model = load_gemma()
    setup_llm_settings(model, tokenizer)
    
    vector_index = create_faiss_vector_index(nodes, embed_model)

    # 5. GLiNER í† í¬ë‚˜ì´ì € ìƒì„± (íŒŒì¼ í•˜ë‹¨ì— ì •ì˜ëœ í•¨ìˆ˜ í˜¸ì¶œ)
    # íŒŒì¼ ë‚´ ì „ì—­ ë³€ìˆ˜ì¸ gliner_modelê³¼ labelsë¥¼ ì¸ìë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    gliner_tokenizer = create_gliner_tokenizers(
        model=gliner_model, 
        labels=labels
    )
    
    # 6. GLiNER Fusion Retriever ìƒì„± (íŒŒì¼ í•˜ë‹¨ì— ì •ì˜ëœ í•¨ìˆ˜ í˜¸ì¶œ)
    fusion_retriever = create_gliner_fusion_retriever(
        vector_index=vector_index,
        nodes=nodes,
        gliner_tokenizer=gliner_tokenizer
    )
    
    # 7. Reranker ìƒì„±
    reranker = Reranker()
    
    return {
        'wiki_data': wiki_data,
        'id_to_title': id_to_title,
        'train_dataset': train_dataset,
        'documents': documents,
        'nodes': nodes,
        'embed_model': embed_model,
        'vector_index': vector_index,
        'fusion_retriever': fusion_retriever,
        'reranker': reranker
    }

def main():
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    components = initialize_pipeline()
    
    # ê²€ìƒ‰ ìˆ˜í–‰ ë° ê²°ê³¼ JSON ì €ì¥
    json_result = retrieve_formatted_results(
        fusion_retriever=components['fusion_retriever'],
        reranker=components['reranker'],
        train_dataset=components['train_dataset'],
        output_path=OUTPUT_FILE_PATH,
        rerank_top_k=5
    )
    
    return json_result


if __name__ == "__main__":
    main()