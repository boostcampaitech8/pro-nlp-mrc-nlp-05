{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8Vz6oxhLAFk4"},"outputs":[],"source":["# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ (ë°ì´í„° ë¡œë“œìš©)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (A100ì€ ë¹ ë¥´ë‹ˆê¹Œ ê¸ˆë°© ë©ë‹ˆë‹¤)\n","!pip install -q gliner kiwipiepy bm25s networkx scipy sentence-transformers llama-index llama-index-embeddings-huggingface llama-index-llms-huggingface python-mecab-ko faiss-cpu llama-index-vector-stores-faiss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzBDpGunAk1g"},"outputs":[],"source":["# ì½”ë©ì—ì„œ ì‚¬ìš©í•˜ì‹œë©´ ì•„ë˜ ì£¼ì„ í•´ì œí•´ ì£¼ì„¸ìš”\n","# import os\n","# # ğŸ›‘ [ì¤‘ìš”] ìš•ì‹¬ìŸì´ í”„ë ˆì„ì›Œí¬ë“¤ì˜ ë©”ëª¨ë¦¬ ì ìœ  ë°©ì§€ (Import ì „ì— ì‹¤í–‰ í•„ìˆ˜!)\n","# os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"       # TensorFlowê°€ í•„ìš”í•  ë•Œë§Œ ë©”ëª¨ë¦¬ ì“°ê²Œ í•¨\n","# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # JAXê°€ ë©”ëª¨ë¦¬ ë¯¸ë¦¬ ì°œí•˜ëŠ” ê±° ê¸ˆì§€\n","# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".10\"   # í˜¹ì‹œ ì“°ë”ë¼ë„ 10%ë§Œ ì“°ê²Œ ì œí•œ\n","\n","from datasets import load_from_disk\n","from huggingface_hub import login\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from typing import List, Dict, Callable, Optional\n","import numpy as np\n","import json\n","import tqdm\n","from functools import partial\n","\n","# LlamaIndex ê´€ë ¨\n","from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext\n","from llama_index.core.node_parser import SentenceSplitter\n","from llama_index.core.retrievers import QueryFusionRetriever, BaseRetriever\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.llms.huggingface import HuggingFaceLLM\n","from llama_index.core.schema import TextNode, NodeWithScore, QueryBundle, BaseNode\n","from llama_index.vector_stores.faiss import FaissVectorStore\n","import faiss\n","\n","# ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n","import bm25s\n","from gliner import GLiNER\n","from sentence_transformers import CrossEncoder\n","\n","# Hugging Face ë¡œê·¸ì¸\n","print(\"ğŸš€ í™˜ê²½ ì„¤ì • ì‹œì‘!\")\n","HF_TOKEN = \"\"\n","os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = HF_TOKEN\n","login(token=HF_TOKEN)\n","\n","print(f\"GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZaXZKdM7AoZQ"},"outputs":[],"source":["# wiki data load\n","with open('/data/wikipedia_documents.json') as f:\n","    wiki_data = json.load(f)\n","id_to_title = {v[\"document_id\"]: v[\"title\"] for v in wiki_data.values()}\n","\n","\n","train_set_dir = \"/data/train_dataset/\"\n","dataset = load_from_disk(train_set_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7gvZR2zA_EB"},"outputs":[],"source":["# --- ëª¨ë¸ ë¡œë“œ ì„¤ì • ---\n","GEMMA_MODEL_NAME = \"google/gemma-3-4b-it\"  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ 4b ëŒ€ì‹  9bë¥¼ ì˜ˆì‹œë¡œ ì‚¬ìš© (ì‚¬ìš©ì í™˜ê²½ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥)\n","EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n","RERANKER_MODEL_NAME = \"BAAI/bge-reranker-v2-m3\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdkMIkP1BC03"},"outputs":[],"source":["# --- LLM ë° Tokenizer ë¡œë“œ ---\n","def load_gemma():\n","    \"\"\"Gemma ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n","    # Q: Gemma 3-4b-it ì‚¬ìš© ì˜ˆì •ì´ì—ˆëŠ”ë°, í˜„ì¬ëŠ” Gemma 2-9b-itì„ ì‚¬ìš©í•˜ë ¤ í•©ë‹ˆë‹¤.\n","    # A: VRAM ìƒí™©ì— ë”°ë¼ ëª¨ë¸ ì´ë¦„ì„ ì ì ˆíˆ ë³€ê²½í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.\n","    tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        GEMMA_MODEL_NAME,\n","        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","        torch_dtype=torch.bfloat16,\n","    )\n","    return tokenizer, model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQuGZzJ_BE6T"},"outputs":[],"source":["documents: List[Document] = []\n","for doc_id, data in wiki_data.items():\n","    # 'text' í•„ë“œë¥¼ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©\n","    documents.append(\n","        Document(\n","            text=data['text'],\n","            metadata={\n","                \"document_id\": data['document_id'],\n","                \"title\": data['title'],\n","                \"corpus_source\": data['corpus_source']\n","            }\n","        )\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hj3ZwZRWBLCq"},"outputs":[],"source":["from llama_index.core import Document\n","from llama_index.core.node_parser import SentenceSplitter\n","import json\n","\n","# 3. ë¬¸ì„œ ì²­í‚¹ (Node ìƒì„±)\n","# SentenceSplitterëŠ” ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©´ì„œ,\n","# ìµœì¢… ì²­í¬ í¬ê¸°ë¥¼ chunk_size=512ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n","splitter = SentenceSplitter(chunk_size=256, chunk_overlap=128)\n","\n","# nodesì—ëŠ” ì‘ì€ í…ìŠ¤íŠ¸ ì²­í¬(TextNode)ë“¤ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë‹´ê¹ë‹ˆë‹¤.\n","nodes: List[TextNode] = splitter.get_nodes_from_documents(documents)\n","\n","print(f\"ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}ê°œ\")\n","print(f\"ìƒì„±ëœ ì²­í¬(Node) ê°œìˆ˜: {len(nodes)}ê°œ\")\n","print(f\"ì²« ë²ˆì§¸ ì²­í¬ í…ìŠ¤íŠ¸ ì˜ˆì‹œ: {nodes[0].get_content()[:100]}...\")\n","\n","\n","# ğŸš¨ [ì¤‘ìš”] ì´ë²ˆì—” ê¹Œë¨¹ì§€ ë§ê³  ë°”ë¡œ ì €ì¥í•©ì‹œë‹¤!\n","save_path = \"/my_search_index\"\n","os.makedirs(save_path, exist_ok=True)\n","\n","with open(f\"{save_path}/nodes.pkl\", \"wb\") as f:\n","    pickle.dump(nodes, f)\n","\n","print(\"ğŸ’¾ Nodes íŒŒì¼ ì˜êµ¬ ì €ì¥ ì™„ë£Œ! (ì´ì œ êº¼ì ¸ë„ ì•ˆì „í•¨)\")"]},{"cell_type":"code","source":["import pickle\n","\n","# ğŸš¨ [ì¤‘ìš”] ì´ë²ˆì—” ê¹Œë¨¹ì§€ ë§ê³  ë°”ë¡œ ì €ì¥í•©ì‹œë‹¤!\n","save_path = \"my_search_index\"\n","os.makedirs(save_path, exist_ok=True)\n","\n","with open(f\"{save_path}/nodes.pkl\", \"wb\") as f:\n","    pickle.dump(nodes, f)\n","\n","print(\"ğŸ’¾ Nodes íŒŒì¼ ì˜êµ¬ ì €ì¥ ì™„ë£Œ! (ì´ì œ êº¼ì ¸ë„ ì•ˆì „í•¨)\")"],"metadata":{"id":"0iq44797v1pO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import os\n","from google.colab import drive\n","\n","\n","# 2. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","save_path = \"my_search_index\"\n","node_file_path = os.path.join(save_path, \"nodes.pkl\")\n","\n","# 3. ë¶ˆëŸ¬ì˜¤ê¸° (Load)\n","if os.path.exists(node_file_path):\n","    print(f\"ğŸ“‚ '{node_file_path}' íŒŒì¼ì„ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...\")\n","\n","    with open(node_file_path, \"rb\") as f:\n","        nodes = pickle.load(f)\n","\n","    print(f\"âœ… ë¡œë“œ ì™„ë£Œ! ì´ {len(nodes)}ê°œì˜ ì²­í¬(Node)ê°€ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n","    # 4. ì˜ ë¶ˆëŸ¬ì™€ì¡ŒëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸ (ì²« ë²ˆì§¸ ë…¸ë“œ ë‚´ìš©)\n","    # (ì—ëŸ¬ ì—†ì´ ë‚´ìš©ì´ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤!)\n","    try:\n","        print(f\"ğŸ‘€ ì²« ë²ˆì§¸ ë…¸ë“œ ë‚´ìš©: {nodes[0].text[:50]}...\")\n","    except:\n","        print(\"ğŸ‘€ ë‚´ìš© í™•ì¸ ì‹¤íŒ¨ (í˜•ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)\")\n","\n","else:\n","    print(f\"ğŸ˜± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”: {node_file_path}\")"],"metadata":{"id":"moLZ_PyLNqyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LM1PW5mLBND1"},"outputs":[],"source":["embed_model = HuggingFaceEmbedding(\n","    model_name=EMBEDDING_MODEL_NAME,\n","    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    embed_batch_size = 512\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fgN0R1oBOxu"},"outputs":[],"source":["# faiss vs\n","\n","dummy_emb = embed_model.get_text_embedding(\"dim ì²´í¬ìš©\")\n","dim = len(dummy_emb)\n","faiss_index = faiss.IndexFlatIP(dim)\n","vector_store = FaissVectorStore(faiss_index=faiss_index)\n","storage_context = StorageContext.from_defaults(vector_store=vector_store)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LNyD1FsBPlE"},"outputs":[],"source":["vector_index = VectorStoreIndex(\n","    nodes,\n","    storage_context=storage_context,\n","    embed_model=embed_model,\n","    show_progress=True\n",")"]},{"cell_type":"code","source":["# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê²½ë¡œ\n","vector_save_path = \"vector_store_index\"\n","\n","# ì €ì¥ ì‹¤í–‰\n","vector_index.storage_context.persist(persist_dir=vector_save_path)\n","\n","print(f\"âœ… ë²¡í„° ì¸ë±ìŠ¤ ì˜êµ¬ ì €ì¥ ì™„ë£Œ: {vector_save_path}\")"],"metadata":{"id":"vJzJcLhWyM3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from llama_index.core import StorageContext, load_index_from_storage\n","# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","# import os\n","\n","# # 1. ì €ì¥ëœ ê²½ë¡œ ì„¤ì • (ì•„ê¹Œ ì €ì¥í•œ ê·¸ ê²½ë¡œ)\n","# vector_save_path = \"/vector_store_index\"\n","\n","# # 2. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ (í•„ìˆ˜! ğŸš¨)\n","# # ì €ì¥í•  ë•Œ BAAI/bge-m3 ëª¨ë¸ë¡œ ìˆ«ìë¥¼ ë§Œë“¤ì—ˆìœ¼ë‹ˆ, ë¶ˆëŸ¬ì˜¬ ë•Œë„ ë˜‘ê°™ì€ ë²ˆì—­ê¸°ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n","# embed_model = HuggingFaceEmbedding(\n","#     model_name=\"BAAI/bge-m3\",\n","#     device=\"cuda\",\n","# )\n","\n","# # 3. ì¸ë±ìŠ¤ ë¡œë“œ (Load)\n","# if os.path.exists(vector_save_path):\n","#     print(f\"ğŸ“‚ '{vector_save_path}'ì—ì„œ ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\")\n","\n","#     # ì €ì¥ì†Œ ì»¨í…ìŠ¤íŠ¸(ë°ì´í„° ì°½ê³ ) ì¬êµ¬ì„±\n","#     storage_context = StorageContext.from_defaults(persist_dir=vector_save_path)\n","\n","#     # ì¸ë±ìŠ¤ ê°ì²´ë¡œ ë¶€í™œì‹œí‚¤ê¸°\n","#     vector_index = load_index_from_storage(\n","#         storage_context,\n","#         embed_model=embed_model # ëª¨ë¸ ì£¼ì…\n","#     )\n","\n","#     print(\"ğŸ‰ ë²¡í„° ì¸ë±ìŠ¤ ë³µêµ¬ ì™„ë£Œ! (ë‹¤ì‹œ ì¸ë±ì‹±í•  í•„ìš” ì—†ìŒ)\")\n","\n","# else:\n","#     print(f\"ğŸ˜± ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! í™•ì¸í•´ì£¼ì„¸ìš”: {vector_save_path}\")\n","\n","# # 4. (ì„ íƒ) ë°”ë¡œ ê²€ìƒ‰ê¸°ë¡œ ë§Œë“¤ì–´ì„œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n","# # dense_retriever = vector_index.as_retriever(similarity_top_k=100)"],"metadata":{"id":"vQefbuMKOa2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhOpKAwSBTIF"},"outputs":[],"source":["# --- 3. Reranker (ì‚¬ìš©ì ì •ì˜) ---\n","from sentence_transformers import CrossEncoder\n","\n","class Reranker:\n","    def __init__(self, model_name: str = RERANKER_MODEL_NAME):\n","        self.model = CrossEncoder(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    def rerank(self, query: str, docs: List[str], doc_ids: List[str], top_k: int = 5):\n","        \"\"\"\n","        [ìˆ˜ì •ë¨] ì¤‘ë³µëœ doc_idëŠ” ì œê±°í•˜ê³  ê°€ì¥ ì ìˆ˜ ë†’ì€ ëŒ€í‘œ 1ê°œë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n","        \"\"\"\n","        if not docs:\n","            return [], []\n","\n","        # 1. ì ìˆ˜ ê³„ì‚°\n","        pairs = [[query, d] for d in docs]\n","        scores = self.model.predict(pairs)\n","\n","        # 2. í•˜ë‚˜ë¡œ ë¬¶ê¸° (ì ìˆ˜, í…ìŠ¤íŠ¸, ë¬¸ì„œID)\n","        # 3ê°œë¥¼ í•œ ë²ˆì— zipìœ¼ë¡œ ë¬¶ì–´ì„œ ê´€ë¦¬í•˜ëŠ” ê²Œ í¸í•©ë‹ˆë‹¤.\n","        combined = list(zip(scores, docs, doc_ids))\n","\n","        # 3. ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n","        combined.sort(key=lambda x: x[0], reverse=True)\n","\n","        # 4. [í•µì‹¬] ì¤‘ë³µ ì œê±° ë¡œì§ (Deduplication) ğŸš¨\n","        seen_ids = set()\n","        final_results = []\n","        final_ids = []\n","\n","        for score, text, did in combined:\n","            # ì´ë¯¸ ë“±ë¡ëœ ë¬¸ì„œ IDë¼ë©´ ìŠ¤í‚µ! (ê°€ì¥ ì ìˆ˜ ë†’ì€ ë†ˆë§Œ ì´ë¯¸ ë“¤ì–´ê°”ì„ í…Œë‹ˆê¹Œ)\n","            if did in seen_ids:\n","                continue\n","\n","            seen_ids.add(did)\n","            final_results.append((text, score)) # ê²°ê³¼ í¬ë§· ìœ ì§€\n","            final_ids.append(did)\n","\n","            # top_kê°œ ì±„ì› ìœ¼ë©´ ê·¸ë§Œ\n","            if len(final_results) >= top_k:\n","                break\n","\n","        return final_results, final_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daiju4xlBVJh"},"outputs":[],"source":["reranker = Reranker()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBXwc87ZBZNJ"},"outputs":[],"source":["from kiwipiepy import Kiwi\n","import re\n","from typing import List\n","\n","def _fallback_tokenize(text: str) -> list[str]:\n","    \"\"\"Kiwi ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ whitespace + ë¬¸ì ê¸°ë°˜ í† í°í™”\"\"\"\n","    # ê³µë°± ë¶„ë¦¬ + ì•ŒíŒŒë²³/ìˆ«ì/ê¸°íƒ€ ìœ ë‹ˆì½”ë“œ ë‹¨ì–´ ì¶”ì¶œ\n","    tokens = re.findall(r'\\b\\w+\\b', text, re.UNICODE)\n","    return [t for t in tokens]\n","\n","def tokenize_kiwi(\n","    text: str,\n","    kiwi: Kiwi,\n","    tag_include: List[str],\n","    text_type: str,\n","    top_n: int,\n","    score_threshold: float = 1.2,\n",") -> list[str]:\n","    try:\n","        # 1. í† í°í™”í•  í…ìŠ¤íŠ¸ê°€ ë¬¸ì„œ(Corpus)ì¼ ë•Œ\n","        if text_type == \"corpus\":\n","            # ë¬¸ì„œëŠ” ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ top_nì„ ìœ ë™ì ìœ¼ë¡œ ì„¤ì •\n","            analyzed = kiwi.analyze(text, top_n=top_n + len(text) // 200)\n","\n","            if not analyzed:\n","                return _fallback_tokenize(text)\n","\n","            num_candi = 1\n","            # 1ìœ„ ì ìˆ˜ ê¸°ì¤€ threshold ì´ë‚´ì˜ í›„ë³´êµ° ì¶”ê°€\n","            while (\n","                num_candi < len(analyzed)\n","                and analyzed[num_candi][1] > score_threshold * analyzed[0][1]\n","            ):\n","                num_candi += 1\n","\n","        # 2. í† í°í™”í•  í…ìŠ¤íŠ¸ê°€ ì¿¼ë¦¬(Query)ì¼ ë•Œ\n","        elif text_type == \"query\":\n","            analyzed = kiwi.analyze(text, top_n=top_n)\n","\n","            if not analyzed:\n","                return _fallback_tokenize(text)\n","\n","            num_candi = 3 # ì¿¼ë¦¬ëŠ” ì¢€ ë” ë‹¤ì–‘í•˜ê²Œ í›„ë³´ë¥¼ ë´„\n","\n","        # 3. í›„ë³´êµ°ì—ì„œ í† í° ì¶”ì¶œ\n","        all_tokenized = [\n","            (t.form, t.tag)\n","            for nc in range(num_candi)\n","            for t in analyzed[nc][0]\n","        ]\n","\n","        # 4. ì¤‘ë³µ ì œê±°\n","        unique_tokenized = set(all_tokenized)\n","\n","        # 5. [í•µì‹¬ ìˆ˜ì •] í•„í„°ë§ì€ í•˜ë˜, íƒœê·¸(/NNG ë“±)ëŠ” ë–¼ê³  'ë‹¨ì–´'ë§Œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŒ\n","        filtered = [\n","            form  # ğŸš¨ ìˆ˜ì •ë¨: f\"{form}/{tag}\" -> form\n","            for form, tag in unique_tokenized\n","            if tag in tag_include\n","        ]\n","\n","        return filtered if filtered else _fallback_tokenize(text)\n","\n","    except Exception:\n","        return _fallback_tokenize(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN2faJypHXXq"},"outputs":[],"source":["kiwi_tags = ['NNG', 'NNP', 'NNB', 'NR', 'VV', 'VA', 'MM', 'XR', 'SW', 'SL', 'SH', 'SN', 'SB']\n","\n","GLINER_TAG = {\n","    \"ì–¸ì–´\": {\"CV_LANGUAGE\": \"ë¬¸ëª…_ì–¸ì–´\", \"TR_HUMANITIES\": \"ì´ë¡ _ì² í•™/ì–¸ì–´/ì—­ì‚¬\"}, \"ë¬¸í•™\": {\"FD_HUMANITIES\": \"í•™ë¬¸ ë¶„ì•¼_ì¸ë¬¸í•™\", \"AFA_DOCUMENT\": \"ì¸ê³µë¬¼_ë„ì„œ/ì„œì  ì‘í’ˆëª…\"}, \"ì—­ì‚¬\": {\"CV_CULTURE\": \"ë¬¸ëª…_ë¬¸ëª…/ë¬¸í™”\", \"AF_CULTURAL_ASSET\": \"ì¸ê³µë¬¼_ë¬¸í™”ì¬\", \"DT_DYNASTY\": \"ë‚ ì§œ_ì™•ì¡°ì‹œëŒ€\", \"DT_GEOAGE\": \"ë‚ ì§œ_ì§€ì§ˆì‹œëŒ€\", \"EV_WAR_REVOLUTION\": \"ì‚¬ê±´_ì „ìŸ/í˜ëª…\", \"EV_OTHERS\": \"ì‚¬ê±´_ê¸°íƒ€\"}, \"ì² í•™\": {\"TR_HUMANITIES\": \"ì´ë¡ _ì² í•™/ì–¸ì–´/ì—­ì‚¬\", \"CV_TRIBE\": \"ë¬¸ëª…_ë¯¼ì¡±/ì¢…ì¡±\"}, \"êµìœ¡\": {\"OGG_EDUCATION\": \"ê¸°ê´€_êµìœ¡\", \"OGG_LIBRARY\": \"ê¸°ê´€_ë„ì„œê´€\"}, \"ë¯¼ì†\": {\"CV_CULTURE\": \"ë¬¸ëª…_ë¬¸ëª…/ë¬¸í™”\", \"EV_FESTIVAL\": \"ì‚¬ê±´_ì¶•ì œ/ì˜í™”ì œ\"}, \"ì¸ë¬¸ ì¼ë°˜\": {\"FD_HUMANITIES\": \"í•™ë¬¸ ë¶„ì•¼_ì¸ë¬¸í•™\"},\n","    \"ë²•ë¥ \": {\"CV_LAW\": \"ë¬¸ëª…_ë²•/ë²•ë¥ \", \"OGG_LAW\": \"ê¸°ê´€_ë²•ë¥ \", \"CV_POLICY\": \"ë¬¸ëª…_ì œë„/ì •ì±…\"}, \"êµ°ì‚¬\": {\"OGG_MILITARY\": \"ê¸°ê´€_êµ°ì‚¬\", \"AF_WEAPON\": \"ì¸ê³µë¬¼_ë¬´ê¸°\"}, \"ê²½ì˜\": {\"OGG_ECONOMY\": \"ê¸°ê´€_ê²½ì œ\", \"AFW_SERVICE_PRODUCTS\": \"ì¸ê³µë¬¼_ì„œë¹„ìŠ¤ ìƒí’ˆ\"}, \"ê²½ì œ\": {\"OGG_ECONOMY\": \"ê¸°ê´€_ê²½ì œ\", \"CV_CURRENCY\": \"ë¬¸ëª…_í†µí™”\", \"CV_TAX\": \"ë¬¸ëª…_ì¡°ì„¸\"}, \"ë³µì§€\": {\"CV_FUNDS\": \"ë¬¸ëª…_ì—°ê¸ˆ/ê¸°ê¸ˆ\"}, \"ì •ì¹˜\": {\"OGG_POLITICS\": \"ê¸°ê´€_ì •ë¶€/ê³µê³µ\", \"CV_POLICY\": \"ë¬¸ëª…_ì œë„/ì •ì±…\", \"EV_ACTIVITY\": \"ì‚¬ê±´_ì‚¬íšŒìš´ë™/ì„ ì–¸\"}, \"ë§¤ì²´\": {\"OGG_MEDIA\": \"ê¸°ê´€_ë¯¸ë””ì–´\", \"AFA_VIDEO\": \"ì¸ê³µë¬¼_ì˜í™”/TV í”„ë¡œê·¸ë¨\"}, \"í–‰ì •\": {\"OGG_POLITICS\": \"ê¸°ê´€_ì •ë¶€/ê³µê³µ\"}, \"ì‹¬ë¦¬\": {\"FD_SOCIAL_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ì‚¬íšŒê³¼í•™\"}, \"ì‚¬íšŒ ì¼ë°˜\": {\"FD_SOCIAL_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ì‚¬íšŒê³¼í•™\"},\n","    \"ì§€êµ¬\": {\"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\", \"MT_ROCK\": \"ë¬¼ì§ˆ_ì•”ì„\"}, \"ì§€ë¦¬\": {\"LC_OTHERS\": \"ì¥ì†Œ_ê¸°íƒ€\", \"LCG_MOUNTAIN\": \"ì¥ì†Œ_ì‚°/ì‚°ë§¥\", \"LCG_RIVER\": \"ì¥ì†Œ_ê°•/í˜¸ìˆ˜\", \"LCG_OCEAN\": \"ì¥ì†Œ_ë°”ë‹¤\", \"LCG_ISLAND\": \"ì¥ì†Œ_ì„¬\", \"LCG_CONTINENT\": \"ì¥ì†Œ_ëŒ€ë¥™\", \"TM_DIRECTION\": \"ìš©ì–´_ë°©í–¥\"}, \"í•´ì–‘\": {\"LCG_OCEAN\": \"ì¥ì†Œ_ë°”ë‹¤\", \"LCG_BAY\": \"ì¥ì†Œ_ë°˜ë„/ë§Œ\"}, \"ì²œë¬¸\": {\"LC_SPACE\": \"ì¥ì†Œ_ì²œì²´\", \"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\"}, \"í™˜ê²½\": {\"TM_CLIMATE\": \"ìš©ì–´_ê¸°í›„ ì§€ì—­\", \"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\"}, \"ìƒëª…\": {\"TM_CELL_TISSUE_ORGAN\": \"ìš©ì–´_ì„¸í¬/ì¡°ì§/ê¸°ê´€\", \"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\"}, \"ë™ë¬¼\": {\"AM_INSECT\": \"ë™ë¬¼_ê³¤ì¶©\", \"AM_BIRD\": \"ë™ë¬¼_ì¡°ë¥˜\", \"AM_FISH\": \"ë™ë¬¼_ì–´ë¥˜\", \"AM_MAMMALIA\": \"ë™ë¬¼_í¬ìœ ë¥˜\", \"AM_AMPHIBIA\": \"ë™ë¬¼_ì–‘ì„œë¥˜\", \"AM_REPTILIA\": \"ë™ë¬¼_íŒŒì¶©ë¥˜\", \"AM_TYPE\": \"ë™ë¬¼_ë¶„ë¥˜ëª…\", \"AM_PART\": \"ë™ë¬¼_ë¶€ìœ„ëª…\", \"AM_OTHERS\": \"ë™ë¬¼_ê¸°íƒ€\"}, \"ì‹ë¬¼\": {\"PT_FLOWER\": \"ì‹ë¬¼_ê½ƒ\", \"PT_GRASS\": \"ì‹ë¬¼_í’€\", \"PT_TYPE\": \"ì‹ë¬¼_ë¶„ë¥˜ëª…\", \"PT_PART\": \"ì‹ë¬¼_ë¶€ìœ„ëª…\", \"PT_OTHERS\": \"ì‹ë¬¼_ê¸°íƒ€\", \"PT_TREE\": \"ì‹ë¬¼_ë‚˜ë¬´\", \"PT_FRUIT\": \"ì‹ë¬¼_ê³¼ì¼/ì—´ë§¤\"}, \"ì²œì—°ìì›\": {\"MT_ELEMENT\": \"ë¬¼ì§ˆ_ì›ì†Œ\"}, \"ìˆ˜í•™\": {\"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\", \"TM_SHAPE\": \"ìš©ì–´_ëª¨ì–‘/í˜•íƒœ\", \"QT_SIZE\": \"ìˆ˜ëŸ‰_ë„“ì´/ë©´ì \", \"QT_LENGTH\": \"ìˆ˜ëŸ‰_ê¸¸ì´/ê±°ë¦¬\", \"QT_VOLUME\": \"ìˆ˜ëŸ‰_ë¶€í”¼\", \"QT_PERCENTAGE\": \"ìˆ˜ëŸ‰_ë°±ë¶„ìœ¨\"}, \"ë¬¼ë¦¬\": {\"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\", \"TR_SCIENCE\": \"ì´ë¡ _ê³¼í•™\", \"QT_SPEED\": \"ìˆ˜ëŸ‰_ì†ë„\", \"QT_TEMPERATURE\": \"ìˆ˜ëŸ‰_ì˜¨ë„\", \"QT_WEIGHT\": \"ìˆ˜ëŸ‰_ë¬´ê²Œ\"}, \"í™”í•™\": {\"MT_CHEMICAL\": \"ë¬¼ì§ˆ_í™”í•™\", \"MT_ELEMENT\": \"ë¬¼ì§ˆ_ì›ì†Œ\", \"MT_METAL\": \"ë¬¼ì§ˆ_ê¸ˆì†\"}, \"ìì—° ì¼ë°˜\": {\"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\"},\n","    \"ë†ì—…\": {\"PT_FRUIT\": \"ì‹ë¬¼_ê³¼ì¼/ì—´ë§¤\", \"FD_OTHERS\": \"í•™ë¬¸ ë¶„ì•¼_ê¸°íƒ€\"}, \"ìˆ˜ì‚°ì—…\": {\"AM_FISH\": \"ë™ë¬¼_ì–´ë¥˜\"}, \"ì„ì—…\": {\"PT_TREE\": \"ì‹ë¬¼_ë‚˜ë¬´\"}, \"ê´‘ì—…\": {\"MT_ROCK\": \"ë¬¼ì§ˆ_ì•”ì„\", \"MT_METAL\": \"ë¬¼ì§ˆ_ê¸ˆì†\"}, \"ê³µì—…\": {\"AFW_OTHER_PRODUCTS\": \"ì¸ê³µë¬¼_ê¸°íƒ€ ìƒí’ˆ\"}, \"ì„œë¹„ìŠ¤ì—…\": {\"AFW_SERVICE_PRODUCTS\": \"ì¸ê³µë¬¼_ì„œë¹„ìŠ¤ ìƒí’ˆ\", \"OGG_HOTEL\": \"ê¸°ê´€_ìˆ™ë°• ì—…ì²´\"}, \"ì‚°ì—… ì¼ë°˜\": {\"OGG_ECONOMY\": \"ê¸°ê´€_ê²½ì œ\"},\n","    \"ì˜í•™\": {\"FD_MEDICINE\": \"í•™ë¬¸ ë¶„ì•¼_ì˜í•™\", \"TR_MEDICINE\": \"ì´ë¡ _ì˜í•™\", \"TMM_DISEASE\": \"ìš©ì–´_ì¦ìƒ/ì§ˆë³‘\"}, \"ì•½í•™\": {\"TMM_DRUG\": \"ìš©ì–´_ì•½í’ˆ\"}, \"í•œì˜\": {\"FD_MEDICINE\": \"í•™ë¬¸ ë¶„ì•¼_ì˜í•™\"}, \"ìˆ˜ì˜\": {\"FD_MEDICINE\": \"í•™ë¬¸ ë¶„ì•¼_ì˜í•™\"}, \"ì‹í’ˆ\": {\"CV_FOOD\": \"ë¬¸ëª…_ìŒì‹\", \"CV_DRINK\": \"ë¬¸ëª…_ìŒë£Œ/ìˆ \", \"CV_FOOD_STYLE\": \"ë¬¸ëª…_ìŒì‹ ìœ í˜•\"}, \"ë³´ê±´ ì¼ë°˜\": {\"OGG_MEDICINE\": \"ê¸°ê´€_ì˜ë£Œ\"},\n","    \"ê±´ì„¤\": {\"AF_BUILDING\": \"ì¸ê³µë¬¼_ê±´ì¶•ë¬¼/í† ëª©ê±´ì„¤ë¬¼\", \"CV_BUILDING_TYPE\": \"ë¬¸ëª…_ê±´ì¶• ì–‘ì‹\"}, \"êµí†µ\": {\"AF_TRANSPORT\": \"ì¸ê³µë¬¼_êµí†µìˆ˜ë‹¨/ìš´ì†¡ìˆ˜ë‹¨\", \"AF_ROAD\": \"ì¸ê³µë¬¼_ë„ë¡œ/ì² ë¡œ\"}, \"ê¸°ê³„\": {\"TMI_HW\": \"ìš©ì–´_IT í•˜ë“œì›¨ì–´\"}, \"ì „ê¸°Â·ì „ì\": {\"TMI_HW\": \"ìš©ì–´_IT í•˜ë“œì›¨ì–´\"}, \"ì¬ë£Œ\": {\"MT_ELEMENT\": \"ë¬¼ì§ˆ_ì›ì†Œ\"}, \"ì •ë³´Â·í†µì‹ \": {\"TMI_SW\": \"ìš©ì–´_IT ì†Œí”„íŠ¸ì›¨ì–´\", \"TMI_HW\": \"ìš©ì–´_IT í•˜ë“œì›¨ì–´\", \"TMI_SITE\": \"ìš©ì–´_URL ì£¼ì†Œ\", \"TMI_EMAIL\": \"ìš©ì–´_ì´ë©”ì¼ ì£¼ì†Œ\", \"TMI_MODEL\": \"ìš©ì–´_ì œí’ˆ ëª¨ë¸ëª…\", \"TMI_SERVICE\": \"ìš©ì–´_IT ì„œë¹„ìŠ¤\", \"TMI_PROJECT\": \"ìš©ì–´_í”„ë¡œì íŠ¸\"}, \"ê³µí•™ ì¼ë°˜\": {\"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\"},\n","    \"ì²´ìœ¡\": {\"CV_SPORTS\": \"ë¬¸ëª…_ìŠ¤í¬ì¸ \", \"OGG_SPORTS\": \"ê¸°ê´€_ìŠ¤í¬ì¸ \", \"CV_SPORTS_POSITION\": \"ë¬¸ëª…_ìŠ¤í¬ì¸  í¬ì§€ì…˜\", \"CV_SPORTS_INST\": \"ë¬¸ëª…_ìŠ¤í¬ì¸  ìš©í’ˆ/ë„êµ¬\", \"EV_SPORTS\": \"ì‚¬ê±´_ìŠ¤í¬ì¸  í–‰ì‚¬\", \"TM_SPORTS\": \"ìš©ì–´_ìŠ¤í¬ì¸ \"}, \"ì—°ê¸°\": {\"AFA_PERFORMANCE\": \"ì¸ê³µë¬¼_ì¶¤/ê³µì—°/ì—°ê·¹ ì‘í’ˆëª…\"}, \"ì˜ìƒ\": {\"AFA_VIDEO\": \"ì¸ê³µë¬¼_ì˜í™”/TV í”„ë¡œê·¸ë¨\"}, \"ë¬´ìš©\": {\"AFA_PERFORMANCE\": \"ì¸ê³µë¬¼_ì¶¤/ê³µì—°/ì—°ê·¹ ì‘í’ˆëª…\"}, \"ìŒì•…\": {\"AFA_MUSIC\": \"ì¸ê³µë¬¼_ìŒì•… ì‘í’ˆëª…\", \"AF_MUSICAL_INSTRUMENT\": \"ì¸ê³µë¬¼_ì•…ê¸°\", \"OGG_ART\": \"ê¸°ê´€_ì˜ˆìˆ \"}, \"ë¯¸ìˆ \": {\"AFA_ART_CRAFT\": \"ì¸ê³µë¬¼_ë¯¸ìˆ /ì¡°í˜• ì‘í’ˆëª…\", \"FD_ART\": \"í•™ë¬¸ ë¶„ì•¼_ì˜ˆìˆ \", \"TM_COLOR\": \"ìš©ì–´_ìƒ‰ê¹”\"}, \"ë³µì‹\": {\"CV_CLOTHING\": \"ë¬¸ëª…_ì˜ë³µ/ì„¬ìœ \"}, \"ê³µì˜ˆ\": {\"AFA_ART_CRAFT\": \"ì¸ê³µë¬¼_ë¯¸ìˆ /ì¡°í˜• ì‘í’ˆëª…\"}, \"ì˜ˆì²´ëŠ¥ ì¼ë°˜\": {\"FD_ART\": \"í•™ë¬¸ ë¶„ì•¼_ì˜ˆìˆ \"},\n","    \"ê°€í†¨ë¦­\": {\"OGG_RELIGION\": \"ê¸°ê´€_ì¢…êµ\"}, \"ê¸°ë…êµ\": {\"OGG_RELIGION\": \"ê¸°ê´€_ì¢…êµ\"}, \"ë¶ˆêµ\": {\"OGG_RELIGION\": \"ê¸°ê´€_ì¢…êµ\"}, \"ì¢…êµ ì¼ë°˜\": {\"OGG_RELIGION\": \"ê¸°ê´€_ì¢…êµ\"},\n","    \"ì¸ëª…\": {\"PS_NAME\": \"ì¸ë¬¼_ì‚¬ëŒ\", \"PS_CHARACTER\": \"ì¸ë¬¼_ê°€ìƒ ìºë¦­í„°\", \"CV_OCCUPATION\": \"ë¬¸ëª…_ì§ì—…\", \"CV_POSITION\": \"ë¬¸ëª…_ì§ìœ„/ì§ì±…\", \"CV_RELATION\": \"ë¬¸ëª…_ê°€ì¡±/ì¹œì¡± ê´€ê³„\"}, \"ì§€ëª…\": {\"LCP_COUNTRY\": \"ì¥ì†Œ_êµ­ê°€\", \"LCP_PROVINCE\": \"ì¥ì†Œ_ë„/ì£¼ ì§€ì—­\", \"LCP_COUNTY\": \"ì¥ì†Œ_ì„¸ë¶€ í–‰ì •êµ¬ì—­\", \"LCP_CITY\": \"ì¥ì†Œ_ë„ì‹œ\", \"LCP_CAPITALCITY\": \"ì¥ì†Œ_ìˆ˜ë„\", \"LC_OTHERS\": \"ì¥ì†Œ_ê¸°íƒ€\"}, \"ì±…ëª…\": {\"AFA_DOCUMENT\": \"ì¸ê³µë¬¼_ë„ì„œ/ì„œì  ì‘í’ˆëª…\"}, \"ê³ ìœ ëª… ì¼ë°˜\": {\"OGG_OTHERS\": \"ê¸°ê´€_ê¸°íƒ€\"}\n","}\n","\n","gliner_labels = []\n","\n","for middle_cat, inner_map in GLINER_TAG.items():\n","    for gliner_code, leaf_label in inner_map.items():\n","        gliner_labels.append(leaf_label)\n","gliner_labels = sorted(list(set(gliner_labels)))\n","\n","KIWI_TO_URIMALSAEM_MAP = {\n","    \"NNG\": \"ëª…ì‚¬\", \"NNP\": \"ëª…ì‚¬\", \"NNB\": \"ì˜ì¡´ ëª…ì‚¬\", \"NR\": \"ìˆ˜ì‚¬\", \"XR\": \"ëª…ì‚¬\", \"SN\": \"ìˆ˜ì‚¬\",\n","    \"VV\": \"ë™ì‚¬\", \"VA\": \"í˜•ìš©ì‚¬\", \"MM\": \"ê´€í˜•ì‚¬\",\n","    \"SW\": None, \"SB\": None, \"SL\": None, \"SH\": None\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHhXq0gVHZ24"},"outputs":[],"source":["from kiwipiepy import Kiwi\n","from gliner import GLiNER\n","from typing import List, Dict, Set, Any\n","\n","class RichHybridTokenizer:\n","    def __init__(self, gliner_model: GLiNER, labels: List[str], kiwi_tags: List[str]):\n","        print(\"ğŸ”§ Rich Hybrid Tokenizer (Raw Mode: í•„í„°ë§ ì—†ìŒ) ì´ˆê¸°í™”...\")\n","        self.gliner = gliner_model\n","        self.labels = labels\n","        self.kiwi = Kiwi()\n","\n","        # ì‚¬ìš©ìê°€ ìš”ì²­í•œ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","        self.target_tags = set(kiwi_tags)\n","\n","        # ğŸš¨ C++ì˜ '++' ë“±ì„ ì¡ê¸° ìœ„í•´ SW(ê¸°í˜¸)ê°€ target_tagsì— ì—†ë‹¤ë©´ ê°•ì œë¡œ ì¶”ê°€ ê¶Œì¥\n","        # (ì‚¬ìš©ìë‹˜ì´ ì „ë‹¬ì£¼ì‹¤ kiwi_tagsì— SWê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ì¶”ê°€)\n","        # ë¶™ì„í‘œë„ ì¼ë‹¨ ê°€ì ¸ì˜´ (í•„ìš” ì—†ìœ¼ë©´ ë‚˜ì¤‘ì— ë§¤í•‘ì—ì„œ None ì²˜ë¦¬)\n","\n","    def tokenize(self, text: str) -> List[Dict[str, Any]]:\n","        if not text: return []\n","\n","        token_info = {}\n","\n","        # -------------------------------------------------------\n","        # 1. GLiNer: í•µì‹¬ ê°œì²´ëª… & ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ\n","        # -------------------------------------------------------\n","        try:\n","            preds = self.gliner.predict_entities(\n","                text, self.labels, flat_ner=True, threshold=0.1\n","            )\n","            for e in preds:\n","                raw_token = e['text'] # ê³µë°± ìœ ì§€ (ì˜ˆ: Visual Basic)\n","                category = e['label']\n","\n","                if raw_token not in token_info:\n","                    token_info[raw_token] = {\n","                        'text': raw_token,\n","                        'category': category,\n","                        'pos': '(-)'\n","                    }\n","                else:\n","                    token_info[raw_token]['category'] = category\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ GLiNer Error: {e}\")\n","\n","        # -------------------------------------------------------\n","        # 2. Kiwi: í˜•íƒœì†Œ ë¶„ì„ & í’ˆì‚¬ íƒœê¹…\n","        # -------------------------------------------------------\n","        try:\n","            res = self.kiwi.analyze(text, top_n=1)\n","            if res:\n","                for token in res[0][0]:\n","                    # 1. íƒ€ê²Ÿ íƒœê·¸ì¸ì§€ í™•ì¸\n","                    if token.tag in self.target_tags:\n","                        word = token.form\n","\n","                        # ğŸš¨ [ì‚­ì œë¨] SW, SO ë°˜ë³µ ë¬¸ì í•„í„°ë§ ë¡œì§ ì œê±°!\n","                        # ì´ì œ \"++\", \"C#\", \"~~~~\" ëª¨ë‘ ìˆëŠ” ê·¸ëŒ€ë¡œ ë“¤ì–´ì˜µë‹ˆë‹¤.\n","\n","                        # ì €ì¥ ë¡œì§\n","                        if word in token_info:\n","                            # GLiNerì™€ ê²¹ì¹˜ë©´ POS ì •ë³´ ì—…ë°ì´íŠ¸\n","                            token_info[word]['pos'] = token.tag\n","                        else:\n","                            # Kiwië§Œ ì°¾ì€ ë‹¨ì–´ ì¶”ê°€\n","                            token_info[word] = {\n","                                'text': word,\n","                                'category': None,\n","                                'pos': token.tag\n","                            }\n","        except Exception as e:\n","            print(f\"âš ï¸ Kiwi Error: {e}\")\n","\n","        return list(token_info.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y55yo00CHdVY"},"outputs":[],"source":["from typing import List, Dict, Any\n","from collections import defaultdict\n","import json\n","\n","# ==============================================================================\n","# 1. Helper Functions (ì‚¬ìš©ì ì œê³µ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n","# ==============================================================================\n","\n","def create_leaf_to_middle_map(simplified_map: Dict[str, Dict[str, str]]) -> Dict[str, List[str]]:\n","    \"\"\"ë¶„ë¥˜ ì²´ê³„ë¥¼ í‰íƒ„í™”í•˜ì—¬ (ìƒì„¸ ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ ë¦¬ìŠ¤íŠ¸) ë§µì„ ìƒì„±\"\"\"\n","    leaf_to_middle = defaultdict(set)\n","    # GLINER_TAG êµ¬ì¡°: {ì¤‘ë¶„ë¥˜: {ì½”ë“œ: ìƒì„¸ë¶„ë¥˜}}\n","    for middle_cat, tag_dict in simplified_map.items():\n","        for gliner_code, leaf_value in tag_dict.items():\n","            leaf_to_middle[leaf_value].add(middle_cat)\n","    return {k: sorted(list(v)) for k, v in leaf_to_middle.items()}\n","\n","def transform_tokens_enrich_data(token_list: List[Dict[str, Any]], leaf_to_mid_map: Dict[str, List[str]]) -> List[Dict[str, Any]]:\n","    \"\"\"\n","    ì…ë ¥ í† í° ë¦¬ìŠ¤íŠ¸ì— 'dict_cat'(ì¤‘ë¶„ë¥˜)ê³¼ 'dict_pos'(ì‚¬ì „ í’ˆì‚¬)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    enriched_list = []\n","\n","    # ì „ì—­ ë³€ìˆ˜ KIWI_TO_URIMALSAEM_MAP ì‚¬ìš© (ìœ— ì…€ì—ì„œ ì •ì˜ë¨)\n","    # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ì•ˆì „ì¥ì¹˜ë¡œ get ì‚¬ìš©\n","    global KIWI_TO_URIMALSAEM_MAP\n","\n","    for token in token_list:\n","        raw_text = token.get('text')\n","        leaf_cat = token.get('category')\n","        raw_pos = token.get('pos')\n","\n","        # 1. ì¤‘ë¶„ë¥˜ ì¡°íšŒ (GLINER_TAG ê¸°ë°˜ ì—­ì¶”ì )\n","        middle_categories = leaf_to_mid_map.get(leaf_cat, [])\n","\n","        # 2. í’ˆì‚¬ ë§¤í•‘ (KIWI_TO_URIMALSAEM_MAP ì‚¬ìš©)\n","        dict_pos = KIWI_TO_URIMALSAEM_MAP.get(raw_pos)\n","\n","        new_token = {\n","            'text': raw_text,\n","            'category': leaf_cat,\n","            'pos': raw_pos,\n","            'dict_cat': middle_categories, # ì˜ˆ: ['ì¸ëª…']\n","            'dict_pos': dict_pos           # ì˜ˆ: 'ëª…ì‚¬'\n","        }\n","        enriched_list.append(new_token)\n","\n","    return enriched_list\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAk_5fT5HgXG"},"outputs":[],"source":["import pickle\n","import networkx as nx\n","\n","# ==============================================================================\n","# 1. ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ í•¨ìˆ˜\n","# ==============================================================================\n","def load_knowledge_graph(file_path: str) -> nx.Graph:\n","    \"\"\"pkl íŒŒì¼ì—ì„œ NetworkX ê·¸ë˜í”„ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n","    try:\n","        with open(file_path, 'rb') as f:\n","            graph = pickle.load(f)\n","        print(f\"âœ… ê·¸ë˜í”„ ë¡œë“œ ì„±ê³µ! (ë…¸ë“œ: {graph.number_of_nodes()}ê°œ, ì—£ì§€: {graph.number_of_edges()}ê°œ)\")\n","        return graph\n","    except Exception as e:\n","        print(f\"âŒ ê·¸ë˜í”„ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n","        return None\n","\n","class GraphRetriever:\n","    def __init__(self, graph: nx.Graph, min_weight: float = 0.7):\n","        print(\"ğŸ” GraphRetriever ì´ˆê¸°í™” (Word Indexing)...\")\n","        self.graph = graph\n","        self.min_weight = min_weight\n","        self.word_index = defaultdict(list)\n","        self._build_word_index()\n","\n","    def _build_word_index(self):\n","        for node_id, data in self.graph.nodes(data=True):\n","            word = data.get('word')\n","            if word:\n","                self.word_index[word].append(node_id)\n","\n","    def retrieve(self, enriched_tokens: List[Dict[str, Any]]) -> Dict[str, float]:\n","        expanded_weights = defaultdict(float)\n","        original_texts = set()\n","\n","        for item in enriched_tokens:\n","            raw_text = item['text']\n","            dict_cats = item['dict_cat']\n","            dict_pos = item['dict_pos']\n","            raw_pos = item['pos']\n","\n","            # 1. ê²€ìƒ‰ì–´ ì •ê·œí™”\n","            search_text = raw_text\n","            if raw_pos in ['VV', 'VA']:\n","                search_text += 'ë‹¤'\n","\n","            original_texts.add(raw_text)\n","            original_texts.add(search_text)\n","\n","            # 2. ìƒ‰ì¸ ì¡°íšŒ\n","            candidate_ids = self.word_index.get(search_text, [])\n","            valid_ids = []\n","\n","            # 3. ì •ë°€ í•„í„°ë§ (ìˆ˜ì •ëœ ë¡œì§) ğŸš¨\n","            for nid in candidate_ids:\n","                node_cat = self.graph.nodes[nid].get('category')\n","\n","                # Case A: GLiNERê°€ ì°¾ì•„ì¤€ ì¤‘ë¶„ë¥˜ê°€ ìˆìœ¼ë©´ -> ê°•ë ¥ í•„í„°ë§ (ìœ ì§€)\n","                if dict_cats:\n","                    if node_cat in dict_cats:\n","                        valid_ids.append(nid)\n","\n","                # Case B: í’ˆì‚¬ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš°\n","                elif dict_pos:\n","                    # [ìˆ˜ì • í¬ì¸íŠ¸] ë™ì‚¬/í˜•ìš©ì‚¬ëŠ” ì—„ê²©í•˜ê²Œ ê²€ì‚¬\n","                    if dict_pos in ['ë™ì‚¬', 'í˜•ìš©ì‚¬']:\n","                        if node_cat == dict_pos:\n","                            valid_ids.append(nid)\n","\n","                    # [ìˆ˜ì • í¬ì¸íŠ¸] ëª…ì‚¬(NNG/NNP) ë“±ì€ ì¹´í…Œê³ ë¦¬ ë¶ˆì¼ì¹˜ í—ˆìš©!\n","                    # \"êµ­ê°€/ëª…ì‚¬\" -> \"êµ­ê°€/ì •ì¹˜\" (OK!)\n","                    else:\n","                        valid_ids.append(nid)\n","\n","            # 4. ìœ ì˜ì–´ í™•ì¥ (ê¸°ì¡´ ë™ì¼)\n","            for nid in valid_ids:\n","                expanded_weights[search_text] = 1.0\n","                for neighbor, edge in self.graph[nid].items():\n","                    w = edge.get('weight', 0.0)\n","                    if w >= self.min_weight:\n","                        neighbor_word = self.graph.nodes[neighbor].get('word', neighbor)\n","                        if neighbor_word not in original_texts:\n","                            expanded_weights[neighbor_word] = max(expanded_weights[neighbor_word], w)\n","\n","        return dict(expanded_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1aK2VzsHk60"},"outputs":[],"source":["# íŒŒì¼ ê²½ë¡œ (ì•„ê¹Œ ì–¸ê¸‰í•˜ì‹  ê²½ë¡œë¡œ ê°€ì •)\n","KG_FILE_PATH = \"/urimalsaem_graph_FINAL2.pkl\"\n","\n","# 1. ê·¸ë˜í”„ ë¡œë“œ\n","synonym_graph = load_knowledge_graph(KG_FILE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EkF-D8cHl2S"},"outputs":[],"source":["from functools import partial\n","from typing import List\n","\n","# -----------------------------------------------------------\n","# 1. Enrich í•¨ìˆ˜ ì´ë¦„ ë§¤ì¹­ (ì¤‘ìš”!)\n","# -----------------------------------------------------------\n","# ì•„ê¹Œ ë§Œë“  ê¸´ ì´ë¦„ì˜ í•¨ìˆ˜ë¥¼ 'enrich_tokens'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì“°ê² ë‹¤ê³  ì„ ì–¸\n","# (ë§Œì•½ ìœ„ì—ì„œ def transform_tokens_enrich_data... ë¡œ ì •ì˜í–ˆë‹¤ë©´)\n","if 'transform_tokens_enrich_data' in locals():\n","    enrich_tokens = transform_tokens_enrich_data\n","elif 'enrich_tokens' not in locals():\n","    print(\"âš ï¸ ê²½ê³ : enrich_tokens í•¨ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! ìœ„ìª½ ì…€ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n","\n","from typing import List, Dict, Any\n","\n","def tokenize_query_rich(\n","    text: str,\n","    tokenizer_obj,\n","    l2m_map_obj,\n","    graph_retriever_obj\n",") -> Dict[str,float]:\n","    \"\"\"\n","    [ì§ˆë¬¸ ë¶„ì„] -> [í’ë¶€í™”] -> [ê·¸ë˜í”„ í™•ì¥] -> [BoW ë³‘í•©(ì•ˆì „ì¥ì¹˜)] -> [ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜]\n","    \"\"\"\n","\n","    # 1. Rich Tokenizing (GLiNER + Kiwi)\n","    # ì˜ˆ: [{'text': 'ì—ì¼€ë¥´íŠ¸', ...}, {'text': 'ì‹œí–‰ì°©ì˜¤', ...}]\n","    tokens = tokenizer_obj.tokenize(text)\n","\n","    # 2. Enrich (ë©”íƒ€ë°ì´í„° ì¶”ê°€)\n","    enriched = enrich_tokens(tokens, l2m_map_obj)\n","\n","    # 3. Graph Expansion (ìœ ì˜ì–´ ì°¾ê¸°)\n","    # ì˜ˆ: {'ì‹œí–‰ì°©ì˜¤': 1.0, 'ì‹œì˜¤ë²•': 0.7}\n","    retrieval_results = graph_retriever_obj.retrieve(enriched)\n","\n","    # ---------------------------------------------------------\n","    # 4. [ì‚¬ìš©ì ë¡œì§ í†µí•©] Weighted BoW ìƒì„± (Safety Net) ğŸš¨\n","    # ---------------------------------------------------------\n","    final_bow = retrieval_results.copy()\n","\n","    for token in enriched:\n","        raw_text = token['text']\n","        # ì›ë³¸ ë‹¨ì–´ê°€ ìœ ì˜ì–´ ì‚¬ì „ì— ì—†ë”ë¼ë„ ê²€ìƒ‰ì–´ì— ë¬´ì¡°ê±´ í¬í•¨ì‹œí‚´ (ê°€ì¤‘ì¹˜ 1.0)\n","        if raw_text not in final_bow:\n","            final_bow[raw_text] = 1.0\n","\n","    # 5. ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (BM25s ì…ë ¥ìš©)\n","    # í˜„ì¬ KiwiBM25RetrieverëŠ” List[str] ì…ë ¥ì„ ë°›ìœ¼ë¯€ë¡œ í‚¤ê°’ë§Œ ì¶”ì¶œí•´ì„œ ë„˜ê¹ë‹ˆë‹¤.\n","    # (ê°€ì¤‘ì¹˜ 0.7 ì •ë³´ë¥¼ ì ìˆ˜ ê³„ì‚°ì— ì§ì ‘ ë°˜ì˜í•˜ë ¤ë©´ Retriever ë‚´ë¶€ ë¡œì§ì„ ë°”ê¿”ì•¼ í•˜ì§€ë§Œ,\n","    #  ì¼ë‹¨ 'ê²€ìƒ‰ì–´ì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒ(Recall)'ë§Œìœ¼ë¡œë„ íš¨ê³¼ëŠ” ì¶©ë¶„í•©ë‹ˆë‹¤.)\n","    return final_bow\n","\n","print(\"âœ… ì§ˆë¬¸ìš© í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸ ì™„ë£Œ (Safety Net ì ìš©ë¨)\")\n","\n","gliner_model = GLiNER.from_pretrained(\"lots-o/gliner-bi-ko-xlarge-v1\")\n","gliner_model = gliner_model.to(\"cuda\")\n","\n","tokenizer = RichHybridTokenizer(\n","    gliner_model=gliner_model,  # ë¡œë“œí•´ë‘” ëª¨ë¸\n","    labels=gliner_labels,       # ë¼ë²¨ ë¦¬ìŠ¤íŠ¸\n","    kiwi_tags=kiwi_tags         # í’ˆì‚¬ ë¦¬ìŠ¤íŠ¸\n",")\n","l2m_map = create_leaf_to_middle_map(GLINER_TAG)\n","graph_retriever = GraphRetriever(synonym_graph, min_weight=0.7)\n","\n","from functools import partial\n","\n","query_tokenizer_rich = partial(\n","    tokenize_query_rich,\n","    tokenizer_obj=tokenizer,         # ë¡œë´‡\n","    l2m_map_obj=l2m_map,             # ì§€ë„\n","    graph_retriever_obj=graph_retriever # ê²€ìƒ‰ê¸°\n",")\n","\n","print(\"âœ… ìµœì¢… ì¡°ë¦½ ì™„ë£Œ!\")\n","print(\"âœ… ì§ˆë¬¸ìš© Rich í† í¬ë‚˜ì´ì € ì¡°ë¦½ ì™„ë£Œ!\")"]},{"cell_type":"code","source":[],"metadata":{"id":"SaiTZHHw2JMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8a9X4AtP0ue"},"outputs":[],"source":["from llama_index.core.retrievers import BaseRetriever\n","from llama_index.core.schema import NodeWithScore, QueryBundle, BaseNode\n","import bm25s\n","from collections import defaultdict\n","import tqdm\n","\n","class KiwiWeightedBM25Retriever(BaseRetriever):\n","    \"\"\"\n","    [í†µí•© ë²„ì „]\n","    1. ë¬¸ì„œëŠ” Kiwië¡œ ë¹ ë¥´ê²Œ ì¸ë±ì‹±\n","    2. ì§ˆë¬¸ì€ RichTokenizerê°€ ì¤€ ê°€ì¤‘ì¹˜(Dict)ë¥¼ ë°›ì•„\n","    3. ë‚´ë¶€ì—ì„œ ì ìˆ˜ë¥¼ ê³±í•˜ê³  ë”í•´ì„œ(Weight Sum) ê²°ê³¼ë¥¼ ë°˜í™˜\n","    \"\"\"\n","    def __init__(\n","        self,\n","        nodes: List[BaseNode],\n","        similarity_top_k: int,\n","        corpus_tokenizer: Callable[[str], List[str]],         # ë¬¸ì„œëŠ” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n","        query_tokenizer: Callable[[str], Dict[str, float]]    # ğŸš¨ ì§ˆë¬¸ì€ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜!\n","    ) -> None:\n","        self._nodes = nodes\n","        self._similarity_top_k = similarity_top_k\n","        self._corpus_tokenizer = corpus_tokenizer\n","        self._query_tokenizer = query_tokenizer\n","\n","        print(\"ğŸš€ [Index] ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘ (Kiwi)...\")\n","        # ë¬¸ì„œëŠ” ê¸°ì¡´ëŒ€ë¡œ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¸ë±ì‹±\n","        corpus_tokens = [self._corpus_tokenizer(node.text) for node in nodes]\n","\n","        self._bm25 = bm25s.BM25()\n","        self._bm25.index(corpus_tokens)\n","        print(\"âœ… [Index] ì¸ë±ì‹± ì™„ë£Œ!\")\n","\n","        super().__init__()\n","\n","    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n","        query_str = query_bundle.query_str\n","\n","        # 1. ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§• (ì´ì œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ìŠµë‹ˆë‹¤!)\n","        # ì˜ˆ: {'ì‹œí–‰ì°©ì˜¤': 1.0, 'ì‹œì˜¤ë²•': 0.7}\n","        weighted_query = self._query_tokenizer(query_str)\n","\n","        # 2. [í•µì‹¬ ë¡œì§ ì´ì‹] ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ (WeightedBM25S_Final ë¡œì§)\n","        doc_scores = defaultdict(float)\n","\n","        # ë‹¨ì–´ í•˜ë‚˜ì”© ê²€ìƒ‰í•´ì„œ ê°€ì¤‘ì¹˜ ê³±í•´ì„œ ë”í•˜ê¸°\n","        for token, weight in weighted_query.items():\n","            try:\n","                # bm25sëŠ” ì…ë ¥ì„ ì´ì¤‘ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ìŒ [[token]]\n","                results = self._bm25.retrieve([[token]], k=len(self._nodes))\n","            except Exception:\n","                continue\n","\n","            if results.documents.size == 0:\n","                continue\n","\n","            indices = results.documents[0]\n","            scores = results.scores[0]\n","\n","            # (BM25ì ìˆ˜ * ìš°ë¦¬ê°€ ì •í•œ ê°€ì¤‘ì¹˜) ëˆ„ì \n","            for idx, score in zip(indices, scores):\n","                doc_scores[idx] += (score * weight)\n","\n","        # 3. ì ìˆ˜ìˆœ ì •ë ¬\n","        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","        # 4. ìƒìœ„ kê°œë§Œ ì˜ë¼ì„œ LlamaIndex í¬ë§·(NodeWithScore)ìœ¼ë¡œ ë³€í™˜\n","        top_k_docs = sorted_docs[:self._similarity_top_k]\n","\n","        nodes_with_scores = []\n","        for idx, score in top_k_docs:\n","            nodes_with_scores.append(\n","                NodeWithScore(node=self._nodes[idx], score=float(score))\n","            )\n","\n","        return nodes_with_scores\n","\n","    # persist í•¨ìˆ˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ ê°€ëŠ¥\n","    def persist(self, path: str):\n","        self._bm25.save(path)\n","        print(f\"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJweZ1z4P3Fa"},"outputs":[],"source":["from functools import partial\n","\n","# 1. ì¿¼ë¦¬ í† í¬ë‚˜ì´ì € ë‹¤ì‹œ ì¡°ë¦½ (Dict ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •í•œ ë²„ì „ ì ìš©)\n","query_tokenizer_rich = partial(\n","    tokenize_query_rich, # ğŸš¨ 1ë‹¨ê³„ì—ì„œ ìˆ˜ì •í•œ(Dictë°˜í™˜) í•¨ìˆ˜ì—¬ì•¼ í•¨!\n","    tokenizer_obj=tokenizer,\n","    l2m_map_obj=l2m_map,\n","    graph_retriever_obj=graph_retriever\n",")\n","\n","kiwi_instance = Kiwi()\n","# 2. ë¬¸ì„œ í† í¬ë‚˜ì´ì € (ê¸°ì¡´ Kiwi - List ë°˜í™˜)\n","corpus_tokenizer = partial(\n","    tokenize_kiwi,\n","    kiwi=kiwi_instance,\n","    tag_include=kiwi_tags,\n","    text_type=\"corpus\",\n","    top_n=2,\n","    score_threshold=1.2,\n",")\n","\n","# 3. [ìµœì¢…] í†µí•© ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±\n","final_retriever = KiwiWeightedBM25Retriever(\n","    nodes=nodes,\n","    similarity_top_k=30,\n","    corpus_tokenizer=corpus_tokenizer,  # ë¬¸ì„œëŠ” kiwi\n","    query_tokenizer=query_tokenizer_rich # ì§ˆë¬¸ì€ kiwi+ner+ìœ ì˜ì–´\n",")\n","\n","print(\"ğŸ‰ ì™„ë²½í•œ í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ ê²€ìƒ‰ê¸° ì™„ì„±!\")"]},{"cell_type":"code","source":["# 1. íƒ€ê²Ÿ ì„¤ì • (ì•„ê¹Œ ì°¾ì€ ëŒ€ë°• ì‚¬ë¡€)\n","target_idx = 306\n","target_doc_id = 52478\n","\n","# 2. ë°ì´í„°ì…‹ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n","item = dataset['train'][target_idx]\n","question = item['question']\n","# ì •ë‹µì€ ë³´í†µ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ({'text': ['ì •ë‹µ'], 'answer_start': [...]})ë¡œ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.\n","gt_answer = item['answers']['text'][0]\n","\n","print(f\" ì§ˆë¬¸ (Q{target_idx}): {question}\")\n","print(f\" ì •ë‹µ (Ground Truth): **{gt_answer}**\")\n","print(\"=\" * 60)\n","\n","# 3. ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì™„ì „ì²´)\n","full_text = \"\"\n","for node in nodes:\n","    did = node.metadata.get('docid') or node.metadata.get('document_id')\n","    if did and int(did) == target_doc_id:\n","        full_text += node.get_content() + \"\\n\" # ì²­í¬ ì´ì–´ë¶™ì´ê¸°\n","\n","# 4. ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n","if not full_text:\n","    print(\" ë¬¸ì„œë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n","else:\n","    # ì •ë‹µì´ ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n","    if gt_answer in full_text:\n","        print(f\" [ê²€ì¦ ì„±ê³µ] ì°¾ì€ ë¬¸ì„œì— ì •ë‹µì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! \")\n","\n","        # ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸° (ì •ë‹µ ë¶€ë¶„ í•˜ì´ë¼ì´íŠ¸)\n","        # ì •ë‹µ ì£¼ë³€ 50ìë§Œ ì˜ë¼ì„œ ë³´ì—¬ì¤Œ\n","        idx = full_text.find(gt_answer)\n","        start = max(0, idx - 50)\n","        end = min(len(full_text), idx + len(gt_answer) + 50)\n","\n","        snippet = full_text[start:end].replace('\\n', ' ')\n","\n","        print(f\"ğŸ“ ë¬¸ì„œ ë‚´ìš© ë°œì·Œ:\")\n","        print(f\"   ...{snippet.replace(gt_answer, f'[{gt_answer}]')}...\")\n","\n","        print(\"-\" * 60)\n","        print(\" ê²°ë¡ :\")\n","        print(\"   Basic ê²€ìƒ‰ê¸°ëŠ” 'ë³¸ë˜'ë§Œ ì°¾ë‹¤ê°€ ì´ ë¬¸ì„œë¥¼ ë†“ì³¤ì§€ë§Œ,\")\n","        print(\"   Rich ê²€ìƒ‰ê¸°ëŠ” 'ì›ë˜'ë¥¼ ìœ ì˜ì–´ë¡œ í™•ì¥í•´ì„œ ì´ ë¬¸ì„œë¥¼ ì°¾ì•˜ê³ ,\")\n","        print(\"   ê²°êµ­ **ì •ë‹µ(ì—°ê¸°ë°°ìš°...)ì„ ë§ì¶œ ìˆ˜ ìˆëŠ” ê·¼ê±°**ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤!\")\n","\n","    else:\n","        print(f\" [ê²€ì¦ ì‹¤íŒ¨] ë¬¸ì„œëŠ” ì°¾ì•˜ëŠ”ë° í…ìŠ¤íŠ¸ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.\")\n","        print(f\"   (ì‚¬ëŒ ëˆˆìœ¼ë¡œ í™•ì¸ í•„ìš”. ì˜ë¯¸ëŠ” ê°™ì§€ë§Œ í‘œí˜„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)\")\n","        print(f\"   ë¬¸ì„œ ì•ë¶€ë¶„: {full_text[:100]}...\")"],"metadata":{"id":"xF8NTOi9qjDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ë¶„ì„í•  íƒ€ê²Ÿ ë¬¸ì„œ ID (Rich 1ë“±, Basic 5ë“±ì´ì—ˆë˜ ê·¸ ë…€ì„)\n","target_doc_id = 51638\n","target_query = dataset['train'][1]['question']\n","\n","print(f\"ğŸ¯ íƒ€ê²Ÿ ë¶„ì„: ë¬¸ì„œ {target_doc_id} vs ì§ˆë¬¸ '{target_query}'\")\n","print(\"=\" * 60)\n","\n","# 1. ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (Rich ê²°ê³¼ì—ì„œ ì°¾ê¸°)\n","# (ì´ë¯¸ ê²€ìƒ‰ëœ ê²°ê³¼ res_rich ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ìŠµë‹ˆë‹¤)\n","target_node = None\n","for node_item in res_rich:\n","    did = node_item.node.metadata.get('docid') or node_item.node.metadata.get('document_id')\n","    if int(did) == target_doc_id:\n","        target_node = node_item.node\n","        break\n","\n","if not target_node:\n","    print(\"ğŸ˜± Rich ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•´ë‹¹ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n","else:\n","    doc_content = target_node.get_content().replace('\\n', ' ')\n","    print(f\"ğŸ“ ë¬¸ì„œ ë‚´ìš© (ì¼ë¶€): {doc_content[:100]}...\")\n","    print(\"-\" * 60)\n","\n","    # 2. ì¿¼ë¦¬ ë¶„ì„ (Rich í† í¬ë‚˜ì´ì € ì‚¬ìš©)\n","    # í† í°ê³¼ ì ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n","    token_scores = retriever_rich._query_tokenizer(target_query)\n","\n","    # ì›ë³¸ë‹¨ì–´(Score >= 1.0) vs ìœ ì˜ì–´(Score < 1.0) ë¶„ë¦¬\n","    original_tokens = {t for t, s in token_scores.items() if s >= 1.0}\n","    synonym_tokens = {t for t, s in token_scores.items() if s < 1.0}\n","\n","    # 3. ë§¤ì¹­ ê²€ì‚¬ (Hit Check)\n","    hit_originals = [t for t in original_tokens if t in doc_content]\n","    hit_synonyms = [t for t in synonym_tokens if t in doc_content]\n","\n","    # 4. ì‹œê°í™” ì¶œë ¥\n","    print(f\"ğŸ“Š [Basic ê´€ì ] ì›ë³¸ ë‹¨ì–´ ë§¤ì¹­: {len(hit_originals)}ê°œ\")\n","    print(f\"   ğŸ‘‰ ëª©ë¡: {hit_originals}\")\n","    print(f\"   (Basicì—ì„œëŠ” ì´ ë‹¨ì–´ë“¤ë§Œ ë³´ì—¬ì„œ ì ìˆ˜ê°€ ë‚®ì•˜ìŒ)\")\n","\n","    print(\"-\" * 30)\n","\n","    print(f\"ğŸš€ [Rich ê´€ì ] ìœ ì˜ì–´ ì¶”ê°€ ë§¤ì¹­: {len(hit_synonyms)}ê°œ\")\n","    print(f\"   ğŸ‘‰ ëª©ë¡: {hit_synonyms}\")\n","    print(f\"   (ì´ ë‹¨ì–´ë“¤ì´ ì¶”ê°€ ì ìˆ˜ë¥¼ ì¤˜ì„œ 1ë“±ìœ¼ë¡œ ë°€ì–´ì˜¬ë¦¼!)\")\n","\n","    print(\"-\" * 30)\n","\n","    # 5. ë¬¸ë§¥ í•˜ì´ë¼ì´íŠ¸ (ìœ ì˜ì–´ê°€ ì“°ì¸ ë¬¸ì¥ ì°¾ê¸°)\n","    if hit_synonyms:\n","        print(\"ğŸ‘€ [ê²°ì •ì  ë¬¸ë§¥] ìœ ì˜ì–´ê°€ ë“±ì¥í•œ ë¶€ë¶„:\")\n","        for syn in hit_synonyms:\n","            idx = doc_content.find(syn)\n","            start = max(0, idx - 20)\n","            end = min(len(doc_content), idx + 20)\n","            print(f\"   ğŸ”¥ '{syn}': ...{doc_content[start:end]}...\")\n","    else:\n","        print(\"   (ìœ ì˜ì–´ê°€ ë³¸ë¬¸ì— ì§ì ‘ ë“±ì¥í•˜ì§„ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³µí•©ëª…ì‚¬ ë“± ë‹¤ë¥¸ ìš”ì¸ì¼ ìˆ˜ ìˆìŒ)\")\n","\n","print(\"=\" * 60)"],"metadata":{"id":"dxY8RLSnimSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive') # ì´ë¯¸ ë§ˆìš´íŠ¸ë˜ì–´ ìˆìœ¼ë©´ íŒ¨ìŠ¤\n","\n","# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë‚´ì˜ ì›í•˜ëŠ” ê²½ë¡œ ì§€ì •\n","save_path = \"/content/drive/MyDrive/vllm_baseline/my_search_index\"\n","\n","# ì €ì¥ ì‹¤í–‰!\n","final_retriever.persist(save_path)\n","\n","print(f\"âœ… êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì•ˆì „í•˜ê²Œ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {save_path}\")"],"metadata":{"id":"linIoUUAuaFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama_index.core import Settings\n","\n","# ğŸ›‘ \"ë‚˜ OpenAI í‚¤ ì—†ì–´! ê¸°ë³¸ LLMìœ¼ë¡œ OpenAI ì°¾ì§€ ë§ˆ!\" ë¼ê³  ì„ ì–¸\n","Settings.llm = None\n","# (ë§Œì•½ ë¡œë“œí•´ë‘” Gemma ëª¨ë¸ ë³€ìˆ˜ 'llm'ì´ ìˆë‹¤ë©´ Settings.llm = llm ì´ë¼ê³  ë„£ìœ¼ì…”ë„ ë©ë‹ˆë‹¤)\n","\n","print(\"âœ… ê¸°ë³¸ LLM ì„¤ì •ì„ 'None'ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. (OpenAI ì—ëŸ¬ í•´ê²°)\")"],"metadata":{"id":"DvROGDqF4OGC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_Vs3zNbRe2l"},"outputs":[],"source":["from llama_index.core.retrievers import QueryFusionRetriever\n","\n","# 1. Dense Retriever (ë²¡í„° ê²€ìƒ‰ - ì˜ë¯¸ ê¸°ë°˜)\n","# (Top-kë¥¼ ë„‰ë„‰í•˜ê²Œ 50~100ê°œ ê°€ì ¸ì˜¤ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤. ì„ì„ ë•Œ í›„ë³´ê°€ ë§ì•„ì•¼ ì¢‹ê±°ë“ ìš”.)\n","dense_retriever = vector_index.as_retriever(similarity_top_k=50)\n","\n","# 2. Sparse Retriever (ìš°ë¦¬ê°€ ë§Œë“  í•˜ì´ë¸Œë¦¬ë“œ - í‚¤ì›Œë“œ ê¸°ë°˜)\n","# (ì´ ë³€ìˆ˜ ì´ë¦„ì´ ì•„ê¹Œ ë§Œë“  final_retriever ì¸ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”!)\n","sparse_retriever = final_retriever\n","\n","print(\"ğŸ”— Fusion Retriever (RRF ë°©ì‹) ì—°ê²° ì¤‘...\")\n","\n","fusion_retriever = QueryFusionRetriever(\n","    retrievers=[dense_retriever, sparse_retriever], # ë‘ ê°œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬\n","    similarity_top_k=30,          # ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ê°œìˆ˜\n","    num_queries=1,                # ì¿¼ë¦¬ ìƒì„± ì•ˆ í•¨ (ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n","    use_async=False,\n","    mode=\"reciprocal_rerank\"      # ğŸŒŸ í•µì‹¬: ë“±ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ê³µí‰í•˜ê²Œ ì„ê¸°\n",")\n","\n","print(\"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ!\")"]},{"cell_type":"code","source":["# 1. í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n","query = dataset['train'][123]['question']\n","print(f\"â“ ì§ˆë¬¸: {query}\")\n","print(\"-\" * 60)\n","\n","# ---------------------------------------------------------\n","# 2. [ì¶”ê°€ë¨] ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° ğŸ‘€\n","# ---------------------------------------------------------\n","target_retriever = final_retriever\n","\n","print(f\"ğŸ§© [{target_retriever.__class__.__name__}] ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼\")\n","\n","try:\n","    # ë¦¬íŠ¸ë¦¬ë²„ ì•ˆì— ìˆ¨ê²¨ì§„ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ë¥¼ êº¼ëƒ…ë‹ˆë‹¤.\n","    tokenizer_func = target_retriever._query_tokenizer\n","\n","    # ì§ˆë¬¸ì„ ë„£ì–´ì„œ ê²°ê³¼ë¥¼ ë´…ë‹ˆë‹¤.\n","    tokenized_result = tokenizer_func(query)\n","\n","    # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬(ê°€ì¤‘ì¹˜ í¬í•¨)ì¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ì— ë”°ë¼ ì˜ˆì˜ê²Œ ì¶œë ¥\n","    if isinstance(tokenized_result, dict):\n","        print(f\"   ğŸ‘‰ íƒ€ì…: ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ (Token: Weight)\")\n","        print(f\"   ğŸ‘‰ í‚¤ì›Œë“œ ê°œìˆ˜: {len(tokenized_result)}ê°œ\")\n","        print(f\"   ğŸ‘‰ ë‚´ìš©: {tokenized_result}\")\n","    else:\n","        print(f\"   ğŸ‘‰ íƒ€ì…: í† í° ë¦¬ìŠ¤íŠ¸\")\n","        print(f\"   ğŸ‘‰ ë‚´ìš©: {tokenized_result}\")\n","\n","except Exception as e:\n","    print(f\"   âš ï¸ í† í¬ë‚˜ì´ì € í™•ì¸ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}\")\n","    print(\"   (í˜¹ì‹œ FusionRetrieverë¥¼ targetìœ¼ë¡œ í•˜ì…¨ë‚˜ìš”? ê·¸ë ‡ë‹¤ë©´ ë‚´ë¶€ì˜ sparse_retrieverë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.)\")\n","\n","print(\"-\" * 60)\n","\n","# ---------------------------------------------------------\n","# 3. ê²€ìƒ‰ ì‹¤í–‰\n","# ---------------------------------------------------------\n","print(f\"ğŸš€ ê²€ìƒ‰ê¸° ì‹¤í–‰ ì¤‘...\")\n","results = target_retriever.retrieve(query)\n","\n","# ---------------------------------------------------------\n","# 4. ê²°ê³¼ ì¶œë ¥\n","# ---------------------------------------------------------\n","print(f\"\\nğŸ” ê²€ìƒ‰ ê²°ê³¼: ì´ {len(results)}ê°œ ë°œê²¬\\n\")\n","\n","for i, node_with_score in enumerate(results[:5]): # ìƒìœ„ 5ê°œë§Œ í™•ì¸\n","    node = node_with_score.node\n","    score = node_with_score.score\n","\n","    # ì¤„ë°”ê¿ˆ ì œê±°\n","    clean_content = node.get_content().replace('\\n', ' ')\n","\n","    # ë¬¸ì„œ ID ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n","    doc_id = node.metadata.get('docid') or node.metadata.get('document_id') or node.node_id\n","\n","    print(f\"ğŸ… [Rank {i+1}] Score: {score:.4f}\")\n","    print(f\"ğŸ“„ ë¬¸ì„œ ID: {doc_id}\")\n","    print(f\"ğŸ“ ë‚´ìš©: {clean_content[:100]}...\")\n","    print(\"-\" * 60)"],"metadata":{"id":"sO5P86GBcNhB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbzT2Sn-RtNn"},"outputs":[],"source":["# 1. í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n","query = dataset['train'][307]['question']\n","print(f\"â“ ì§ˆë¬¸: {query}\")\n","print(\"-\" * 60)\n","\n","# ---------------------------------------------------------\n","# 2. ê²€ìƒ‰ ì‹¤í–‰ (Sparse / Fusion ì„ íƒ)\n","# ---------------------------------------------------------\n","# final_retriever: ìš°ë¦¬ê°€ ë§Œë“  Kiwi+GLiNER+Graph ê²€ìƒ‰ê¸° (Sparse)\n","# fusion_retriever: Dense + Sparse í•©ì¹œ ê²ƒ (Fusion)\n","target_retriever = final_retriever\n","\n","print(f\"ğŸš€ ê²€ìƒ‰ê¸° ì‹¤í–‰ ì¤‘... ({target_retriever.__class__.__name__})\")\n","results = target_retriever.retrieve(query)\n","\n","# ---------------------------------------------------------\n","# 3. ê²°ê³¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥í•˜ê¸° (ìš”ì²­í•˜ì‹  í¬ë§·)\n","# ---------------------------------------------------------\n","print(f\"\\nğŸ” ê²€ìƒ‰ ê²°ê³¼: ì´ {len(results)}ê°œ ë°œê²¬\\n\")\n","\n","for i, node_with_score in enumerate(results[:5]): # ìƒìœ„ 5ê°œë§Œ í™•ì¸\n","    node = node_with_score.node\n","    score = node_with_score.score\n","\n","    # ë‚´ìš©ì—ì„œ ì¤„ë°”ê¿ˆ(\\n)ì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í•œ ì¤„ë¡œ ë³´ê¸° ì¢‹ê²Œ ë§Œë“¦\n","    clean_content = node.get_content().replace('\\n', ' ')\n","\n","    print(f\"ğŸ… [Rank {i+1}] Score: {score:.4f}\")\n","\n","    # ë¬¸ì„œ ID (metadataì— ì—†ìœ¼ë©´ node_id ì‚¬ìš©)\n","    doc_id = node.metadata.get('docid') or node.metadata.get('document_id') or node.node_id\n","    print(f\"ğŸ“„ ë¬¸ì„œ ID: {doc_id}\")\n","\n","    # ë‚´ìš©ì€ 100ìê¹Œì§€ë§Œ ì¶œë ¥\n","    print(f\"ğŸ“ ë‚´ìš©: {clean_content[:100]}...\")\n","    print(\"-\" * 60)"]},{"cell_type":"code","source":["# 1. í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n","query = dataset['train'][307]['question']\n","print(f\"â“ ì§ˆë¬¸: {query}\")\n","print(\"-\" * 60)\n","\n","# ---------------------------------------------------------\n","# 2. ê²€ìƒ‰ ì‹¤í–‰ (Sparse / Fusion ì„ íƒ)\n","# ---------------------------------------------------------\n","# final_retriever: ìš°ë¦¬ê°€ ë§Œë“  Kiwi+GLiNER+Graph ê²€ìƒ‰ê¸° (Sparse)\n","# fusion_retriever: Dense + Sparse í•©ì¹œ ê²ƒ (Fusion)\n","target_retriever = fusion_retriever\n","\n","print(f\"ğŸš€ ê²€ìƒ‰ê¸° ì‹¤í–‰ ì¤‘... ({target_retriever.__class__.__name__})\")\n","results = target_retriever.retrieve(query)\n","\n","# ---------------------------------------------------------\n","# 3. ê²°ê³¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥í•˜ê¸° (ìš”ì²­í•˜ì‹  í¬ë§·)\n","# ---------------------------------------------------------\n","print(f\"\\nğŸ” ê²€ìƒ‰ ê²°ê³¼: ì´ {len(results)}ê°œ ë°œê²¬\\n\")\n","\n","for i, node_with_score in enumerate(results[:5]): # ìƒìœ„ 5ê°œë§Œ í™•ì¸\n","    node = node_with_score.node\n","    score = node_with_score.score\n","\n","    # ë‚´ìš©ì—ì„œ ì¤„ë°”ê¿ˆ(\\n)ì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í•œ ì¤„ë¡œ ë³´ê¸° ì¢‹ê²Œ ë§Œë“¦\n","    clean_content = node.get_content().replace('\\n', ' ')\n","\n","    print(f\"ğŸ… [Rank {i+1}] Score: {score:.4f}\")\n","\n","    # ë¬¸ì„œ ID (metadataì— ì—†ìœ¼ë©´ node_id ì‚¬ìš©)\n","    doc_id = node.metadata.get('docid') or node.metadata.get('document_id') or node.node_id\n","    print(f\"ğŸ“„ ë¬¸ì„œ ID: {doc_id}\")\n","\n","    # ë‚´ìš©ì€ 100ìê¹Œì§€ë§Œ ì¶œë ¥\n","    print(f\"ğŸ“ ë‚´ìš©: {clean_content[:100]}...\")\n","    print(\"-\" * 60)"],"metadata":{"id":"JDxwBHIQ52Cz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = dataset['train'][86]['question']\n","retrieved_nodes = fusion_retriever.retrieve(query)\n","\n","docs_for_rerank = [n.node.text for n in retrieved_nodes]\n","ids_for_rerank = [n.node.metadata['document_id'] for n in retrieved_nodes]\n","\n","reranked_results = reranker.rerank(query, docs_for_rerank, ids_for_rerank, top_k=5)\n","\n","final_context = \"\\n\\n---\\n\\n\".join([f\"[{i+1}] {d[0]}\" for i, d in enumerate(reranked_results[0])])\n","print(f'Q. {query}')\n","\n","print(reranked_results[1])\n"],"metadata":{"id":"0bm3k94P73nZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = dataset['train'][86]['question']\n","retrieved_nodes = dense_retriever.retrieve(query)\n","\n","docs_for_rerank = [n.node.text for n in retrieved_nodes]\n","ids_for_rerank = [n.node.metadata['document_id'] for n in retrieved_nodes]\n","\n","reranked_results = reranker.rerank(query, docs_for_rerank, ids_for_rerank, top_k=5)\n","\n","final_context = \"\\n\\n---\\n\\n\".join([f\"[{i+1}] {d[0]}\" for i, d in enumerate(reranked_results[0])])\n","print(f'Q. {query}')\n","\n","print(reranked_results[1])\n"],"metadata":{"id":"SHz5r8Gr8stH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZ7Aro0bRvw4"},"outputs":[],"source":["# 1. í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n","query = dataset['train'][307]['question']\n","print(f\"â“ ì§ˆë¬¸: {query}\")\n","print(\"-\" * 60)\n","\n","# 2. GLiNer ë‹¨ë… ê²€ìƒ‰ ì‹¤í–‰\n","# (ë‚´ë¶€ì—ì„œ print(f\"ğŸ” Query Tokens: ...\") ë¡œê·¸ê°€ ì°í ê²ë‹ˆë‹¤. ì´ê±¸ ê¼­ í™•ì¸í•˜ì„¸ìš”!)\n","gliner_results = dense_retriever.retrieve(query)\n","\n","# 3. ê²°ê³¼ ëœ¯ì–´ë³´ê¸°\n","print(f\"\\nğŸ” [Dense Only] ê²€ìƒ‰ ê²°ê³¼: {len(gliner_results)}ê°œ ë°œê²¬\\n\")\n","\n","for i, node_with_score in enumerate(gliner_results[:5]): # ìƒìœ„ 5ê°œë§Œ\n","    node = node_with_score.node\n","    score = node_with_score.score\n","\n","    print(f\"ğŸ… [Rank {i+1}] BM25 Score: {score:.4f}\")\n","    print(f\"ğŸ“„ ë¬¸ì„œ ID: {node.metadata.get('document_id', 'N/A')}\")\n","    # ê°€ë…ì„±ì„ ìœ„í•´ ì¤„ë°”ê¿ˆ ì œê±°í•˜ê³  100ìë§Œ ì¶œë ¥\n","    print(f\"ğŸ“ ë‚´ìš©: {node.text[:100].replace('\\n', ' ')}...\")\n","    print(\"-\" * 60)"]},{"cell_type":"code","source":["valid_set_dir = \"/data/train_dataset\"\n","valid_dataset = load_from_disk(valid_set_dir)"],"metadata":{"id":"OKR_I5BJ5BAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. ì—¬ê¸°ë¥¼ ë°”ê¿”ì£¼ì„¸ìš”! (notebook ì „ìš© tqdm ì‚¬ìš©)\n","from tqdm.notebook import tqdm\n","import json\n","import numpy as np\n","\n","result_for_test = []\n","\n","print(\"ğŸš€ Validation ë°ì´í„° í‰ê°€ ì‹œì‘...\")\n","\n","# 2. tqdm(...) ìœ¼ë¡œ ê°ì‹¸ë©´ ë©ë‹ˆë‹¤. (tqdm.tqdm ì•„ë‹˜)\n","for i in tqdm(range(len(valid_dataset['validation']))):\n","\n","    # ì§ˆë¬¸ê³¼ id\n","    test_q_query = valid_dataset['validation'][i]['question']\n","    test_q_id = valid_dataset['validation'][i]['id']\n","\n","    # ê³¨ë“ ë¦¬íŠ¸ë¦¬ë²„ ê·€ì—½ë‹¤ (Fusion ê²€ìƒ‰)\n","    retrieved_nodes_test = fusion_retriever.retrieve(test_q_query)\n","\n","    # data for reranker\n","    docs_for_rerank_test = [n.node.text for n in retrieved_nodes_test]\n","    # metadata ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n","    ids_for_rerank_test = [n.node.metadata.get('document_id', -1) for n in retrieved_nodes_test]\n","\n","    # rerank result\n","    reranked_results_test = reranker.rerank(test_q_query, docs_for_rerank_test, ids_for_rerank_test, top_k=5)\n","\n","    # ID ì¶”ì¶œ ë° int ë³€í™˜\n","    final_doc_ids = [int(did) for did in reranked_results_test[1]]\n","\n","    result_for_test.append([test_q_id, final_doc_ids])\n","\n","# ---------------------------------------------------------\n","# JSON ë³€í™˜ ë° ì €ì¥\n","# ---------------------------------------------------------\n","def convert_to_json(data):\n","    question_ids = []\n","    document_lists = []\n","\n","    for q_id, doc_list in data:\n","        question_ids.append(q_id)\n","        document_lists.append(doc_list)\n","    result_dict = {\n","        \"question_id\": question_ids,\n","        \"document_id\": document_lists\n","    }\n","    return result_dict\n","\n","json_test = convert_to_json(result_for_test)\n","\n","file_path = '/kiwi+ner+ìœ ì˜ì–´ ê²°ê³¼/validation_ner_hybrid.json'\n","\n","import os\n","os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","with open(file_path, 'w', encoding='utf-8') as f:\n","    json.dump(json_test, f, ensure_ascii=False, indent=4)\n","\n","print(f\"ğŸ’¾ ì €ì¥ ì™„ë£Œ! ê²½ë¡œ: {file_path}\")"],"metadata":{"id":"rPDBXTk_6MQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_set_dir = \"/data/test_dataset\"\n","test_dataset = load_from_disk(test_set_dir)"],"metadata":{"id":"hZuuTH4c-gHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_dataset['validation'][0])"],"metadata":{"id":"7LXMXMwS9sOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","import json\n","import numpy as np\n","import os\n","\n","# ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ (ì´ë¦„ ë³€ê²½)\n","test_results_final = []\n","\n","print(\"ğŸš€ Test ë°ì´í„° í‰ê°€ ì‹œì‘...\")\n","\n","# 1. ë°ì´í„°ì…‹ ë£¨í”„ (valid_dataset -> test_dataset['test'] ë¡œ ë³€ê²½)\n","for i in tqdm(range(len(test_dataset['validation']))):\n","\n","    # ì§ˆë¬¸ê³¼ id ê°€ì ¸ì˜¤ê¸°\n","    target_query = test_dataset['validation'][i]['question']\n","    target_id = test_dataset['validation'][i]['id']\n","\n","    # 2. ê³¨ë“ ë¦¬íŠ¸ë¦¬ë²„ (Fusion ê²€ìƒ‰)\n","    retrieved_nodes = fusion_retriever.retrieve(target_query)\n","\n","    # 3. Rerankerìš© ë°ì´í„° ì¤€ë¹„\n","    docs_for_rerank = [n.node.text for n in retrieved_nodes]\n","    # metadata ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n","    ids_for_rerank = [n.node.metadata.get('document_id', -1) for n in retrieved_nodes]\n","\n","    # 4. Rerank ì‹¤í–‰\n","    reranked_results = reranker.rerank(target_query, docs_for_rerank, ids_for_rerank, top_k=5)\n","\n","    # 5. ID ì¶”ì¶œ ë° int ë³€í™˜\n","    # reranked_results[1]ì— ì´ë¯¸ ì¤‘ë³µ ì œê±°ëœ ID ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ\n","    final_doc_ids = [int(did) for did in reranked_results[1]]\n","\n","    test_results_final.append([target_id, final_doc_ids])\n","\n","# ---------------------------------------------------------\n","# JSON ë³€í™˜ ë° ì €ì¥\n","# ---------------------------------------------------------\n","def convert_to_json(data):\n","    question_ids = []\n","    document_lists = []\n","\n","    for q_id, doc_list in data:\n","        question_ids.append(q_id)\n","        document_lists.append(doc_list)\n","\n","    result_dict = {\n","        \"question_id\": question_ids,\n","        \"document_id\": document_lists\n","    }\n","    return result_dict\n","\n","# ë³€í™˜\n","json_output = convert_to_json(test_results_final)\n","\n","# ğŸ’¾ íŒŒì¼ëª… ë³€ê²½: validation -> test\n","save_path = '/kiwi+ner+ìœ ì˜ì–´ ê²°ê³¼/test_ner_synonym_hybrid.json'\n","\n","# í´ë” ìƒì„± (ì•ˆì „ì¥ì¹˜)\n","os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","\n","# ì €ì¥\n","with open(save_path, 'w', encoding='utf-8') as f:\n","    json.dump(json_output, f, ensure_ascii=False, indent=4)\n","\n","print(f\"ğŸ’¾ Test ê²°ê³¼ ì €ì¥ ì™„ë£Œ! ê²½ë¡œ: {save_path}\")"],"metadata":{"id":"OCT-6H6F-klx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. ì—¬ê¸°ë¥¼ ë°”ê¿”ì£¼ì„¸ìš”! (notebook ì „ìš© tqdm ì‚¬ìš©)\n","# ì¼ì–´ë‚˜ì„œ ë‹¤ì‹œ ëŒë¦¬ê¸°\n","from tqdm.notebook import tqdm\n","import json\n","import numpy as np\n","\n","# ğŸš¨ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ë¦„ í™•ì¸\n","result_for_test_only = []\n","\n","print(\"ğŸš€ Validation ë°ì´í„° í‰ê°€ ì‹œì‘ (Sparse Only)...\")\n","\n","# 2. tqdm(...) ìœ¼ë¡œ ê°ì‹¸ë©´ ë©ë‹ˆë‹¤. (tqdm.tqdm ì•„ë‹˜)\n","for i in tqdm(range(len(valid_dataset['validation']))):\n","\n","    # ì§ˆë¬¸ê³¼ id\n","    test_q_query = valid_dataset['validation'][i]['question']\n","    test_q_id = valid_dataset['validation'][i]['id']\n","\n","    # ğŸš¨ final_retriever (Sparse Only) ì‚¬ìš© í™•ì¸\n","    retrieved_nodes_test = final_retriever.retrieve(test_q_query)\n","\n","    # data for reranker\n","    docs_for_rerank_test = [n.node.text for n in retrieved_nodes_test]\n","    # metadata ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n","    ids_for_rerank_test = [n.node.metadata.get('document_id', -1) for n in retrieved_nodes_test]\n","\n","    # rerank result\n","    reranked_results_test = reranker.rerank(test_q_query, docs_for_rerank_test, ids_for_rerank_test, top_k=5)\n","\n","    # ID ì¶”ì¶œ ë° int ë³€í™˜\n","    final_doc_ids = [int(did) for did in reranked_results_test[1]]\n","\n","    # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n","    result_for_test_only.append([test_q_id, final_doc_ids])\n","\n","# ---------------------------------------------------------\n","# JSON ë³€í™˜ ë° ì €ì¥\n","# ---------------------------------------------------------\n","def convert_to_json(data):\n","    question_ids = []\n","    document_lists = []\n","\n","    for q_id, doc_list in data:\n","        question_ids.append(q_id)\n","        document_lists.append(doc_list)\n","    result_dict = {\n","        \"question_id\": question_ids,\n","        \"document_id\": document_lists\n","    }\n","    return result_dict\n","\n","# ğŸš¨ [ìˆ˜ì • ì™„ë£Œ] ìœ„ì—ì„œ ë§Œë“  ë¦¬ìŠ¤íŠ¸(result_for_test_only)ë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤!\n","json_test = convert_to_json(result_for_test_only)\n","\n","file_path = '/kiwi+ner+ìœ ì˜ì–´ ê²°ê³¼/validation_ner_synonym_only.json'\n","\n","import os\n","os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","\n","with open(file_path, 'w', encoding='utf-8') as f:\n","    json.dump(json_test, f, ensure_ascii=False, indent=4)\n","\n","print(f\"ğŸ’¾ ì €ì¥ ì™„ë£Œ! ê²½ë¡œ: {file_path}\")"],"metadata":{"id":"KMd85Io2BwwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","import json\n","import numpy as np\n","import os\n","\n","# ğŸš¨ ë³€ìˆ˜ëª… ë³€ê²½: _onlyë¥¼ ë¶™ì—¬ì„œ ì´ì „ ë³€ìˆ˜ì™€ ê²¹ì¹˜ì§€ ì•Šê²Œ í•¨\n","test_results_final_only = []\n","\n","print(\"ğŸš€ Test ë°ì´í„° í‰ê°€ ì‹œì‘ (Sparse Only)...\")\n","\n","# 1. ë°ì´í„°ì…‹ ë£¨í”„\n","for i in tqdm(range(len(test_dataset['validation']))):\n","\n","    # ì§ˆë¬¸ê³¼ id ê°€ì ¸ì˜¤ê¸°\n","    target_query = test_dataset['validation'][i]['question']\n","    target_id = test_dataset['validation'][i]['id']\n","\n","    # 2. ê³¨ë“ ë¦¬íŠ¸ë¦¬ë²„ (final_retriever = Sparse Only)\n","    # ğŸš¨ ì—¬ê¸°ê°€ fusion_retrieverê°€ ì•„ë‹Œì§€ ê¼­ í™•ì¸! (í˜„ì¬ ì½”ë“œëŠ” final_retrieverë¼ ë§ìŒ)\n","    retrieved_nodes = final_retriever.retrieve(target_query)\n","\n","    # 3. Rerankerìš© ë°ì´í„° ì¤€ë¹„\n","    docs_for_rerank = [n.node.text for n in retrieved_nodes]\n","    # metadata ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n","    ids_for_rerank = [n.node.metadata.get('document_id', -1) for n in retrieved_nodes]\n","\n","    # 4. Rerank ì‹¤í–‰\n","    reranked_results = reranker.rerank(target_query, docs_for_rerank, ids_for_rerank, top_k=5)\n","\n","    # 5. ID ì¶”ì¶œ ë° int ë³€í™˜\n","    # reranked_results[1]ì— ì´ë¯¸ ì¤‘ë³µ ì œê±°ëœ ID ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ\n","    final_doc_ids = [int(did) for did in reranked_results[1]]\n","\n","    # ğŸš¨ ë³€ê²½ëœ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n","    test_results_final_only.append([target_id, final_doc_ids])\n","\n","# ---------------------------------------------------------\n","# JSON ë³€í™˜ ë° ì €ì¥\n","# ---------------------------------------------------------\n","def convert_to_json(data):\n","    question_ids = []\n","    document_lists = []\n","\n","    for q_id, doc_list in data:\n","        question_ids.append(q_id)\n","        document_lists.append(doc_list)\n","\n","    result_dict = {\n","        \"question_id\": question_ids,\n","        \"document_id\": document_lists\n","    }\n","    return result_dict\n","\n","# ğŸš¨ ë³€ê²½ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³€í™˜\n","json_output = convert_to_json(test_results_final_only)\n","\n","# ğŸ’¾ íŒŒì¼ëª… ì„¤ì •\n","save_path = '/kiwi+ner+ìœ ì˜ì–´ ê²°ê³¼/test_Ner_synonym_only.json'\n","\n","# í´ë” ìƒì„± (ì•ˆì „ì¥ì¹˜)\n","os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","\n","# ì €ì¥\n","with open(save_path, 'w', encoding='utf-8') as f:\n","    json.dump(json_output, f, ensure_ascii=False, indent=4)\n","\n","print(f\"ğŸ’¾ Test ê²°ê³¼(Only) ì €ì¥ ì™„ë£Œ! ê²½ë¡œ: {save_path}\")"],"metadata":{"id":"tpron2PEB-eO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mpgd2W3FEbap"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ì•„ë˜ëŠ” ë°±ì—… í›„ ìë£Œ í™•ë³´ë¥¼ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤."],"metadata":{"id":"23HBvGLnIcfD"}},{"cell_type":"code","source":["import pickle\n","import os\n","import bm25s\n","from llama_index.core.node_parser import SentenceSplitter\n","from llama_index.core import Document\n","from functools import partial\n","from kiwipiepy import Kiwi\n","\n","# =========================================================\n","# 1. Nodes(ë³¸ë¬¸) ë‹¤ì‹œ ë§Œë“¤ê¸° (CPU ì‘ì—… - ì•½ 2~3ë¶„ ì†Œìš”)\n","# =========================================================\n","# 2. ë„êµ¬(Tokenizer) ë‹¤ì‹œ ì¡°ë¦½\n","# =========================================================\n","# (ìœ„ì—ì„œ ì´ë¯¸ tokenizer, l2m_map, graph_retriever ê°ì²´ëŠ” ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•¨)\n","print(\"ğŸš€ 2. í† í¬ë‚˜ì´ì € ì¡°ë¦½ ì¤‘...\")\n","\n","query_tokenizer_rich = partial(\n","    tokenize_query_rich,\n","    tokenizer_obj=tokenizer,\n","    l2m_map_obj=l2m_map,\n","    graph_retriever_obj=graph_retriever\n",")\n","\n","kiwi_instance = Kiwi()\n","corpus_tokenizer = partial(\n","    tokenize_kiwi,\n","    kiwi=kiwi_instance,\n","    tag_include=kiwi_tags,\n","    text_type=\"corpus\",\n","    top_n=2,\n","    score_threshold=1.2,\n",")\n","# =========================================================\n","# 3. [í•µì‹¬] ì €ì¥ëœ ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì™€ì„œ í•©ì²´ (Indexing ìŠ¤í‚µ!)\n","# =========================================================\n","print(\"ğŸš€ 3. ì €ì¥ëœ BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\")\n","\n","# ì €ì¥ëœ ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ\n","loaded_bm25 = bm25s.BM25.load(save_path, load_corpus=False)\n","\n","# ê»ë°ê¸°ë§Œ ìƒì„± (__new__ ì‚¬ìš©) -> __init__ ì‹¤í–‰ ì•ˆ í•¨\n","final_retriever = KiwiWeightedBM25Retriever.__new__(KiwiWeightedBM25Retriever)\n","\n","# ë¶€í’ˆ ìˆ˜ë™ ì¡°ë¦½\n","final_retriever._nodes = nodes            # ë°©ê¸ˆ ë§Œë“  Nodes\n","final_retriever._bm25 = loaded_bm25       # ì €ì¥í•´ë‘” ì¸ë±ìŠ¤\n","final_retriever._query_tokenizer = query_tokenizer_rich # ì§ˆë¬¸ ì²˜ë¦¬ê¸°\n","final_retriever._similarity_top_k = 30\n","final_retriever._corpus_tokenizer = None  # ê²€ìƒ‰ë• í•„ìš” ì—†ìœ¼ë¯€ë¡œ None ì²˜ë¦¬\n","\n","print(\"ğŸ‰ final_retriever ë¶€í™œ ì™„ë£Œ! (30ë¶„ ê¸°ë‹¤ë¦´ í•„ìš” ì—†ìŒ)\")"],"metadata":{"id":"DmuSqA8V8ut2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) ê·¸ë˜í”„ ê²€ìƒ‰ê¸°: ë¬¸ì§€ê¸°ë¥¼ 1.0ìœ¼ë¡œ ë†’ì—¬ì„œ ìœ ì˜ì–´ ì°¨ë‹¨\n","graph_retriever_basic = GraphRetriever(synonym_graph, min_weight=1.0)\n","\n","# 2) í† í¬ë‚˜ì´ì €: ê¸°ë³¸ ê·¸ë˜í”„ ê²€ìƒ‰ê¸° ì¥ì°©\n","tokenizer_basic = partial(\n","    tokenize_query_rich,\n","    tokenizer_obj=tokenizer,\n","    l2m_map_obj=l2m_map,\n","    graph_retriever_obj=graph_retriever_basic, # ğŸš¨ 1.0 ì§œë¦¬\n",")\n","\n","# 3) ë¦¬íŠ¸ë¦¬ë²„ ì¡°ë¦½\n","retriever_basic = KiwiWeightedBM25Retriever.__new__(KiwiWeightedBM25Retriever)\n","retriever_basic._nodes = nodes\n","retriever_basic._bm25 = loaded_bm25\n","retriever_basic._query_tokenizer = tokenizer_basic\n","retriever_basic._similarity_top_k = 30\n","retriever_basic._corpus_tokenizer = None\n","\n","# =========================================================\n","# ğŸš€ 2. [Rich] í™•ì¥ ê²€ìƒ‰ê¸° ìƒì„± (ìœ ì˜ì–´ O)\n","# =========================================================\n","print(\"ğŸ‡ 2. [Rich] í™•ì¥ ê²€ìƒ‰ê¸° ì¡°ë¦½ ì¤‘ (min_weight=0.7)...\")\n","\n","# 1) ê·¸ë˜í”„ ê²€ìƒ‰ê¸°: ë¬¸ì§€ê¸°ë¥¼ 0.7ë¡œ ë‚®ì¶°ì„œ ìœ ì˜ì–´ í†µê³¼\n","graph_retriever_rich = GraphRetriever(synonym_graph, min_weight=0.7)\n","\n","# 2) í† í¬ë‚˜ì´ì €: í™•ì¥ ê·¸ë˜í”„ ê²€ìƒ‰ê¸° ì¥ì°©\n","tokenizer_rich = partial(\n","    tokenize_query_rich,\n","    tokenizer_obj=tokenizer,\n","    l2m_map_obj=l2m_map,\n","    graph_retriever_obj=graph_retriever_rich, # ğŸš¨ 0.7 ì§œë¦¬\n",")\n","\n","# 3) ë¦¬íŠ¸ë¦¬ë²„ ì¡°ë¦½\n","retriever_rich = KiwiWeightedBM25Retriever.__new__(KiwiWeightedBM25Retriever)\n","retriever_rich._nodes = nodes\n","retriever_rich._bm25 = loaded_bm25\n","retriever_rich._query_tokenizer = tokenizer_rich\n","retriever_rich._similarity_top_k = 30\n","retriever_rich._corpus_tokenizer = None\n","\n"],"metadata":{"id":"zDIrz18Igksj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","# -------------------------------------------------------\n","# 1. ìˆ˜ìƒ‰ ë²”ìœ„ ì„¤ì •\n","# -------------------------------------------------------\n","start_idx = 306\n","end_idx = 307  # 200ê°œë§Œ ë¨¼ì € ë´…ì‹œë‹¤\n","improvement_cases = []\n","\n","print(f\"ğŸ•µï¸â€â™‚ï¸ ì‹¬ì¸µ ìˆ˜ìƒ‰ ì‹œì‘ (í† í¬ë‚˜ì´ì§• ë¹„êµ í¬í•¨): {start_idx} ~ {end_idx}\")\n","\n","# -------------------------------------------------------\n","# 2. ìˆ˜ìƒ‰ ë£¨í”„\n","# -------------------------------------------------------\n","for i in tqdm(range(start_idx, end_idx)):\n","    if i >= len(dataset['train']): break\n","\n","    item = dataset['train'][i]\n","    query = item['question']\n","    gt_doc_id = int(item['document_id'])\n","\n","    # ---------------------------------------------------\n","    # ğŸ” [NEW] í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë¯¸ë¦¬ ìº¡ì²˜!\n","    # ---------------------------------------------------\n","    # A. Basic í† í° (threshold=1.0 ì´ë¼ ì›ë³¸ë§Œ ë‚˜ì˜´)\n","    tokens_basic_raw = retriever_basic._query_tokenizer(query)\n","    tokens_basic_list = list(tokens_basic_raw.keys()) # í‚¤ë§Œ ê°€ì ¸ì˜´\n","\n","    # B. Rich í† í° (threshold=0.0 ì´ë¼ ìœ ì˜ì–´ í¬í•¨)\n","    tokens_rich_raw = retriever_rich._query_tokenizer(query)\n","    # ê°€ë…ì„±ì„ ìœ„í•´ ìœ ì˜ì–´ëŠ” 'ë‹¨ì–´(0.7)' í˜•íƒœë¡œ í¬ë§·íŒ…\n","    tokens_rich_fmt = []\n","    for t, s in tokens_rich_raw.items():\n","        if s < 1.0:\n","            tokens_rich_fmt.append(f\"{t}({s:.1f})\") # ìœ ì˜ì–´ í‘œì‹œ\n","        else:\n","            tokens_rich_fmt.append(t) # ì›ë³¸\n","\n","    # ---------------------------------------------------\n","    # 3. ê²€ìƒ‰ ì‹¤í–‰\n","    # ---------------------------------------------------\n","    res_basic = retriever_basic.retrieve(query)\n","    ids_basic = [int(n.node.metadata.get('docid') or n.node.metadata.get('document_id')) for n in res_basic]\n","\n","    res_rich = retriever_rich.retrieve(query)\n","    ids_rich = [int(n.node.metadata.get('docid') or n.node.metadata.get('document_id')) for n in res_rich]\n","\n","    # ìˆœìœ„ ê³„ì‚°\n","    basic_rank = ids_basic.index(gt_doc_id) if gt_doc_id in ids_basic else 100\n","    rich_rank = ids_rich.index(gt_doc_id) if gt_doc_id in ids_rich else 100\n","\n","    # ---------------------------------------------------\n","    # 4. ì¡°ê±´ ê²€ì‚¬: Richê°€ ë” ì˜í–ˆëŠ”ê°€?\n","    # ---------------------------------------------------\n","    if rich_rank < basic_rank:\n","\n","        # ìœ ì˜ì–´ê°€ ì‹¤ì œë¡œ ë¬¸ì„œì— ìˆëŠ”ì§€ í™•ì¸\n","        synonyms = {t for t, s in tokens_rich_raw.items() if s < 1.0}\n","\n","        target_node = next((n.node for n in res_rich if int(n.node.metadata.get('docid') or n.node.metadata.get('document_id')) == gt_doc_id), None)\n","\n","        if target_node:\n","            doc_text = target_node.get_content()\n","            hit_synonyms = [syn for syn in synonyms if syn in doc_text]\n","\n","            # ìœ ì˜ì–´ê°€ í•˜ë‚˜ë¼ë„ ì ì¤‘í–ˆê±°ë‚˜, í˜¹ì€ ìˆœìœ„ê°€ ë“œë¼ë§ˆí‹±í•˜ê²Œ ì˜¬ëë‹¤ë©´ ì €ì¥\n","            if hit_synonyms or (basic_rank > 10 and rich_rank <= 5):\n","                improvement_cases.append({\n","                    'index': i,\n","                    'query': query,\n","                    'doc_id': gt_doc_id,\n","                    'basic_rank': basic_rank + 1,\n","                    'rich_rank': rich_rank + 1,\n","                    'diff': basic_rank - rich_rank,\n","                    'hit_synonyms': hit_synonyms,\n","                    'snippet': doc_text[:80].replace('\\n', ' '),\n","                    # ğŸ“ í† í° ì •ë³´ ì €ì¥\n","                    'tokens_basic': tokens_basic_list,\n","                    'tokens_rich': tokens_rich_fmt\n","                })\n","\n","# -------------------------------------------------------\n","# 5. ê²°ê³¼ ë°œí‘œ\n","# -------------------------------------------------------\n","improvement_cases.sort(key=lambda x: x['diff'], reverse=True)\n","\n","print(f\"\\n ì´ {len(improvement_cases)}ê°œì˜ ì‚¬ë¡€ ë°œê²¬!\\n\")\n","\n","for case in improvement_cases[:5]: # ìƒìœ„ 5ê°œ\n","    print(f\" [Q{case['index']}] {case['query']}\")\n","    print(f\"   ğŸ“„ ì •ë‹µ ID: {case['doc_id']}\")\n","\n","    b_rank = \"ìˆœìœ„ê¶Œ ë°–\" if case['basic_rank'] > 30 else f\"{case['basic_rank']}ìœ„\"\n","    print(f\"    ìˆœìœ„ ë³€í™”: Basic({b_rank}) -> Rich({case['rich_rank']}ìœ„) ( {case['diff']} ìƒìŠ¹)\")\n","\n","    print(f\"    [Basic í† í°]: {case['tokens_basic']}\")\n","    print(f\"    [Rich í† í°] : {case['tokens_rich']}\")\n","\n","    if case['hit_synonyms']:\n","        print(f\"    [ì ì¤‘ ìœ ì˜ì–´]: {case['hit_synonyms']}\")\n","    else:\n","        print(f\"    (ìœ ì˜ì–´ ì§ì ‘ ë§¤ì¹­ì€ ì—†ì§€ë§Œ, ë‹¤ë¥¸ í…€ ê°€ì¤‘ì¹˜ ë³€ê²½ìœ¼ë¡œ ìƒìŠ¹ ì¶”ì •)\")\n","\n","    print(f\"   ğŸ“ ë¬¸ì„œ: {case['snippet']}...\")\n","    print(\"=\" * 60)"],"metadata":{"id":"kVr7lSaTjfNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","# -------------------------------------------------------\n","# 1. ìˆ˜ìƒ‰ ë²”ìœ„ ì„¤ì •\n","# -------------------------------------------------------\n","start_idx = 306\n","end_idx = 307\n","print(f\"[System] Search started: Index {start_idx} ~ {end_idx}\")\n","\n","# -------------------------------------------------------\n","# 2. ìˆ˜ìƒ‰ ë£¨í”„\n","# -------------------------------------------------------\n","for i in tqdm(range(start_idx, end_idx)):\n","    if i >= len(dataset['train']): break\n","\n","    item = dataset['train'][i]\n","    query = item['question']\n","    gt_doc_id = int(item['document_id'])\n","    gt_answer = item['answers']['text'][0] # ì •ë‹µ í…ìŠ¤íŠ¸\n","\n","    # ---------------------------------------------------\n","    # A. í† í¬ë‚˜ì´ì§• ê²°ê³¼ ìº¡ì²˜\n","    # ---------------------------------------------------\n","    # Basic (ìœ ì˜ì–´ X)\n","    tokens_basic_raw = retriever_basic._query_tokenizer(query)\n","    tokens_basic_list = list(tokens_basic_raw.keys())\n","\n","    # Rich (ìœ ì˜ì–´ O)\n","    tokens_rich_raw = retriever_rich._query_tokenizer(query)\n","    tokens_rich_fmt = []\n","    rich_synonyms_only = set()\n","\n","    for t, s in tokens_rich_raw.items():\n","        if s < 1.0:\n","            tokens_rich_fmt.append(f\"{t}({s:.1f})\")\n","            rich_synonyms_only.add(t)\n","        else:\n","            tokens_rich_fmt.append(t)\n","\n","    # ---------------------------------------------------\n","    # B. ê²€ìƒ‰ ì‹¤í–‰ ë° ìˆœìœ„ ê³„ì‚°\n","    # ---------------------------------------------------\n","    res_basic = retriever_basic.retrieve(query)\n","    ids_basic = [int(n.node.metadata.get('docid') or n.node.metadata.get('document_id')) for n in res_basic]\n","\n","    res_rich = retriever_rich.retrieve(query)\n","    ids_rich = [int(n.node.metadata.get('docid') or n.node.metadata.get('document_id')) for n in res_rich]\n","\n","    # ìˆœìœ„ (ì—†ìœ¼ë©´ 100ìœ„ ì²˜ë¦¬)\n","    basic_rank = ids_basic.index(gt_doc_id) if gt_doc_id in ids_basic else 100\n","    rich_rank = ids_rich.index(gt_doc_id) if gt_doc_id in ids_rich else 100\n","\n","    # ---------------------------------------------------\n","    # C. ì¡°ê±´ ê²€ì‚¬: Rich ìˆœìœ„ê°€ Basicë³´ë‹¤ ë†’ì€ê°€?\n","    # ---------------------------------------------------\n","    if rich_rank < basic_rank:\n","\n","        # ---------------------------------------------------\n","        # D. ë¬¸ì„œ ì „ì²´ ë‚´ìš©(Full Text) ê°€ì ¸ì˜¤ê¸°\n","        # ---------------------------------------------------\n","        # ê²€ìƒ‰ëœ ì²­í¬ë§Œ ë³´ëŠ”ê²Œ ì•„ë‹ˆë¼, í•´ë‹¹ IDì˜ ì›ë³¸ ì „ì²´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n","        full_doc_text = \"\"\n","        found_chunks = 0\n","\n","        for node in nodes:\n","            did = node.metadata.get('docid') or node.metadata.get('document_id')\n","            if did and int(did) == gt_doc_id:\n","                full_doc_text += node.get_content() + \"\\n\"\n","                found_chunks += 1\n","\n","        if not full_doc_text:\n","            full_doc_text = \"(Error: Document not found in nodes list)\"\n","\n","        # ---------------------------------------------------\n","        # E. ìƒì„¸ ë¶„ì„ (ìœ ì˜ì–´ & ì •ë‹µ ë§¤ì¹­)\n","        # ---------------------------------------------------\n","\n","        # 1) ìœ ì˜ì–´ ì ì¤‘ í™•ì¸\n","        hit_synonyms = [syn for syn in rich_synonyms_only if syn in full_doc_text]\n","\n","        # 2) ì •ë‹µ í¬í•¨ ì—¬ë¶€ í™•ì¸\n","        has_answer = gt_answer in full_doc_text\n","\n","        # 3) ì¶œë ¥ìš© ìŠ¤ë‹ˆí« ìƒì„± (ì •ë‹µ ì£¼ë³€ or ìœ ì˜ì–´ ì£¼ë³€)\n","        snippet = \"\"\n","        if has_answer:\n","            idx = full_doc_text.find(gt_answer)\n","            start = max(0, idx - 40)\n","            end = min(len(full_doc_text), idx + len(gt_answer) + 40)\n","            snippet = full_doc_text[start:end].replace('\\n', ' ')\n","            # ì •ë‹µ ê°•ì¡° í‘œì‹œ\n","            snippet = snippet.replace(gt_answer, f\" **[{gt_answer}]** \")\n","        elif hit_synonyms:\n","            # ì •ë‹µì€ ëª» ì°¾ì•˜ì§€ë§Œ ìœ ì˜ì–´ê°€ ìˆëŠ” ê²½ìš° ìœ ì˜ì–´ ì£¼ë³€ í‘œì‹œ\n","            idx = full_doc_text.find(hit_synonyms[0])\n","            start = max(0, idx - 40)\n","            end = min(len(full_doc_text), idx + 20)\n","            snippet = full_doc_text[start:end].replace('\\n', ' ')\n","        else:\n","            snippet = full_doc_text[:100].replace('\\n', ' ')\n","\n","        # ---------------------------------------------------\n","        # F. ê²°ê³¼ ì¶œë ¥ (ì´ëª¨í‹°ì½˜ ì œì™¸)\n","        # ---------------------------------------------------\n","        print(\"=\" * 80)\n","        print(f\"[Case Analysis] Q{i} : {query}\")\n","        print(f\" - Doc ID: {gt_doc_id}\")\n","        print(f\" - Ground Truth: {gt_answer}\")\n","        print(\"-\" * 80)\n","\n","        print(f\"[Rank Change]\")\n","        b_rank_str = \"Out of Rank (>30)\" if basic_rank > 30 else f\"{basic_rank}\"\n","        print(f\" - Basic: {b_rank_str} -> Rich: {rich_rank} (Diff: +{basic_rank - rich_rank})\")\n","\n","        print(f\"[Token Analysis]\")\n","        print(f\" - Basic Tokens: {tokens_basic_list}\")\n","        print(f\" - Rich Tokens : {tokens_rich_fmt}\")\n","        print(f\" - Hit Synonyms: {hit_synonyms if hit_synonyms else 'None (Rank boost by other factors)'}\")\n","\n","        print(f\"[Fact Check]\")\n","        print(f\" - Answer in Doc: {'YES' if has_answer else 'NO'}\")\n","        if has_answer:\n","            print(f\" - Context Snippet: ...{snippet}...\")\n","        else:\n","            print(f\" - Context Snippet: ...{snippet}...\")\n","\n","        print(\"=\" * 80)\n","        print(\"\\n\")"],"metadata":{"id":"2bwz3bnOsc8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ í•˜ì‹  ê±°)\n","query = dataset['train'][123]['question']\n","tokens_dict = target_retriever._query_tokenizer(query)\n","\n","# 2. ì˜¤ë¦¬ì§€ë„ vs ìœ ì˜ì–´ ë¶„ë¦¬ (ê°€ì¤‘ì¹˜ë¡œ êµ¬ë¶„)\n","# (ë³´í†µ ì›ë³¸ì€ 1.0 ì´ìƒ, ìœ ì˜ì–´ëŠ” 1.0 ë¯¸ë§Œìœ¼ë¡œ ì„¤ì •í•˜ì…¨ì„ í…Œë‹ˆ)\n","original_keywords = {k for k, v in tokens_dict.items() if v >= 1.0}\n","synonym_keywords = {k for k, v in tokens_dict.items() if v < 1.0}\n","\n","print(f\"ğŸ”¹ ì›ë³¸ í‚¤ì›Œë“œ ({len(original_keywords)}ê°œ): {list(original_keywords)[:5]}...\")\n","print(f\"ğŸ”¸ ìœ ì˜ì–´ í‚¤ì›Œë“œ ({len(synonym_keywords)}ê°œ): {list(synonym_keywords)[:5]}...\")\n","print(\"=\" * 60)\n","\n","# 3. ê²€ìƒ‰ ê²°ê³¼ í•˜ë‚˜ì”© ëœ¯ì–´ë³´ë©° ìœ ì˜ì–´ ì°¾ê¸°\n","print(\"ğŸ” [ìœ ì˜ì–´ ì ì¤‘ ë¶„ì„ ì‹œì‘]\\n\")\n","\n","for i, node_with_score in enumerate(results[:5]): # ìƒìœ„ 5ê°œë§Œ\n","    full_text = node_with_score.node.get_content()\n","    doc_id = node_with_score.node.metadata.get('docid') or \"N/A\"\n","\n","    # í•´ë‹¹ ë¬¸ì„œì— í¬í•¨ëœ ë‹¨ì–´ ì°¾ê¸°\n","    hit_originals = [w for w in original_keywords if w in full_text]\n","    hit_synonyms = [w for w in synonym_keywords if w in full_text]\n","\n","    print(f\"ğŸ“„ [Rank {i+1}] (ID: {doc_id}) Score: {node_with_score.score:.4f}\")\n","\n","    # ğŸ¯ í•µì‹¬: ìœ ì˜ì–´ê°€ ìˆìœ¼ë©´ ê°•ì¡°!\n","    if hit_synonyms:\n","        print(f\"   ğŸ”¥ [ìœ ì˜ì–´ ì ì¤‘!] ë°œê²¬ëœ ìœ ì˜ì–´: {hit_synonyms}\")\n","    else:\n","        print(f\"   ğŸ’¨ (ìœ ì˜ì–´ ì—†ìŒ - ì›ë³¸ ë‹¨ì–´ë§Œ ë§¤ì¹­ë¨)\")\n","\n","    print(f\"   âœ… [ì›ë³¸ ë‹¨ì–´ ë§¤ì¹­]: {hit_originals}\")\n","\n","    # ë¬¸ë§¥ í™•ì¸ (ìœ ì˜ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶œë ¥)\n","    if hit_synonyms:\n","        print(\"   ğŸ‘€ [ë¬¸ë§¥ í™•ì¸]:\")\n","        for syn in hit_synonyms:\n","            # ìœ ì˜ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ ì£¼ë³€ 30ìë§Œ ì˜ë¼ì„œ ë³´ì—¬ì£¼ê¸°\n","            idx = full_text.find(syn)\n","            start = max(0, idx - 20)\n","            end = min(len(full_text), idx + 20)\n","            snippet = full_text[start:end].replace('\\n', ' ')\n","            print(f\"      ...{snippet}...\")\n","\n","    print(\"-\" * 60)"],"metadata":{"id":"_oydFtvKcuo2"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyM+pS0rO1ipX50+gy9cX2wZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}