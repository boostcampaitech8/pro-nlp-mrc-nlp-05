{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ í™˜ê²½ ì„¤ì • ì‹œì‘!\n",
      "GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: True\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from typing import List, Dict, Callable, Optional\n",
    "import numpy as np\n",
    "import json\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "# LlamaIndex ê´€ë ¨\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import QueryFusionRetriever, BaseRetriever\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.schema import TextNode, NodeWithScore, QueryBundle, BaseNode\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "# ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import bm25s\n",
    "from gliner import GLiNER\n",
    "from sentence_transformers import CrossEncoder \n",
    "\n",
    "# Hugging Face ë¡œê·¸ì¸\n",
    "print(\"ğŸš€ í™˜ê²½ ì„¤ì • ì‹œì‘!\")\n",
    "HF_TOKEN = \"\"\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = HF_TOKEN \n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki data load\n",
    "with open('/data/ephemeral/home/data/wikipedia_documents.json') as f:\n",
    "    wiki_data = json.load(f)\n",
    "id_to_title = {v[\"document_id\"]: v[\"title\"] for v in wiki_data.values()}\n",
    "\n",
    "\n",
    "train_set_dir = \"/data/ephemeral/home/data/train_dataset/\"\n",
    "dataset = load_from_disk(train_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ëª¨ë¸ ë¡œë“œ ì„¤ì • ---\n",
    "GEMMA_MODEL_NAME = \"google/gemma-3-4b-it\"  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ 4b ëŒ€ì‹  9bë¥¼ ì˜ˆì‹œë¡œ ì‚¬ìš© (ì‚¬ìš©ì í™˜ê²½ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥)\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "RERANKER_MODEL_NAME = \"BAAI/bge-reranker-v2-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM ë° Tokenizer ë¡œë“œ ---\n",
    "def load_gemma():\n",
    "    \"\"\"Gemma ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    # Q: Gemma 3-4b-it ì‚¬ìš© ì˜ˆì •ì´ì—ˆëŠ”ë°, í˜„ì¬ëŠ” Gemma 2-9b-itì„ ì‚¬ìš©í•˜ë ¤ í•©ë‹ˆë‹¤.\n",
    "    # A: VRAM ìƒí™©ì— ë”°ë¼ ëª¨ë¸ ì´ë¦„ì„ ì ì ˆíˆ ë³€ê²½í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        GEMMA_MODEL_NAME,\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents: List[Document] = []\n",
    "for doc_id, data in wiki_data.items():\n",
    "    # 'text' í•„ë“œë¥¼ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©\n",
    "    documents.append(\n",
    "        Document(\n",
    "            text=data['text'],\n",
    "            metadata={\n",
    "                \"document_id\": data['document_id'],\n",
    "                \"title\": data['title'],\n",
    "                \"corpus_source\": data['corpus_source']\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: 60613ê°œ\n",
      "ìƒì„±ëœ ì²­í¬(Node) ê°œìˆ˜: 128727ê°œ\n",
      "ì²« ë²ˆì§¸ ì²­í¬ í…ìŠ¤íŠ¸ ì˜ˆì‹œ: ì´ ë¬¸ì„œëŠ” ë‚˜ë¼ ëª©ë¡ì´ë©°, ì „ ì„¸ê³„ 206ê°œ ë‚˜ë¼ì˜ ê° í˜„í™©ê³¼ ì£¼ê¶Œ ìŠ¹ì¸ ì •ë³´ë¥¼ ê°œìš” í˜•íƒœë¡œ ë‚˜ì—´í•˜ê³  ìˆë‹¤.\n",
      "\n",
      "ì´ ëª©ë¡ì€ ëª…ë£Œí™”ë¥¼ ìœ„í•´ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆë‹¤.\n",
      "\n",
      "# ì²« ë²ˆì§¸ ë¶€...\n"
     ]
    }
   ],
   "source": [
    "# 3. ë¬¸ì„œ ì²­í‚¹ (Node ìƒì„±)\n",
    "# SentenceSplitterëŠ” ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©´ì„œ, \n",
    "# ìµœì¢… ì²­í¬ í¬ê¸°ë¥¼ chunk_size=512ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "# nodesì—ëŠ” ì‘ì€ í…ìŠ¤íŠ¸ ì²­í¬(TextNode)ë“¤ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë‹´ê¹ë‹ˆë‹¤.\n",
    "nodes: List[TextNode] = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}ê°œ\")\n",
    "print(f\"ìƒì„±ëœ ì²­í¬(Node) ê°œìˆ˜: {len(nodes)}ê°œ\")\n",
    "print(f\"ì²« ë²ˆì§¸ ì²­í¬ í…ìŠ¤íŠ¸ ì˜ˆì‹œ: {nodes[0].get_content()[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VectorStoreIndex ìƒì„± ì‹œì‘ (ì„ë² ë”© ì¤‘) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- VectorStoreIndex ìƒì„± ì‹œì‘ (ì„ë² ë”© ì¤‘) ---\")\n",
    "#vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss vs\n",
    "\n",
    "dummy_emb = embed_model.get_text_embedding(\"dim ì²´í¬ìš©\")\n",
    "dim = len(dummy_emb)\n",
    "faiss_index = faiss.IndexFlatIP(dim) \n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Reranker (ì‚¬ìš©ì ì •ì˜) ---\n",
    "from sentence_transformers import CrossEncoder \n",
    "\n",
    "class Reranker:\n",
    "    def __init__(self, model_name: str = RERANKER_MODEL_NAME):\n",
    "        self.model = CrossEncoder(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def rerank(self, query: str, docs: List[Dict], doc_id, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        queryì™€ docs[{'text': ..., ...}]ë¥¼ ë°›ì•„, score ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•´ì„œ top_kë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            return []\n",
    "\n",
    "        pairs = [[query, d] for d in docs]\n",
    "        scores = self.model.predict(pairs)  # shape (len(docs),)\n",
    "        scored_docs = list(zip(docs, scores))\n",
    "        scored_id = list(zip(doc_id, scores)) \n",
    "\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        scored_id.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return scored_docs[:top_k], scored_id[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = Reranker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886cee6f77fd434899462b64c0c0470b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_gemma()\n",
    "gemma_llm = HuggingFaceLLM(\n",
    "    # model_nameì„ ì§€ì •í•  í•„ìš”ê°€ ì—†ê±°ë‚˜, ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•´ë„ model/tokenizer ì¸ìê°€ ìš°ì„ ë©ë‹ˆë‹¤.\n",
    "    model=model,        # ì´ë¯¸ ë¡œë“œëœ PyTorch ëª¨ë¸ ê°ì²´\n",
    "    tokenizer=tokenizer,  # ì´ë¯¸ ë¡œë“œëœ Tokenizer ê°ì²´\n",
    "    #device_map==\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # device_map=\"auto\" ë“±ì€ ì´ë¯¸ model ë¡œë“œ ì‹œ ì ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ LlamaIndex LLMì—ì„œëŠ” ë¶ˆí•„ìš”\n",
    "    \n",
    "    # í…œí”Œë¦¿ ì²˜ë¦¬ ë°©ì‹ ë“± LlamaIndex ê´€ë ¨ ì„¤ì •ë§Œ ì¶”ê°€\n",
    "    context_window=8192, # ì˜ˆì‹œ: Gemmaì˜ Context Window ì„¤ì • (í•„ìš”ì— ë”°ë¼)\n",
    ")\n",
    "\n",
    "# 3. LlamaIndex ì„¤ì •ì— ì ìš©\n",
    "Settings.llm = gemma_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ ì²­ì†Œ ì „ ë©”ëª¨ë¦¬: 19.93 GB\n",
      "âœ¨ ì²­ì†Œ í›„ ë©”ëª¨ë¦¬: 17.82 GB\n",
      "   -> ì´ì œ GLiNerê°€ ìš´ë™ì¥ì„ í˜¼ì ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# print(f\"ğŸ§¹ ì²­ì†Œ ì „ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# # 1. LLM (Gemma) ë°© ë¹¼!\n",
    "# if 'gemma_llm' in locals(): del gemma_llm\n",
    "# if 'model' in locals(): del model  # LLM ì›ë³¸ ëª¨ë¸ ê°ì²´\n",
    "# if 'tokenizer' in locals(): del tokenizer # LLM í† í¬ë‚˜ì´ì €\n",
    "\n",
    "# # 2. Reranker ë°© ë¹¼!\n",
    "# if 'reranker' in locals(): del reranker\n",
    "\n",
    "# # 3. Embedding Model ë°© ë¹¼! (ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ëë‚¬ë‹¤ë©´)\n",
    "# if 'embed_model' in locals(): del embed_model\n",
    "\n",
    "# # 4. ê°•ì œ ìˆ˜ê±° (Garbage Collection)\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(f\"âœ¨ ì²­ì†Œ í›„ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "# print(\"   -> ì´ì œ GLiNerê°€ ìš´ë™ì¥ì„ í˜¼ì ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ GLiNer XL ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mcr/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1263e228e4e4a92b16d3dff1bead803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/mcr/.venv/lib/python3.12/site-packages/gliner/model.py:418: UserWarning: Resizing embeddings is not supported for bi-encoder models.\n",
      "  instance.resize_embeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GLiNer XL ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”¥ GLiNer XL ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "# 1. ë¼ë²¨ ì •ì˜\n",
    "entity_type_mapping = {\n",
    "    \"PS\": { \"PS_NAME\": \"ì¸ë¬¼_ì‚¬ëŒ\", \"PS_CHARACTER\": \"ì¸ë¬¼_ê°€ìƒ ìºë¦­í„°\", \"PS_PET\": \"ì¸ë¬¼_ë°˜ë ¤ë™ë¬¼\"},\n",
    "    \"FD\": { \"FD_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ê³¼í•™\", \"FD_SOCIAL_SCIENCE\": \"í•™ë¬¸ ë¶„ì•¼_ì‚¬íšŒê³¼í•™\", \"FD_MEDICINE\": \"í•™ë¬¸ ë¶„ì•¼_ì˜í•™\", \"FD_ART\": \"í•™ë¬¸ ë¶„ì•¼_ì˜ˆìˆ \", \"FD_HUMANITIES\": \"í•™ë¬¸ ë¶„ì•¼_ì¸ë¬¸í•™\", \"FD_OTHERS\": \"í•™ë¬¸ ë¶„ì•¼_ê¸°íƒ€\"},\n",
    "    \"TR\": { \"TR_SCIENCE\": \"ì´ë¡ _ê³¼í•™\", \"TR_SOCIAL_SCIENCE\": \"ì´ë¡ _ì‚¬íšŒê³¼í•™\", \"TR_MEDICINE\": \"ì´ë¡ _ì˜í•™\", \"TR_ART\": \"ì´ë¡ _ì˜ˆìˆ \", \"TR_HUMANITIES\": \"ì´ë¡ _ì² í•™/ì–¸ì–´/ì—­ì‚¬\", \"TR_OTHERS\": \"ì´ë¡ _ê¸°íƒ€\"},\n",
    "    \"AF\": { \"AF_BUILDING\": \"ì¸ê³µë¬¼_ê±´ì¶•ë¬¼/í† ëª©ê±´ì„¤ë¬¼\", \"AF_CULTURAL_ASSET\": \"ì¸ê³µë¬¼_ë¬¸í™”ì¬\", \"AF_ROAD\": \"ì¸ê³µë¬¼_ë„ë¡œ/ì² ë¡œ\", \"AF_TRANSPORT\": \"ì¸ê³µë¬¼_êµí†µìˆ˜ë‹¨/ìš´ì†¡ìˆ˜ë‹¨\", \"AF_MUSICAL_INSTRUMENT\": \"ì¸ê³µë¬¼_ì•…ê¸°\", \"AF_WEAPON\": \"ì¸ê³µë¬¼_ë¬´ê¸°\", \"AFA_DOCUMENT\": \"ì¸ê³µë¬¼_ë„ì„œ/ì„œì  ì‘í’ˆëª…\", \"AFA_PERFORMANCE\": \"ì¸ê³µë¬¼_ì¶¤/ê³µì—°/ì—°ê·¹ ì‘í’ˆëª…\", \"AFA_VIDEO\": \"ì¸ê³µë¬¼_ì˜í™”/TV í”„ë¡œê·¸ë¨\", \"AFA_ART_CRAFT\": \"ì¸ê³µë¬¼_ë¯¸ìˆ /ì¡°í˜• ì‘í’ˆëª…\", \"AFA_MUSIC\": \"ì¸ê³µë¬¼_ìŒì•… ì‘í’ˆëª…\", \"AFW_SERVICE_PRODUCTS\": \"ì¸ê³µë¬¼_ì„œë¹„ìŠ¤ ìƒí’ˆ\", \"AFW_OTHER_PRODUCTS\": \"ì¸ê³µë¬¼_ê¸°íƒ€ ìƒí’ˆ\"},\n",
    "    \"OG\": { \"OGG_ECONOMY\": \"ê¸°ê´€_ê²½ì œ\", \"OGG_EDUCATION\": \"ê¸°ê´€_êµìœ¡\", \"OGG_MILITARY\": \"ê¸°ê´€_êµ°ì‚¬\", \"OGG_MEDIA\": \"ê¸°ê´€_ë¯¸ë””ì–´\", \"OGG_SPORTS\": \"ê¸°ê´€_ìŠ¤í¬ì¸ \", \"OGG_ART\": \"ê¸°ê´€_ì˜ˆìˆ \", \"OGG_MEDICINE\": \"ê¸°ê´€_ì˜ë£Œ\", \"OGG_RELIGION\": \"ê¸°ê´€_ì¢…êµ\", \"OGG_SCIENCE\": \"ê¸°ê´€_ê³¼í•™\", \"OGG_LIBRARY\": \"ê¸°ê´€_ë„ì„œê´€\", \"OGG_LAW\": \"ê¸°ê´€_ë²•ë¥ \", \"OGG_POLITICS\": \"ê¸°ê´€_ì •ë¶€/ê³µê³µ\", \"OGG_FOOD\": \"ê¸°ê´€_ìŒì‹ ì—…ì²´\", \"OGG_HOTEL\": \"ê¸°ê´€_ìˆ™ë°• ì—…ì²´\", \"OGG_OTHERS\": \"ê¸°ê´€_ê¸°íƒ€\"},\n",
    "    \"LC\": { \"LCP_COUNTRY\": \"ì¥ì†Œ_êµ­ê°€\", \"LCP_PROVINCE\": \"ì¥ì†Œ_ë„/ì£¼ ì§€ì—­\", \"LCP_COUNTY\": \"ì¥ì†Œ_ì„¸ë¶€ í–‰ì •êµ¬ì—­\", \"LCP_CITY\": \"ì¥ì†Œ_ë„ì‹œ\", \"LCP_CAPITALCITY\": \"ì¥ì†Œ_ìˆ˜ë„\", \"LCG_RIVER\": \"ì¥ì†Œ_ê°•/í˜¸ìˆ˜\", \"LCG_OCEAN\": \"ì¥ì†Œ_ë°”ë‹¤\", \"LCG_BAY\": \"ì¥ì†Œ_ë°˜ë„/ë§Œ\", \"LCG_MOUNTAIN\": \"ì¥ì†Œ_ì‚°/ì‚°ë§¥\", \"LCG_ISLAND\": \"ì¥ì†Œ_ì„¬\", \"LCG_CONTINENT\": \"ì¥ì†Œ_ëŒ€ë¥™\", \"LC_SPACE\": \"ì¥ì†Œ_ì²œì²´\", \"LC_OTHERS\": \"ì¥ì†Œ_ê¸°íƒ€\"},\n",
    "    \"CV\": { \"CV_CULTURE\": \"ë¬¸ëª…_ë¬¸ëª…/ë¬¸í™”\", \"CV_TRIBE\": \"ë¬¸ëª…_ë¯¼ì¡±/ì¢…ì¡±\", \"CV_LANGUAGE\": \"ë¬¸ëª…_ì–¸ì–´\", \"CV_POLICY\": \"ë¬¸ëª…_ì œë„/ì •ì±…\", \"CV_LAW\": \"ë¬¸ëª…_ë²•/ë²•ë¥ \", \"CV_CURRENCY\": \"ë¬¸ëª…_í†µí™”\", \"CV_TAX\": \"ë¬¸ëª…_ì¡°ì„¸\", \"CV_FUNDS\": \"ë¬¸ëª…_ì—°ê¸ˆ/ê¸°ê¸ˆ\", \"CV_ART\": \"ë¬¸ëª…_ì˜ˆìˆ \", \"CV_SPORTS\": \"ë¬¸ëª…_ìŠ¤í¬ì¸ \", \"CV_SPORTS_POSITION\": \"ë¬¸ëª…_ìŠ¤í¬ì¸  í¬ì§€ì…˜\", \"CV_SPORTS_INST\": \"ë¬¸ëª…_ìŠ¤í¬ì¸  ìš©í’ˆ/ë„êµ¬\", \"CV_PRIZE\": \"ë¬¸ëª…_ìƒ/í›ˆì¥\", \"CV_RELATION\": \"ë¬¸ëª…_ê°€ì¡±/ì¹œì¡± ê´€ê³„\", \"CV_OCCUPATION\": \"ë¬¸ëª…_ì§ì—…\", \"CV_POSITION\": \"ë¬¸ëª…_ì§ìœ„/ì§ì±…\", \"CV_FOOD\": \"ë¬¸ëª…_ìŒì‹\", \"CV_DRINK\": \"ë¬¸ëª…_ìŒë£Œ/ìˆ \", \"CV_FOOD_STYLE\": \"ë¬¸ëª…_ìŒì‹ ìœ í˜•\", \"CV_CLOTHING\": \"ë¬¸ëª…_ì˜ë³µ/ì„¬ìœ \", \"CV_BUILDING_TYPE\": \"ë¬¸ëª…_ê±´ì¶• ì–‘ì‹\"},\n",
    "    \"DT\": { \"DT_DURATION\": \"ë‚ ì§œ_ê¸°ê°„\", \"DT_DAY\": \"ë‚ ì§œ_ì¼\", \"DT_WEEK\": \"ë‚ ì§œ_ì£¼(ì£¼ì°¨)\", \"DT_MONTH\": \"ë‚ ì§œ_ë‹¬(ì›”)\", \"DT_YEAR\": \"ë‚ ì§œ_ì—°(ë…„)\", \"DT_SEASON\": \"ë‚ ì§œ_ê³„ì ˆ\", \"DT_GEOAGE\": \"ë‚ ì§œ_ì§€ì§ˆì‹œëŒ€\", \"DT_DYNASTY\": \"ë‚ ì§œ_ì™•ì¡°ì‹œëŒ€\", \"DT_OTHERS\": \"ë‚ ì§œ_ê¸°íƒ€\"},\n",
    "    \"TI\": { \"TI_DURATION\": \"ì‹œê°„_ê¸°ê°„\", \"TI_HOUR\": \"ì‹œê°„_ì‹œê°(ì‹œ)\", \"TI_MINUTE\": \"ì‹œê°„_ë¶„\", \"TI_SECOND\": \"ì‹œê°„_ì´ˆ\", \"TI_OTHERS\": \"ì‹œê°„_ê¸°íƒ€\"},\n",
    "    \"QT\": { \"QT_AGE\": \"ìˆ˜ëŸ‰_ë‚˜ì´\", \"QT_SIZE\": \"ìˆ˜ëŸ‰_ë„“ì´/ë©´ì \", \"QT_LENGTH\": \"ìˆ˜ëŸ‰_ê¸¸ì´/ê±°ë¦¬\", \"QT_COUNT\": \"ìˆ˜ëŸ‰_ìˆ˜ëŸ‰/ë¹ˆë„\", \"QT_MAN_COUNT\": \"ìˆ˜ëŸ‰_ì¸ì›ìˆ˜\", \"QT_WEIGHT\": \"ìˆ˜ëŸ‰_ë¬´ê²Œ\", \"QT_PERCENTAGE\": \"ìˆ˜ëŸ‰_ë°±ë¶„ìœ¨\", \"QT_SPEED\": \"ìˆ˜ëŸ‰_ì†ë„\", \"QT_TEMPERATURE\": \"ìˆ˜ëŸ‰_ì˜¨ë„\", \"QT_VOLUME\": \"ìˆ˜ëŸ‰_ë¶€í”¼\", \"QT_ORDER\": \"ìˆ˜ëŸ‰_ìˆœì„œ\", \"QT_PRICE\": \"ìˆ˜ëŸ‰_ê¸ˆì•¡\", \"QT_PHONE\": \"ìˆ˜ëŸ‰_ì „í™”ë²ˆí˜¸\", \"QT_SPORTS\": \"ìˆ˜ëŸ‰_ìŠ¤í¬ì¸  ìˆ˜ëŸ‰\", \"QT_CHANNEL\": \"ìˆ˜ëŸ‰_ì±„ë„ ë²ˆí˜¸\", \"QT_ALBUM\": \"ìˆ˜ëŸ‰_ì•¨ë²” ìˆ˜ëŸ‰\", \"QT_ADDRESS\": \"ìˆ˜ëŸ‰_ì£¼ì†Œ ê´€ë ¨ ìˆ«ì\", \"QT_OTHERS\": \"ìˆ˜ëŸ‰_ê¸°íƒ€ ìˆ˜ëŸ‰\"},\n",
    "    \"EV\": { \"EV_ACTIVITY\": \"ì‚¬ê±´_ì‚¬íšŒìš´ë™/ì„ ì–¸\", \"EV_WAR_REVOLUTION\": \"ì‚¬ê±´_ì „ìŸ/í˜ëª…\", \"EV_SPORTS\": \"ì‚¬ê±´_ìŠ¤í¬ì¸  í–‰ì‚¬\", \"EV_FESTIVAL\": \"ì‚¬ê±´_ì¶•ì œ/ì˜í™”ì œ\", \"EV_OTHERS\": \"ì‚¬ê±´_ê¸°íƒ€\"},\n",
    "    \"AM\": { \"AM_INSECT\": \"ë™ë¬¼_ê³¤ì¶©\", \"AM_BIRD\": \"ë™ë¬¼_ì¡°ë¥˜\", \"AM_FISH\": \"ë™ë¬¼_ì–´ë¥˜\", \"AM_MAMMALIA\": \"ë™ë¬¼_í¬ìœ ë¥˜\", \"AM_AMPHIBIA\": \"ë™ë¬¼_ì–‘ì„œë¥˜\", \"AM_REPTILIA\": \"ë™ë¬¼_íŒŒì¶©ë¥˜\", \"AM_TYPE\": \"ë™ë¬¼_ë¶„ë¥˜ëª…\", \"AM_PART\": \"ë™ë¬¼_ë¶€ìœ„ëª…\", \"AM_OTHERS\": \"ë™ë¬¼_ê¸°íƒ€\"},\n",
    "    \"PT\": { \"PT_FRUIT\": \"ì‹ë¬¼_ê³¼ì¼/ì—´ë§¤\", \"PT_FLOWER\": \"ì‹ë¬¼_ê½ƒ\", \"PT_TREE\": \"ì‹ë¬¼_ë‚˜ë¬´\", \"PT_GRASS\": \"ì‹ë¬¼_í’€\", \"PT_TYPE\": \"ì‹ë¬¼_ë¶„ë¥˜ëª…\", \"PT_PART\": \"ì‹ë¬¼_ë¶€ìœ„ëª…\", \"PT_OTHERS\": \"ì‹ë¬¼_ê¸°íƒ€\"},\n",
    "    \"MT\": { \"MT_ELEMENT\": \"ë¬¼ì§ˆ_ì›ì†Œ\", \"MT_METAL\": \"ë¬¼ì§ˆ_ê¸ˆì†\", \"MT_ROCK\": \"ë¬¼ì§ˆ_ì•”ì„\", \"MT_CHEMICAL\": \"ë¬¼ì§ˆ_í™”í•™\"},\n",
    "    \"TM\": { \"TM_COLOR\": \"ìš©ì–´_ìƒ‰ê¹”\", \"TM_DIRECTION\": \"ìš©ì–´_ë°©í–¥\", \"TM_CLIMATE\": \"ìš©ì–´_ê¸°í›„ ì§€ì—­\", \"TM_SHAPE\": \"ìš©ì–´_ëª¨ì–‘/í˜•íƒœ\", \"TM_CELL_TISSUE_ORGAN\": \"ìš©ì–´_ì„¸í¬/ì¡°ì§/ê¸°ê´€\", \"TMM_DISEASE\": \"ìš©ì–´_ì¦ìƒ/ì§ˆë³‘\", \"TMM_DRUG\": \"ìš©ì–´_ì•½í’ˆ\", \"TMI_HW\": \"ìš©ì–´_IT í•˜ë“œì›¨ì–´\", \"TMI_SW\": \"ìš©ì–´_IT ì†Œí”„íŠ¸ì›¨ì–´\", \"TMI_SITE\": \"ìš©ì–´_URL ì£¼ì†Œ\", \"TMI_EMAIL\": \"ìš©ì–´_ì´ë©”ì¼ ì£¼ì†Œ\", \"TMI_MODEL\": \"ìš©ì–´_ì œí’ˆ ëª¨ë¸ëª…\", \"TMI_SERVICE\": \"ìš©ì–´_IT ì„œë¹„ìŠ¤\", \"TMI_PROJECT\": \"ìš©ì–´_í”„ë¡œì íŠ¸\", \"TMIG_GENRE\": \"ìš©ì–´_ê²Œì„ ì¥ë¥´\", \"TM_SPORTS\": \"ìš©ì–´_ìŠ¤í¬ì¸ \"},\n",
    "}\n",
    "labels = []\n",
    "for main_category in entity_type_mapping:\n",
    "    sub_dict = entity_type_mapping[main_category]\n",
    "    for key in sub_dict:\n",
    "        labels.append(sub_dict[key])\n",
    "\n",
    "# 2. GLiNer XL ëª¨ë¸ ë¡œë“œ\n",
    "gliner_model = GLiNER.from_pretrained(\"lots-o/gliner-bi-ko-xlarge-v1\")\n",
    "gliner_model = gliner_model.to(\"cuda\")\n",
    "print(\"âœ… GLiNer XL ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import bm25s\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, BaseNode\n",
    "from typing import List, Dict, Callable, Optional\n",
    "import tqdm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. [í•µì‹¬] ë°°ì¹˜ ì§€ì› í† í¬ë‚˜ì´ì € í•¨ìˆ˜ (List[str] ì…ë ¥ì„ ë°›ìŒ)\n",
    "# -----------------------------------------------------------------------------\n",
    "def tokenize_gliner_batch(\n",
    "    texts: List[str],  # ğŸš¨ ì…ë ¥ì´ ë‹¨ì¼ strì´ ì•„ë‹ˆë¼ List[str]ì…ë‹ˆë‹¤!\n",
    "    model: GLiNER, \n",
    "    labels: List[str], \n",
    "    label_chunk_size: int = 20,\n",
    "    score_threshold_ratio: float = 1.05\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ë¬¸ì¥ì„ í•œ ë²ˆì— ë°›ì•„ì„œ, ê° ë¬¸ì¥ë³„ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    (V100 GPU ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜)\n",
    "    \"\"\"\n",
    "    if not texts: return []\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥ìš© (ë¬¸ì„œ ê°œìˆ˜ë§Œí¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±)\n",
    "    batch_results = [[] for _ in texts]\n",
    "    \n",
    "    # ë¼ë²¨ ë°°ì¹˜ ì²˜ë¦¬ (Label Chunking)\n",
    "    for i in range(0, len(labels), label_chunk_size):\n",
    "        sub_labels = labels[i : i + label_chunk_size]\n",
    "        try:\n",
    "            # ğŸš€ model.batch_predict_entities ì‚¬ìš© (ì†ë„ í–¥ìƒì˜ í•µì‹¬)\n",
    "            batch_preds = model.batch_predict_entities(\n",
    "                texts, sub_labels, flat_ner=True, threshold=0.1\n",
    "            )\n",
    "            # ê²°ê³¼ ë³‘í•©\n",
    "            for doc_idx, entities in enumerate(batch_preds):\n",
    "                batch_results[doc_idx].extend(entities)\n",
    "        except Exception: continue\n",
    "    \n",
    "    # í›„ì²˜ë¦¬ (ê° ë¬¸ì„œë³„ë¡œ ì ìˆ˜ í•„í„° & ë¬¸ì¥ ìª¼ê°œê¸° ì ìš©)\n",
    "    final_batch_tokens = []\n",
    "    \n",
    "    for entities in batch_results:\n",
    "        if not entities:\n",
    "            final_batch_tokens.append([])\n",
    "            continue\n",
    "            \n",
    "        # 1. ì ìˆ˜ í•„í„°ë§ (Relative Threshold)\n",
    "        max_score = max(e['score'] for e in entities)\n",
    "        cutoff_score = max_score / score_threshold_ratio\n",
    "        filtered = [e for e in entities if e['score'] >= cutoff_score]\n",
    "        \n",
    "        # 2. ìŠ¤ë§ˆíŠ¸ í•„í„° (ë¬¸ì¥ ìª¼ê°œê¸°)\n",
    "        doc_tokens = set()\n",
    "        for e in filtered:\n",
    "            token_text = e['text']\n",
    "            # ë„ì–´ì“°ê¸° 2ê°œ ì´ìƒ(3ì–´ì ˆ)ì´ë©´ ìª¼ê°œê¸°\n",
    "            if token_text.count(' ') >= 2:\n",
    "                for t in token_text.split():\n",
    "                    doc_tokens.add(t)\n",
    "            else:\n",
    "                doc_tokens.add(token_text)\n",
    "        \n",
    "        final_batch_tokens.append(list(doc_tokens))\n",
    "        \n",
    "    return final_batch_tokens\n",
    "\n",
    "\n",
    "class GLiNerBM25Retriever(BaseRetriever):\n",
    "    \"\"\"partial í† í¬ë‚˜ì´ì €ë¥¼ ë°›ì•„ì„œ ë°°ì¹˜ ì²˜ë¦¬í•˜ëŠ” Retriever\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes: List[BaseNode],\n",
    "        tokenizer: Callable[[List[str]], List[List[str]]], # ğŸš¨ ë°°ì¹˜ í† í¬ë‚˜ì´ì € ì‹œê·¸ë‹ˆì²˜\n",
    "        similarity_top_k: int = 30,\n",
    "        doc_batch_size: int = 64 # ğŸš€ ë¬¸ì„œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ (í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜)\n",
    "    ) -> None:\n",
    "        \n",
    "        self._nodes = nodes\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._tokenizer = tokenizer\n",
    "        self.doc_batch_size = doc_batch_size\n",
    "\n",
    "        print(f\"ğŸš€ GLiNer Batch Indexing... Docs: {len(nodes)}, Batch: {doc_batch_size}\")\n",
    "        \n",
    "        corpus_tokens = []\n",
    "        \n",
    "        # ë¬¸ì„œë¥¼ ë­‰í……ì´(Batch)ë¡œ ì˜ë¼ì„œ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ í˜¸ì¶œ\n",
    "        for i in tqdm.tqdm(range(0, len(nodes), doc_batch_size), desc=\"Indexing\"):\n",
    "            batch_nodes = nodes[i : i + doc_batch_size]\n",
    "            batch_texts = [n.text if n.text else \"\" for n in batch_nodes]\n",
    "            \n",
    "            # ì—¬ê¸°ì„œ partialë¡œ ë§Œë“  í•¨ìˆ˜ì— 'ë¦¬ìŠ¤íŠ¸'ë¥¼ ë˜ì§‘ë‹ˆë‹¤!\n",
    "            batch_tokens_list = self._tokenizer(batch_texts)\n",
    "            corpus_tokens.extend(batch_tokens_list)\n",
    "\n",
    "        self._bm25 = bm25s.BM25()\n",
    "        self._bm25.index(corpus_tokens)\n",
    "        \n",
    "        print(\"âœ… Indexing Complete!\")\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        query = query_bundle.query_str\n",
    "        \n",
    "        # ì¿¼ë¦¬ëŠ” 1ê°œì§€ë§Œ, ë°°ì¹˜ í•¨ìˆ˜ë‹ˆê¹Œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë³´ëƒ„\n",
    "        # ê²°ê³¼ë„ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë‹ˆê¹Œ [0]ìœ¼ë¡œ êº¼ëƒ„\n",
    "        query_tokens = self._tokenizer([query])[0]\n",
    "        \n",
    "        if not query_tokens: return []\n",
    "        \n",
    "        tokenized_query = [query_tokens]\n",
    "        actual_k = min(self._similarity_top_k, len(self._nodes))\n",
    "        if actual_k == 0: return []\n",
    "\n",
    "        results, scores = self._bm25.retrieve(tokenized_query, k=actual_k)\n",
    "        \n",
    "        nodes_with_scores: List[NodeWithScore] = []\n",
    "        for idx, score in zip(results[0], scores[0]):\n",
    "            if score > 0:\n",
    "                nodes_with_scores.append(\n",
    "                    NodeWithScore(node=self._nodes[idx], score=float(score))\n",
    "                )\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ GLiNer Batch Indexing... Docs: 128727, Batch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing:   0%|          | 0/16091 [00:00<?, ?it/s]/tmp/ipykernel_496380/3586281713.py:32: FutureWarning: GLiNER.batch_predict_entities is deprecated and will be removed in a future release. Please use GLiNER.inference instead.\n",
      "  batch_preds = model.batch_predict_entities(\n",
      "Indexing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11565/16091 [4:12:06<1:39:44,  1.32s/it]/data/ephemeral/home/mcr/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:372: UserWarning: Sentence of length 522 has been truncated to 512\n",
      "  batch = [self.preprocess_example(b[\"tokenized_text\"], b[key], class_to_ids) for b in batch_list]\n",
      "Indexing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13115/16091 [4:45:42<1:02:11,  1.25s/it]/data/ephemeral/home/mcr/.venv/lib/python3.12/site-packages/gliner/data_processing/processor.py:372: UserWarning: Sentence of length 523 has been truncated to 512\n",
      "  batch = [self.preprocess_example(b[\"tokenized_text\"], b[key], class_to_ids) for b in batch_list]\n",
      "Indexing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16091/16091 [5:50:08<00:00,  1.31s/it]  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78e81683af345e7af1fecdf76a74846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Create Vocab:   0%|          | 0/128727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7433e115d4a345438b244f6cf064ea75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Convert tokens to indices:   0%|          | 0/128727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3954e3efb44e4d98940ed089c1052a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/128727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba77ebebff09485badac9465f0d81ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/128727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Indexing Complete!\n",
      "âœ… GLiNer Retriever ì¤€ë¹„ ì™„ë£Œ (Batch Mode)\n"
     ]
    }
   ],
   "source": [
    "# 1. ë°°ì¹˜ í† í¬ë‚˜ì´ì € í•¨ìˆ˜ë¥¼ partialë¡œ ì¡°ë¦½\n",
    "my_gliner_batch_tokenizer = partial(\n",
    "    tokenize_gliner_batch, \n",
    "    model=gliner_model,         # XL ëª¨ë¸\n",
    "    labels=labels,              # 150ê°œ ë¼ë²¨\n",
    "    label_chunk_size=50,        # ë¼ë²¨ 20ê°œì”© ëŠê¸°\n",
    "    score_threshold_ratio=2.5  # ì ìˆ˜ í•„í„°\n",
    ")\n",
    "\n",
    "# 2. Retriever ìƒì„± (ì¸ë±ì‹± ì‹œì‘)\n",
    "# nodesëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤!\n",
    "gliner_bm25_retriever = GLiNerBM25Retriever(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=30,\n",
    "    tokenizer=my_gliner_batch_tokenizer, # partial í•¨ìˆ˜ ì „ë‹¬\n",
    "    doc_batch_size=8 # ğŸš€ í•œ ë²ˆì— 64ê°œì”© ì²˜ë¦¬ (V100 ë©”ëª¨ë¦¬ í™œìš©)\n",
    ")\n",
    "\n",
    "print(\"âœ… GLiNer Retriever ì¤€ë¹„ ì™„ë£Œ (Batch Mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Fusion Retriever ì—°ê²°\n"
     ]
    }
   ],
   "source": [
    "#ê³¨ë“ ë¦¬íŠ¸ë¦¬ë²„\n",
    "retriever = vector_index.as_retriever(similarity_top_k=50)\n",
    "\n",
    "\n",
    "\n",
    "print(\"ğŸ”— Fusion Retriever ì—°ê²°\")\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    retrievers=[retriever, gliner_bm25_retriever],\n",
    "    similarity_top_k=30,  \n",
    "    num_queries=1,\n",
    "    use_async=False,\n",
    "    mode=\"reciprocal_rerank\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ ì›ë³¸ ì§ˆë¬¸: ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ì„ ë–¼ì£½ìŒìœ¼ë¡œ ëª°ê³  ê°„ ë³‘ì€?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_496380/3586281713.py:32: FutureWarning: GLiNER.batch_predict_entities is deprecated and will be removed in a future release. Please use GLiNER.inference instead.\n",
      "  batch_preds = model.batch_predict_entities(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© GLiNerê°€ ë½‘ì€ í† í°: ['ë‰´ì‰ê¸€ëœë“œ', 'ë‰´ì‰ê¸€ëœë“œì´ˆì›', 'ë©§ë‹­']\n",
      "--------------------------------------------------\n",
      "âœ… ì´ 3ê°œì˜ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 1. í™•ì¸í•  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "query = dataset['train'][307]['question']\n",
    "\n",
    "print(f\"â“ ì›ë³¸ ì§ˆë¬¸: {query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. ë¦¬íŠ¸ë¦¬ë²„ ë‚´ë¶€ì˜ í† í¬ë‚˜ì´ì € ì§ì ‘ í˜¸ì¶œ\n",
    "# (ìš°ë¦¬ê°€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ List[str] ì…ë ¥ì„ ë°›ê²Œ ë§Œë“¤ì—ˆìœ¼ë¯€ë¡œ [query] ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤)\n",
    "tokens_batch = gliner_bm25_retriever._tokenizer([query])\n",
    "\n",
    "# 3. ê²°ê³¼ í™•ì¸ (ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ)\n",
    "extracted_tokens = tokens_batch[0]\n",
    "\n",
    "print(f\"ğŸ§© GLiNerê°€ ë½‘ì€ í† í°: {extracted_tokens}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4. ê²°ê³¼ í•´ì„\n",
    "if not extracted_tokens:\n",
    "    print(\"ğŸ˜± [ê²½ê³ ] í† í°ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤! (ê²€ìƒ‰ ê²°ê³¼ 0ê±´ í™•ì •)\")\n",
    "    print(\"   -> GLiNerê°€ ì´ ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"âœ… ì´ {len(extracted_tokens)}ê°œì˜ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ ì§ˆë¬¸: ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ì„ ë–¼ì£½ìŒìœ¼ë¡œ ëª°ê³  ê°„ ë³‘ì€?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_496380/3586281713.py:32: FutureWarning: GLiNER.batch_predict_entities is deprecated and will be removed in a future release. Please use GLiNER.inference instead.\n",
      "  batch_preds = model.batch_predict_entities(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2df404586b2442a80840e07503787cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [GLiNer Only] ê²€ìƒ‰ ê²°ê³¼: 30ê°œ ë°œê²¬\n",
      "\n",
      "ğŸ… [Rank 1] BM25 Score: 8.7178\n",
      "ğŸ“„ ë¬¸ì„œ ID: 24634\n",
      "ğŸ“ ë‚´ìš©: 1791ë…„ ì¼ë¶€ ì •ì¹˜ì¸ë“¤ì€ ìˆ˜ë µì‹œì¦Œì„ ë„ì…í•˜ë ¤ í–ˆì§€ë§Œ, ì‚¬ëŒë“¤ì˜ ë¹„ì›ƒìŒë§Œ ì‚¬ê²Œ ë˜ì—ˆë‹¤. ê·¸ ê²°ê³¼ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì€ ë‹¤ë¥¸ ë‘ ì•„ì¢…ì²˜ëŸ¼ ê³¼ë„í•œ ë‚¨íšìœ¼ë¡œ ì¸í•´ í° íƒ€ê²©ì„ ì…ì–´ 19ì„¸ê¸°...\n",
      "------------------------------------------------------------\n",
      "ğŸ… [Rank 2] BM25 Score: 6.7319\n",
      "ğŸ“„ ë¬¸ì„œ ID: 24634\n",
      "ğŸ“ ë‚´ìš©: ì´ˆì›ë‡Œì¡°ë“¤ì€ ë¯¸êµ­ ë…ë¦½ì „ìŸ ì´ì „ì— ë²„ì§€ë‹ˆì•„ ì£¼ì—ì„œë¶€í„° ë©”ì¸ì£¼ê¹Œì§€ ì„œì‹í–ˆë‹¤. ì´ë“¤ì€ ìƒë‹¹íˆ í”í–ˆë‹¤. 1620ë…„ì— í”Œë¦¬ë¨¸ìŠ¤(Plymouth, ì˜êµ­ì˜ í•­êµ¬ ì´ë¦„ì„ ë”´ ë¯¸êµ­ì˜ ë§ˆì„ë¡œ í˜„ì¬...\n",
      "------------------------------------------------------------\n",
      "ğŸ… [Rank 3] BM25 Score: 4.7166\n",
      "ğŸ“„ ë¬¸ì„œ ID: 56478\n",
      "ğŸ“ ë‚´ìš©: í•˜ì§€ë§Œ, ë‰´ì‰ê¸€ëœë“œ ì²­êµë„ëŠ” í•˜ë²„ë“œ ëŒ€í•™ì˜ ì”ë””ë°­ì— ë‚˜ì˜¨ ê²ƒì²˜ëŸ¼,  ë‰´ì‰ê¸€ëœë“œì˜ ì²«ì—´ë§¤ë¡œ í•˜ë²„ë“œ ëŒ€í•™ì„ ìƒì§•í•˜ë„ë¡ í•˜ì˜€ë‹¤. ê·¸ë“¤ì€ ë§ì€ ì‚¬ëŒë“¤ì´ ì˜ì™¸ë¡œ ë¬´ì§€í•œ ê²ƒì— ë†€ëìœ¼ë©°, ê·¸ë“¤...\n",
      "------------------------------------------------------------\n",
      "ğŸ… [Rank 4] BM25 Score: 4.2644\n",
      "ğŸ“„ ë¬¸ì„œ ID: 24635\n",
      "ğŸ“ ë‚´ìš©: ì´ë¡œ ì¸í•´ ì´ˆì›ë©§ë‹­ì€ ë”ìš± í° í”¼í•´ë¥¼ ì…ê²Œ ë˜ì—ˆë‹¤. ê²Œë‹¤ê°€, ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ ë‹¤ìˆ˜ëŠ” ê°€ì¶• ì¹ ë©´ì¡°ê°€ ì´ ì„¬ì— ë„ì…í•œ ê°€ê¸ˆë¥˜ ì§ˆë³‘ì¸ í‘ë‘ë³‘(ë‹­ëª©ì˜ ë™ë¬¼ë“¤ì´ ê±¸ë¦¬ëŠ” ì§ˆë³‘ìœ¼ë¡œ ì¹ ...\n",
      "------------------------------------------------------------\n",
      "ğŸ… [Rank 5] BM25 Score: 3.8913\n",
      "ğŸ“„ ë¬¸ì„œ ID: 47072\n",
      "ğŸ“ ë‚´ìš©: ì‹ ì•™ì£¼ì˜(fideism)ë€ ì‹ ì•™ì— ê·¼ê±°í•œ í•™ë¬¸ê³¼ ì‚¶ì˜ ì²´ê³„ë¥¼ ë§í•œë‹¤. ë³´í†µ ì´ì„±ì£¼ì˜ì— ë°˜ëŒ€ì  ê°œë…ì´ì§€ë§Œ ê¸°ë…êµì—ì„œëŠ” ì‹ ì•™ì´ ì´ì„±ë³´ë‹¤ ì•ì„ ë‹¤(fides quaerens intelle...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "query = dataset['train'][307]['question']\n",
    "print(f\"â“ ì§ˆë¬¸: {query}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 2. GLiNer ë‹¨ë… ê²€ìƒ‰ ì‹¤í–‰\n",
    "# (ë‚´ë¶€ì—ì„œ print(f\"ğŸ” Query Tokens: ...\") ë¡œê·¸ê°€ ì°í ê²ë‹ˆë‹¤. ì´ê±¸ ê¼­ í™•ì¸í•˜ì„¸ìš”!)\n",
    "gliner_results = gliner_bm25_retriever.retrieve(query)\n",
    "\n",
    "# 3. ê²°ê³¼ ëœ¯ì–´ë³´ê¸°\n",
    "print(f\"\\nğŸ” [GLiNer Only] ê²€ìƒ‰ ê²°ê³¼: {len(gliner_results)}ê°œ ë°œê²¬\\n\")\n",
    "\n",
    "for i, node_with_score in enumerate(gliner_results[:5]): # ìƒìœ„ 5ê°œë§Œ\n",
    "    node = node_with_score.node\n",
    "    score = node_with_score.score\n",
    "    \n",
    "    print(f\"ğŸ… [Rank {i+1}] BM25 Score: {score:.4f}\")\n",
    "    print(f\"ğŸ“„ ë¬¸ì„œ ID: {node.metadata.get('document_id', 'N/A')}\")\n",
    "    # ê°€ë…ì„±ì„ ìœ„í•´ ì¤„ë°”ê¿ˆ ì œê±°í•˜ê³  100ìë§Œ ì¶œë ¥\n",
    "    print(f\"ğŸ“ ë‚´ìš©: {node.text[:100].replace('\\n', ' ')}...\") \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ ì§ˆë¬¸: ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ì„ ë–¼ì£½ìŒìœ¼ë¡œ ëª°ê³  ê°„ ë³‘ì€?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_496380/3586281713.py:32: FutureWarning: GLiNER.batch_predict_entities is deprecated and will be removed in a future release. Please use GLiNER.inference instead.\n",
      "  batch_preds = model.batch_predict_entities(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076c33539f8a4d3daf32f68bdc16a92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì´ 30ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "============================================================\n",
      "ğŸ† [Rank 1] Score: 0.0328\n",
      "ğŸ“„ ë¬¸ì„œ ID: 24634\n",
      "ğŸ“ ë‚´ìš©: 1791ë…„ ì¼ë¶€ ì •ì¹˜ì¸ë“¤ì€ ìˆ˜ë µì‹œì¦Œì„ ë„ì…í•˜ë ¤ í–ˆì§€ë§Œ, ì‚¬ëŒë“¤ì˜ ë¹„ì›ƒìŒë§Œ ì‚¬ê²Œ ë˜ì—ˆë‹¤. ê·¸ ê²°ê³¼ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì€ ë‹¤ë¥¸ ë‘ ì•„ì¢…ì²˜ëŸ¼ ê³¼ë„í•œ ë‚¨íšìœ¼ë¡œ ì¸í•´ í° íƒ€ê²©ì„ ì…ì–´ 19ì„¸ê¸°...\n",
      "------------------------------------------------------------\n",
      "ğŸ† [Rank 2] Score: 0.0323\n",
      "ğŸ“„ ë¬¸ì„œ ID: 24635\n",
      "ğŸ“ ë‚´ìš©: ì´ë¡œ ì¸í•´ ì´ˆì›ë©§ë‹­ì€ ë”ìš± í° í”¼í•´ë¥¼ ì…ê²Œ ë˜ì—ˆë‹¤. ê²Œë‹¤ê°€, ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ ë‹¤ìˆ˜ëŠ” ê°€ì¶• ì¹ ë©´ì¡°ê°€ ì´ ì„¬ì— ë„ì…í•œ ê°€ê¸ˆë¥˜ ì§ˆë³‘ì¸ í‘ë‘ë³‘(ë‹­ëª©ì˜ ë™ë¬¼ë“¤ì´ ê±¸ë¦¬ëŠ” ì§ˆë³‘ìœ¼ë¡œ ì¹ ...\n",
      "------------------------------------------------------------\n",
      "ğŸ† [Rank 3] Score: 0.0318\n",
      "ğŸ“„ ë¬¸ì„œ ID: 24634\n",
      "ğŸ“ ë‚´ìš©: ì´ˆì›ë‡Œì¡°ë“¤ì€ ë¯¸êµ­ ë…ë¦½ì „ìŸ ì´ì „ì— ë²„ì§€ë‹ˆì•„ ì£¼ì—ì„œë¶€í„° ë©”ì¸ì£¼ê¹Œì§€ ì„œì‹í–ˆë‹¤. ì´ë“¤ì€ ìƒë‹¹íˆ í”í–ˆë‹¤. 1620ë…„ì— í”Œë¦¬ë¨¸ìŠ¤(Plymouth, ì˜êµ­ì˜ í•­êµ¬ ì´ë¦„ì„ ë”´ ë¯¸êµ­ì˜ ë§ˆì„ë¡œ í˜„ì¬...\n",
      "------------------------------------------------------------\n",
      "ğŸ† [Rank 4] Score: 0.0306\n",
      "ğŸ“„ ë¬¸ì„œ ID: 24635\n",
      "ğŸ“ ë‚´ìš©: ë‹¤í–‰íˆë„, ê·¸ë•Œì¯¤ ì´ë“¤ì„ ë³´í˜¸í•˜ìëŠ” ì—¬ë¡ ì´ ì¡°ì„±ë˜ì—ˆë‹¤. 648ë§Œ í‰ë°©ë¯¸í„°ì— ë‹¬í•˜ëŠ” ì„¬ì€ ë³´í˜¸êµ¬ë¡œ ì§€ì •í–ˆë‹¤. íš¨ê³¼ëŠ” ë›°ì–´ë‚˜ì„œ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì˜ ê°œì²´ìˆ˜ê°€ 1915ë…„ì—ëŠ” 800ë§ˆë¦¬, ...\n",
      "------------------------------------------------------------\n",
      "ğŸ† [Rank 5] Score: 0.0303\n",
      "ğŸ“„ ë¬¸ì„œ ID: 27558\n",
      "ğŸ“ ë‚´ìš©: ê·¸ëŠ” ê³§ ë‰´ì‰ê¸€ëœë“œ í•´ì•ˆì˜ ë‹¤ìˆ˜ ì¢…ì¡±ì¸ íŒŒë‘ì…‹ ë¶€ì¡±ì„ ë°œê²¬í–ˆì§€ë§Œ, ê·¸ë“¤ì€ ì´ë¯¸ ì²œì—°ë‘ë¡œ ì˜ì‹¬ë˜ëŠ” ëª‡ ë…„ì— ê±¸ì¹œ ì „ì—¼ë³‘ìœ¼ë¡œ ë§ì€ ì´ë“¤ì´ ì£½ì€ ì´í›„ì˜€ë‹¤. (2010ë…„ ì—°êµ¬ ì¡°ì‚¬ì—ì„œ ì´...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "query = dataset['train'][307]['question']\n",
    "print(f\"â“ ì§ˆë¬¸: {query}\\n\")\n",
    "\n",
    "# 2. ê²€ìƒ‰ ì‹¤í–‰\n",
    "retrieved_nodes = fusion_retriever.retrieve(query)\n",
    "\n",
    "# 3. ê²°ê³¼ ëœ¯ì–´ë³´ê¸° (ìƒìœ„ 5ê°œë§Œ ì¶œë ¥)\n",
    "print(f\"ğŸ” ì´ {len(retrieved_nodes)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "for i, node_with_score in enumerate(retrieved_nodes[:5]):\n",
    "    node = node_with_score.node\n",
    "    score = node_with_score.score\n",
    "    \n",
    "    # Fusion RetrieverëŠ” ì—¬ëŸ¬ ë¦¬íŠ¸ë¦¬ë²„ ê²°ê³¼ë¥¼ ì„ê¸° ë•Œë¬¸ì—\n",
    "    # ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•´ ì›ë˜ ì–´ë””ì„œ ì™”ëŠ”ì§€(Vector? GLiNer?) ìœ ì¶”í•´ì•¼ í•  ìˆ˜ë„ ìˆì§€ë§Œ,\n",
    "    # ê¸°ë³¸ì ìœ¼ë¡œ RRF(Reciprocal Rank Fusion) ì ìˆ˜ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "    \n",
    "    print(f\"ğŸ† [Rank {i+1}] Score: {score:.4f}\")\n",
    "    print(f\"ğŸ“„ ë¬¸ì„œ ID: {node.metadata.get('document_id', 'N/A')}\")\n",
    "    print(f\"ğŸ“ ë‚´ìš©: {node.text[:100].replace('\\n', ' ')}...\") # 100ìë§Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_for_rerank = [n.node.text for n in retrieved_nodes]\n",
    "ids_for_rerank = [n.node.metadata['document_id'] for n in retrieved_nodes]\n",
    "\n",
    "reranked_results = reranker.rerank(query, docs_for_rerank, ids_for_rerank, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q. ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ì„ ë–¼ì£½ìŒìœ¼ë¡œ ëª°ê³  ê°„ ë³‘ì€?\n",
      "[24635. 24636. 24634. 24634. 24635.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_context = \"\\n\\n---\\n\\n\".join([f\"[{i+1}] {d[0]}\" for i, d in enumerate(reranked_results[0])])\n",
    "print(f'Q. {query}')\n",
    "\n",
    "print(np.array(reranked_results[1])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ë‰´ì‰ê¸€ëœë“œì´ˆì›ë‡Œì¡°',\n",
       " 'context': 'ë‹¤í–‰íˆë„, ê·¸ë•Œì¯¤ ì´ë“¤ì„ ë³´í˜¸í•˜ìëŠ” ì—¬ë¡ ì´ ì¡°ì„±ë˜ì—ˆë‹¤. 648ë§Œ í‰ë°©ë¯¸í„°ì— ë‹¬í•˜ëŠ” ì„¬ì€ ë³´í˜¸êµ¬ë¡œ ì§€ì •í–ˆë‹¤. íš¨ê³¼ëŠ” ë›°ì–´ë‚˜ì„œ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì˜ ê°œì²´ìˆ˜ê°€ 1915ë…„ì—ëŠ” 800ë§ˆë¦¬, 1916ë…„ì—ëŠ” 2ì²œ ë§ˆë¦¬ê¹Œì§€ ëŠ˜ì—ˆë‹¤. ë¬¸ì œëŠ” ì´ ì•„ì¢…ì˜ ì „ì²´ ê°œì²´êµ°ì´ ì´ ì‘ì€ ì„¬ í•˜ë‚˜ì—ë§Œ í•œì •ì ìœ¼ë¡œ ì„œì‹í–ˆê¸° ë•Œë¬¸ì— êµ‰ì¥íˆ ì·¨ì•½í–ˆë‹¤. ê²Œë‹¤ê°€ ì´ ì„¬ì˜ ê°œì²´êµ° ì¼ë¶€ë¥¼ ë‹¤ë¥¸ ì„¬ì´ë‚˜ ë³¸í† ì— ì˜®ê¸°ëŠ” ì‹œë„ëŠ” ì—†ì—ˆë‹¤. ì¦‰, ì´ ì•„ì¢… ì „ì²´ê°€ ì¬ë‚œì— ëŒ€ë‹¨íˆ ì·¨ì•½í•˜ë‹¤ëŠ” ê²°ë¡ ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤. ì‹¤ì œë¡œ 2,000ë§ˆë¦¬ê¹Œì§€ ì¦ê°€í•œ 1916ë…„ì— ì„¬ ì „ì—­ì—ì„œ ì¼ì–´ë‚œ í™”ì¬ë¡œ ë²ˆì‹ì§€ê°€ íŒŒê´´ë˜ê³  ë§ì•˜ë‹¤. ê³µêµë¡­ê²Œë„ ì´ ì‹œê¸°ëŠ” ë²ˆì‹ê¸°ì—¬ì„œ ëŒ€ë¶€ë¶„ì˜ ì´ˆì›ë©§ë‹­ê³¼ ì•Œë“¤ë„ ë¶ˆì— íƒ€ ì£½ì–´ê°”ë‹¤. ê·¸ ê²°ê³¼ ê²¨ìš° 105ë§ˆë¦¬ë§Œ ì‚´ì•„ë‚¨ì•˜ë‹¤. ë‹¤ìŒ í•´ ê²¨ìš¸ì€ ë§¤ìš° í˜¹ë…í•˜ê²Œ ì¶”ì› ìœ¼ë©°, ì´ˆì›ë©§ë‹­ì˜ ì²œì ì¸ ì°¸ë§¤ê°€ ì´ ì„¬ì— ë“¤ì–´ì˜¤ê²Œ ë˜ì—ˆë‹¤. ì´ë¡œ ì¸í•´ ì´ˆì›ë©§ë‹­ì€ ë”ìš± í° í”¼í•´ë¥¼ ì…ê²Œ ë˜ì—ˆë‹¤. ê²Œë‹¤ê°€, ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ ë‹¤ìˆ˜ëŠ” ê°€ì¶• ì¹ ë©´ì¡°ê°€ ì´ ì„¬ì— ë„ì…í•œ ê°€ê¸ˆë¥˜ ì§ˆë³‘ì¸ í‘ë‘ë³‘(ë‹­ëª©ì˜ ë™ë¬¼ë“¤ì´ ê±¸ë¦¬ëŠ” ì§ˆë³‘ìœ¼ë¡œ ì¹ ë©´ì¡°í¸ëª¨ì¶©ì˜ ê°ì—¼ì´ ì›ì¸ì´ë‹¤)ì— ê°ì—¼ë˜ì–´ ëŒ€ëŸ‰ íì‚¬í–ˆë‹¤.',\n",
       " 'question': 'ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ì„ ë–¼ì£½ìŒìœ¼ë¡œ ëª°ê³  ê°„ ë³‘ì€?',\n",
       " 'id': 'mrc-0-000315',\n",
       " 'answers': {'answer_start': [505], 'text': ['í‘ë‘ë³‘']},\n",
       " 'document_id': 24635,\n",
       " '__index_level_0__': 210}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][307]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ì´ë¡œ ì¸í•´ ì´ˆì›ë©§ë‹­ì€ ë”ìš± í° í”¼í•´ë¥¼ ì…ê²Œ ë˜ì—ˆë‹¤. ê²Œë‹¤ê°€, ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ë“¤ ë‹¤ìˆ˜ëŠ” ê°€ì¶• ì¹ ë©´ì¡°ê°€ ì´ ì„¬ì— ë„ì…í•œ ê°€ê¸ˆë¥˜ ì§ˆë³‘ì¸ í‘ë‘ë³‘(ë‹­ëª©ì˜ ë™ë¬¼ë“¤ì´ ê±¸ë¦¬ëŠ” ì§ˆë³‘ìœ¼ë¡œ ì¹ ë©´ì¡°í¸ëª¨ì¶©ì˜ ê°ì—¼ì´ ì›ì¸ì´ë‹¤)ì— ê°ì—¼ë˜ì–´ ëŒ€ëŸ‰ íì‚¬í–ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "[2] ê·¸ ê²°ê³¼ 1927ë…„ì—ëŠ” ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì´ ê²¨ìš° 13ë§ˆë¦¬ë°–ì— ë‚¨ì§€ ì•Šì•˜ê³  ê±°ì˜ ëŒ€ë¶€ë¶„ì€ ìˆ˜ì»·ì´ì—ˆë‹¤. ë§ˆì§€ë§‰ ë‚¨ì€ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì„ ì—°êµ¬í•œ ì•„ì„œ ê·¸ë¡œìŠ¤(Arthur Gross)ëŠ” ë§ˆì§€ë§‰ ë‚¨ì€ ì´ ìƒˆì— ëŒ€í•œ ìµœì¢… ë³´ê³ ì„œì— ë‹¤ìŒê³¼ ê°™ì´ ì¼ë‹¤. \"ëª¨ë“  ë‚ ì”¨ ë³€í™”ì™€ ì§ˆë³‘, ì²œì ì˜ ì˜í–¥ì„ ë°›ëŠ” ì´ ìƒˆê°€ ê·¸ë ‡ê²Œ ì˜¤ë«ë™ì•ˆ í™€ë¡œ ì‚´ ìˆ˜ ìˆì—ˆë˜ ê²ƒì€ ì •ë§ ë†€ëë‹¤.\" 1931ë…„ 4ì›”, ê·¸ë¡œìŠ¤ëŠ” ì´ í™€ë¡œ ë‚¨ì€ ìƒˆë¥¼ ì¡ì•„ ë ë¥¼ ë‘ë¥¸ í›„ ë†“ì•„ì£¼ì—ˆë‹¤.\n",
      "\n",
      "1932ë…„ 3ì›” 11ì¼ ì €ë…ì— ë§ˆì§€ë§‰ í•œë§ˆë¦¬ì¸ 8ì‚´ì§œë¦¬ ìˆ˜ì»·ì´ ì£½ìœ¼ë©´ì„œ, ì—°ì†ì ìœ¼ë¡œ ì¬ë‚œì— íœ©ì‹¸ì¸ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì€ ë©¸ì¢…í–ˆë‹¤. ì§€ë°©ì‹ ë¬¸ì€ \"ìƒì¡´ìëŠ” ì—†ë‹¤. ë¯¸ë˜ë„ ì—†ë‹¤. ì´ëŸ° ì¢…ë¥˜ë¡œ ì¬ì°½ì¡°ë˜ëŠ” ìƒëª…ì€ ë”ì´ìƒ ì—†ì„ ê²ƒì´ë‹¤.\"ê³  ë³´ë„í–ˆë‹¤. ì´ë¡œì¨ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë‡Œì¡°ëŠ” ë©¸ì¢…í–ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "[3] í•œë•Œ ë¡œí‚¤ì‚°ë§¥ì—ì„œë¶€í„° ì• íŒ”ë ˆì¹˜ì•„ ì‚°ë§¥ ë™ë¶€ì— ê±¸ì¹œ ì„œì‹ì˜ì—­ì„ ìë‘í•˜ë˜ í°ì´ˆì›ë©§ë‹­ì€ ë†ë¶€ë“¤ê³¼ ì •ì°©ë¯¼ì´ ì´ë“¤ì˜ ì„œì‹ì§€ë¥¼ íŒŒê´´í–ˆê³ , ì‚¬ëƒ¥ê¾¼ë“¤ì€ ì´ë“¤ì„ ì´ìœ¼ë¡œ ì´ 20ì„¸ê¸° ì´ˆì— ì´ ìœ™ìœ™ê±°ë¦¬ë©° ë‚˜ëŠ” í°ì´ˆì›ë‡Œì¡°ëŠ” ë©¸ì¢…ìœ„ê¸°ì— ì²˜í–ˆë‹¤. í˜„ì¬ëŠ” ì·¨ì•½ì¢…ë‹¨ê³„ì— ë“±ì¬ë˜ì–´ìˆë‹¤. ë˜ë‹¤ë¥¸ ì•„ì¢…ì¸ ì• íŠ¸ì›Œí„°ì´ˆì›ë©§ë‹­ì€ í•œë•Œ ë©•ì‹œì½”ë§Œ í•´ë³€ ë’¤ê¹Œì§€ í•´ì•ˆ ì´ˆì›ì— 100ë§Œ ë§ˆë¦¬ê°€ ì‚´ì•˜ë‹¤ê³  í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë“¤ë„ í°ì´ˆì›ë©§ë‹­ì²˜ëŸ¼ ì„œì‹ì§€íŒŒê´´ì™€ ì‚¬ëƒ¥ ë° ë‚¨íšìœ¼ë¡œ 1937ë…„ ì´ë“¤ì— ê´€í•œ ì—°êµ¬ê°€ ì‹œì‘ë˜ì—ˆì„ ë•Œ, ì´ë¯¸ ë£¨ì´ì§€ì• ë‚˜ì£¼ì—ì„œëŠ” ë©¸ì¢…í–ˆê³ , í…ì‚¬ìŠ¤ì£¼ì—ì„œëŠ” ì•½ 8,700ë§ˆë¦¬ë¡œ ê°ì†Œí–ˆë‹¤. 30ë…„í›„ì—ëŠ” ê°œì²´ìˆ˜ê°€ 1,100ë§ˆë¦¬ë„ ë‚¨ì§€ ì•Šì•„ì„œ ë©¸ì¢…ìœ„ê¸°ì¢… ëª©ë¡ì— ë“¤ì–´ê°”ë‹¤. í˜„ì¬ëŠ” ì´ ì¢…ì—­ì‹œ ì·¨ì•½ì¢…ìœ¼ë¡œ ë“±ê¸‰ì´ ë‚´ë ¤ê°”ë‹¤. ì• íŒ”ë ˆì¹˜ì•„ ì‚°ë§¥ ë™ìª½ì—ì„œëŠ” ì´ˆì›ë©§ë‹­ì˜ ë‹¤ë¥¸ ì•„ì¢…ì´ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ìœ¼ë¡œ ë¶ˆë ¸ë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "[4] 1791ë…„ ì¼ë¶€ ì •ì¹˜ì¸ë“¤ì€ ìˆ˜ë µì‹œì¦Œì„ ë„ì…í•˜ë ¤ í–ˆì§€ë§Œ, ì‚¬ëŒë“¤ì˜ ë¹„ì›ƒìŒë§Œ ì‚¬ê²Œ ë˜ì—ˆë‹¤. ê·¸ ê²°ê³¼ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì€ ë‹¤ë¥¸ ë‘ ì•„ì¢…ì²˜ëŸ¼ ê³¼ë„í•œ ë‚¨íšìœ¼ë¡œ ì¸í•´ í° íƒ€ê²©ì„ ì…ì–´ 19ì„¸ê¸° ì¤‘ë°˜ì—ëŠ” ë‰´ì‰ê¸€ëœë“œ ë³¸í† ì—ì„œ ë©¸ì¢…í–ˆë‹¤. ë‹¨ì§€ ë§¤ì‚¬ì¶”ì„¸ì¸  ì£¼ ì½”ë“œê³¶ ì¸ê·¼í•´ì˜ ì‘ì€ ì„¬ì¸ ë§ˆì„œìŠ¤ ë¹„ë…€ë“œ(Martha's Vineyard, ë§ˆì‚¬ì˜ í¬ë„ì›ì´ë¼ëŠ” ëœ»)ì—ì„œë§Œ ë‹¨ì§€ 200ë§ˆë¦¬ ì •ë„ë§Œ ì‚´ì•„ë‚¨ì•˜ë‹¤. 1907ë…„ì—ëŠ” ê²¨ìš° 50ë§ˆë¦¬ë§Œ ë‚¨ì•˜ë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "[5] ë‹¤í–‰íˆë„, ê·¸ë•Œì¯¤ ì´ë“¤ì„ ë³´í˜¸í•˜ìëŠ” ì—¬ë¡ ì´ ì¡°ì„±ë˜ì—ˆë‹¤. 648ë§Œ í‰ë°©ë¯¸í„°ì— ë‹¬í•˜ëŠ” ì„¬ì€ ë³´í˜¸êµ¬ë¡œ ì§€ì •í–ˆë‹¤. íš¨ê³¼ëŠ” ë›°ì–´ë‚˜ì„œ ë‰´ì‰ê¸€ëœë“œì´ˆì›ë©§ë‹­ì˜ ê°œì²´ìˆ˜ê°€ 1915ë…„ì—ëŠ” 800ë§ˆë¦¬, 1916ë…„ì—ëŠ” 2ì²œ ë§ˆë¦¬ê¹Œì§€ ëŠ˜ì—ˆë‹¤. ë¬¸ì œëŠ” ì´ ì•„ì¢…ì˜ ì „ì²´ ê°œì²´êµ°ì´ ì´ ì‘ì€ ì„¬ í•˜ë‚˜ì—ë§Œ í•œì •ì ìœ¼ë¡œ ì„œì‹í–ˆê¸° ë•Œë¬¸ì— êµ‰ì¥íˆ ì·¨ì•½í–ˆë‹¤. ê²Œë‹¤ê°€ ì´ ì„¬ì˜ ê°œì²´êµ° ì¼ë¶€ë¥¼ ë‹¤ë¥¸ ì„¬ì´ë‚˜ ë³¸í† ì— ì˜®ê¸°ëŠ” ì‹œë„ëŠ” ì—†ì—ˆë‹¤. ì¦‰, ì´ ì•„ì¢… ì „ì²´ê°€ ì¬ë‚œì— ëŒ€ë‹¨íˆ ì·¨ì•½í•˜ë‹¤ëŠ” ê²°ë¡ ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤. ì‹¤ì œë¡œ 2,000ë§ˆë¦¬ê¹Œì§€ ì¦ê°€í•œ 1916ë…„ì— ì„¬ ì „ì—­ì—ì„œ ì¼ì–´ë‚œ í™”ì¬ë¡œ ë²ˆì‹ì§€ê°€ íŒŒê´´ë˜ê³  ë§ì•˜ë‹¤. ê³µêµë¡­ê²Œë„ ì´ ì‹œê¸°ëŠ” ë²ˆì‹ê¸°ì—¬ì„œ ëŒ€ë¶€ë¶„ì˜ ì´ˆì›ë©§ë‹­ê³¼ ì•Œë“¤ë„ ë¶ˆì— íƒ€ ì£½ì–´ê°”ë‹¤. ê·¸ ê²°ê³¼ ê²¨ìš° 105ë§ˆë¦¬ë§Œ ì‚´ì•„ë‚¨ì•˜ë‹¤. ë‹¤ìŒ í•´ ê²¨ìš¸ì€ ë§¤ìš° í˜¹ë…í•˜ê²Œ ì¶”ì› ìœ¼ë©°, ì´ˆì›ë©§ë‹­ì˜ ì²œì ì¸ ì°¸ë§¤ê°€ ì´ ì„¬ì— ë“¤ì–´ì˜¤ê²Œ ë˜ì—ˆë‹¤. ì´ë¡œ ì¸í•´ ì´ˆì›ë©§ë‹­ì€ ë”ìš± í° í”¼í•´ë¥¼ ì…ê²Œ ë˜ì—ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(final_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dir = \"/data/ephemeral/home/data/train_dataset\"\n",
    "train_dataset = load_from_disk(train_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset['train']['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "import tqdm\n",
    "\n",
    "result_for_test = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(train_dataset['train']['question']))):\n",
    "\n",
    "    # ì§ˆë¬¸ê³¼ id\n",
    "    test_q_query = train_dataset['train'][i]['question']\n",
    "    test_q_id = train_dataset['train'][i]['id']\n",
    "\n",
    "    # ê³¨ë“ ë¦¬íŠ¸ë¦¬ë²„ ê·€ì—½ë‹¤\n",
    "    retrieved_nodes_test = fusion_retriever.retrieve(test_q_query)\n",
    "\n",
    "\n",
    "    # data for reranker\n",
    "    docs_for_rerank_test = [n.node.text for n in retrieved_nodes_test]\n",
    "    ids_for_rerank_test = [n.node.metadata['document_id'] for n in retrieved_nodes_test]\n",
    "\n",
    "\n",
    "    # rerank result\n",
    "    reranked_results_test = reranker.rerank(test_q_query, docs_for_rerank_test, ids_for_rerank_test, top_k=5)\n",
    "    result_for_test.append([test_q_id, (list(np.array(reranked_results_test[1])[:,0].astype(int)))])\n",
    "    \n",
    "def convert_to_json(data):\n",
    "    question_id = []\n",
    "    document_list = []\n",
    "\n",
    "    for q_id, doc_list in data:\n",
    "        question_id.append(q_id)\n",
    "        document_list.append(list(map(int, (doc_list))))\n",
    "    result_dict = {\n",
    "        \"question_id\": question_id,\n",
    "        \"document_id\": document_list\n",
    "    }\n",
    "    return result_dict\n",
    "\n",
    "json_test = convert_to_json(result_for_test)\n",
    "\n",
    "\n",
    "file_path = './document_list/train_ner_hybrid.json'\n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(json_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcr (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
